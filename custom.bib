@string{icassp = "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)"}

@string{interspeech = "Interspeech"}


@inproceedings{suresh_diasynth_2025,
	address = {Albuquerque, New Mexico},
	title = {{DiaSynth}: {Synthetic} {Dialogue} {Generation} {Framework} for {Low} {Resource} {Dialogue} {Applications}},
	isbn = {979-8-89176-195-7},
	url = {https://aclanthology.org/2025.findings-naacl.40/},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Suresh, Sathya Krishnan and Mengjun, Wu and Pranav, Tushar and Chng, EngSiong},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {673--690},
}

@inproceedings{sanabria_how2_2018,
	title = {How2: {A} {Large}-scale {Dataset} {For} {Multimodal} {Language} {Understanding}},
	url = {http://arxiv.org/abs/1811.00347},
	booktitle = {Proceedings of the {Workshop} on {Visually} {Grounded} {Interaction} and {Language} ({ViGIL})},
	publisher = {NeurIPS},
	author = {Sanabria, Ramon and Caglayan, Ozan and Palaskar, Shruti and Elliott, Desmond and Barrault, Loïc and Specia, Lucia and Metze, Florian},
	year = {2018},
}

@inproceedings{kano_attention-based_2021,
	series = {2021 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop}, {ASRU} 2021 - {Proceedings}},
	title = {Attention-{Based} {Multi}-{Hypothesis} {Fusion} for {Speech} {Summarization}},
	doi = {10.1109/ASRU51503.2021.9687977},
	abstract = {Speech summarization, which generates a text summary from speech, can be achieved by combining automatic speech recognition (ASR) and text summarization (TS). With this cascade approach, we can exploit state-of-the-art models and large training datasets for both subtasks, i.e., Transformer for ASR and Bidirectional Encoder Representations from Transformers (BERT) for TS. However, ASR errors directly affect the quality of the output summary in the cascade approach. We propose a cascade speech summarization model that is robust to ASR errors and that exploits multiple hypotheses generated by ASR to attenuate the effect of ASR errors on the summary. We investigate several schemes to combine ASR hypotheses. First, we propose using the sum of sub-word embedding vectors weighted by their posterior values provided by an ASR system as an input to a BERT-based TS system. Then, we introduce a more general scheme that uses an attention-based fusion module added to a pre-trained BERT module to align and combine several ASR hypotheses. Finally, we perform speech summarization experiments on the How2 dataset and a newly assembled TED-based dataset that we will release with this paper11https://github.com/nttcslab-sp-admin/TEDSummary. These experiments show that retraining the BERT-based TS system with these schemes can improve summarization performance and that the attention-based fusion module is particularly effective.},
	language = {English},
	booktitle = {2021 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop}, {ASRU} 2021 - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kano, Takatomo and Ogawa, Atsunori and Delcroix, Marc and Watanabe, Shinji},
	year = {2021},
	keywords = {Attention-based Fusion, Automatic Speech Recognition, BERT, Speech Summarization},
	pages = {487--494},
}

@article{fabbri_summeval_2021,
	title = {{SummEval}: {Re}-evaluating {Summarization} {Evaluation}},
	volume = {9},
	url = {https://doi.org/10.1162/tacl\_a\_00373},
	doi = {10.1162/TACL_A_00373},
	journal = {Trans. Assoc. Comput. Linguistics},
	author = {Fabbri, Alexander R. and Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir R.},
	year = {2021},
	pages = {391--409},
}

@inproceedings{binici_medsage_2025,
	title = {{MEDSAGE}: {Enhancing} {Robustness} of {Medical} {Dialogue} {Summarization} to {ASR} {Errors} with {LLM}-generated {Synthetic} {Dialogues}},
	url = {https://doi.org/10.1609/aaai.v39i22.34518},
	doi = {10.1609/AAAI.V39I22.34518},
	booktitle = {{AAAI}-25, {Sponsored} by the {Association} for the {Advancement} of {Artificial} {Intelligence}, {February} 25 - {March} 4, 2025, {Philadelphia}, {PA}, {USA}},
	publisher = {AAAI Press},
	author = {Binici, Kuluhan and Kashyap, Abhinav Ramesh and Schlegel, Viktor and Liu, Andy T. and Dwivedi, Vijay Prakash and Nguyen, Thanh-Tung and Gao, Xiaoxue and Chen, Nancy F. and Winkler, Stefan},
	editor = {Walsh, Toby and Shah, Julie and Kolter, Zico},
	year = {2025},
	pages = {23496--23504},
}

@inproceedings{retkowski_zero-shot_2025,
	address = {Albuquerque, New Mexico},
	title = {Zero-{Shot} {Strategies} for {Length}-{Controllable} {Summarization}},
	isbn = {979-8-89176-195-7},
	url = {https://aclanthology.org/2025.findings-naacl.34/},
	abstract = {Large language models (LLMs) struggle with precise length control, particularly in zero-shot settings. We conduct a comprehensive study evaluating LLMs' length control capabilities across multiple measures and propose practical methods to improve controllability. Our experiments with LLaMA 3 reveal stark differences in length adherence across measures and highlight inherent biases of the model. To address these challenges, we introduce a set of methods: length approximation, target adjustment, sample filtering, and automated revisions. By combining these methods, we demonstrate substantial improvements in length compliance while maintaining or enhancing summary quality, providing highly effective zero-shot strategies for precise length control without the need for model fine-tuning or architectural changes. With our work, we not only advance our understanding of LLM behavior in controlled text generation but also pave the way for more reliable and adaptable summarization systems in real-world applications.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Retkowski, Fabian and Waibel, Alexander},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {551--572},
}

@inproceedings{zhang_summn_2022,
	address = {Dublin, Ireland},
	title = {Summ{\textasciicircum}{N}: {A} {Multi}-{Stage} {Summarization} {Framework} for {Long} {Input} {Dialogues} and {Documents}},
	shorttitle = {Summ{\textasciicircum}{N}},
	url = {https://aclanthology.org/2022.acl-long.112/},
	doi = {10.18653/v1/2022.acl-long.112},
	abstract = {Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose Summ{\textasciicircum}N, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. Summ{\textasciicircum}N first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, Summ{\textasciicircum}N is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that Summ{\textasciicircum}N outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.},
	urldate = {2025-04-16},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yusen and Ni, Ansong and Mao, Ziming and Wu, Chen Henry and Zhu, Chenguang and Deb, Budhaditya and Awadallah, Ahmed and Radev, Dragomir and Zhang, Rui},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {1592--1604},
}

@inproceedings{kim_explainmeetsum_2023,
	address = {Toronto, Canada},
	title = {{ExplainMeetSum}: {A} {Dataset} for {Explainable} {Meeting} {Summarization} {Aligned} with {Human} {Intent}},
	shorttitle = {{ExplainMeetSum}},
	url = {https://aclanthology.org/2023.acl-long.731/},
	doi = {10.18653/v1/2023.acl-long.731},
	abstract = {To enhance the explainability of meeting summarization, we construct a new dataset called “ExplainMeetSum,” an augmented version of QMSum, by newly annotating evidence sentences that faithfully “explain” a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracles. We further present an explainability-aware task, named “Explainable Evidence Extraction” (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics.},
	urldate = {2025-04-16},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Hyun and Cho, Minsoo and Na, Seung-Hoon},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {13079--13098},
}

@techreport{retkowski_speech_2025,
	title = {From {Speech} to {Summary}: {A} {Comprehensive} {Survey} of {Speech} {Summarization}},
	copyright = {Open Access, KITopen License},
	shorttitle = {From {Speech} to {Summary}},
	url = {https://publikationen.bibliothek.kit.edu/1000180972},
	abstract = {Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization is still not clearly defined and intersects with several research areas, including speech recognition, text summarization, and specific applications like meeting summarization. This survey not only examines existing datasets and evaluation methodologies, which are crucial for assessing the effectiveness of summarization approaches but also synthesizes recent developments in the field, highlighting the shift from traditional systems to advanced models like fine-tuned cascaded architectures and end-to-end solutions.},
	language = {en},
	urldate = {2025-04-16},
	institution = {Karlsruher Institut für Technologie (KIT)},
	author = {Retkowski, Fabian and Züfle, Maike and Sudmann, Andreas and Pfau, Dinah and Niehues, Jan and Waibel, Alexander},
	year = {2025},
	doi = {10.5445/IR/1000180972},
	note = {Medium: PDF},
}

@inproceedings{pan_llmlingua-2_2024,
	address = {Bangkok, Thailand},
	title = {{LLMLingua}-2: {Data} {Distillation} for {Efficient} and {Faithful} {Task}-{Agnostic} {Prompt} {Compression}},
	shorttitle = {{LLMLingua}-2},
	url = {https://aclanthology.org/2024.findings-acl.57/},
	doi = {10.18653/v1/2024.findings-acl.57},
	abstract = {This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.},
	urldate = {2025-04-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and Rühle, Victor and Yang, Yuqing and Lin, Chin-Yew and Zhao, H. Vicky and Qiu, Lili and Zhang, Dongmei},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {963--981},
}

@inproceedings{michalopoulos_medicalsum_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{MedicalSum}: {A} {Guided} {Clinical} {Abstractive} {Summarization} {Model} for {Generating} {Medical} {Reports} from {Patient}-{Doctor} {Conversations}},
	shorttitle = {{MedicalSum}},
	url = {https://aclanthology.org/2022.findings-emnlp.349/},
	doi = {10.18653/v1/2022.findings-emnlp.349},
	abstract = {We introduce MedicalSum, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the Unified Medical Language System (UMLS). The novel knowledge augmentation is performed in three ways: (i) introducing a guidance signal that consists of the medical words in the input sequence, (ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings, and (iii) making use of a novel weighted loss function that provides a stronger incentive for the model to correctly predict words with a medical meaning. By applying these three strategies, MedicalSum takes clinical knowledge into consideration during the summarization process and achieves state-of-the-art ROUGE score improvements of 0.8-2.1 points (including 6.2\% ROUGE-1 error reduction in the PE section) when producing medical summaries of patient-doctor conversations.},
	urldate = {2025-04-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Michalopoulos, George and Williams, Kyle and Singh, Gagandeep and Lin, Thomas},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {4741--4749},
}

@inproceedings{laskar_building_2023,
	address = {Singapore},
	title = {Building {Real}-{World} {Meeting} {Summarization} {Systems} using {Large} {Language} {Models}: {A} {Practical} {Perspective}},
	shorttitle = {Building {Real}-{World} {Meeting} {Summarization} {Systems} using {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.emnlp-industry.33/},
	doi = {10.18653/v1/2023.emnlp-industry.33},
	abstract = {This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT-3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA-2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and   cost.},
	urldate = {2025-04-10},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Laskar, Md Tahmid Rahman and Fu, Xue-Yong and Chen, Cheng and Bhushan TN, Shashi},
	editor = {Wang, Mingxuan and Zitouni, Imed},
	month = dec,
	year = {2023},
	pages = {343--352},
}

@inproceedings{godfrey_switchboard_1992,
	title = {{SWITCHBOARD}: telephone speech corpus for research and development},
	volume = {1},
	doi = {10.1109/ICASSP.1992.225858},
	booktitle = icassp # ", 1992",
	author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
	year = {1992},
	keywords = {Authentication, Databases, Instruments, Protocols, Research and development, Speech processing, Speech recognition, Telephony, Testing, Vocabulary},
	pages = {517--520 vol.1},
}

@inproceedings{jones_trec_2020,
	series = {{NIST} {Special} {Publication}},
	title = {{TREC} 2020 {Podcasts} {Track} {Overview}},
	volume = {1266},
	url = {https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.P.pdf},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {Text} {REtrieval} {Conference}, {TREC} 2020, {Virtual} {Event} [{Gaithersburg}, {Maryland}, {USA}], {November} 16-20, 2020},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Jones, Rosie and Carterette, Ben and Clifton, Ann and Karlgren, Jussi and Pappu, Aasish and Reddy, Sravana and Yu, Yongze and Eskevich, Maria and Jones, Gareth J. F.},
	editor = {Voorhees, Ellen M. and Ellis, Angela},
	year = {2020},
}

@inproceedings{liu_topic-aware_2019,
	title = {Topic-{Aware} {Pointer}-{Generator} {Networks} for {Summarizing} {Spoken} {Conversations}},
	doi = {10.1109/ASRU46091.2019.9003764},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Liu, Zhengyuan and Ng, Angela and Lee, Sheldon and Aw, Ai Ti and Chen, Nancy F.},
	year = {2019},
	keywords = {Context modeling, Decoding, Dialogue, Hospitals, Linguistics, Mathematical model, Task analysis, Vocabulary, attention mechanism, conversation technology, neural networks, summarization},
	pages = {814--821},
}

@inproceedings{murray_extractive_2005,
	title = {Extractive summarization of meeting recordings},
	doi = {10.21437/Interspeech.2005-59},
	booktitle = interspeech # " 2005",
	author = {Murray, Gabriel and Renals, Steve and Carletta, Jean},
	year = {2005},
	note = {ISSN: 2958-1796},
	pages = {593--596},
}

@inproceedings{franzini_connectionist_1990,
	title = {Connectionist {Viterbi} training: a new hybrid method for continuous speech recognition},
	doi = {10.1109/ICASSP.1990.115733},
	booktitle = icassp # ", 1990",
	author = {Franzini, M. and Lee, K.-F. and Waibel, A.},
	year = {1990},
	keywords = {Databases, Hidden Markov models, NIST, Size measurement, System testing, Time measurement, Training data, Viterbi algorithm, Volume measurement},
	pages = {425--428 vol.1},
}

@article{waibel_phoneme_1989,
	title = {Phoneme recognition using time-delay neural networks},
	volume = {37},
	doi = {10.1109/29.21701},
	number = {3},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K.J.},
	year = {1989},
	keywords = {Acoustic testing, Backpropagation, Character recognition, Computer networks, Computer science, Hidden Markov models, Loudspeakers, Neural networks, Psychology, Speech recognition},
	pages = {328--339},
}

@inproceedings{kirstein_tell_2024,
	address = {Miami, Florida, US},
	title = {Tell me what {I} need to know: {Exploring} {LLM}-based ({Personalized}) {Abstractive} {Multi}-{Source} {Meeting} {Summarization}},
	shorttitle = {Tell me what {I} need to know},
	url = {https://aclanthology.org/2024.emnlp-industry.69/},
	doi = {10.18653/v1/2024.emnlp-industry.69},
	abstract = {Meeting summarization is crucial in digital communication, but existing solutions struggle with salience identification to generate personalized, workable summaries, and context understanding to fully comprehend the meetings' content.Previous attempts to address these issues by considering related supplementary resources (e.g., presentation slides) alongside transcripts are hindered by models' limited context sizes and handling the additional complexities of the multi-source tasks, such as identifying relevant information in additional files and seamlessly aligning it with the meeting content.This work explores multi-source meeting summarization considering supplementary materials through a three-stage large language model approach: identifying transcript passages needing additional context, inferring relevant details from supplementary materials and inserting them into the transcript, and generating a summary from this enriched transcript.Our multi-source approach enhances model understanding, increasing summary relevance by {\textbackslash}textasciitilde9\% and producing more content-rich outputs.We introduce a personalization protocol that extracts participant characteristics and tailors summaries accordingly, improving informativeness by {\textbackslash}textasciitilde10\%.This work further provides insights on performance-cost trade-offs across four leading model families, including edge-device capable options.Our approach can be extended to similar complex generative tasks benefitting from additional resources and personalization, such as dialogue systems and action planning.},
	urldate = {2025-04-09},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Kirstein, Frederic and Ruas, Terry and Kratel, Robert and Gipp, Bela},
	editor = {Dernoncourt, Franck and Preoţiuc-Pietro, Daniel and Shimorina, Anastasia},
	month = nov,
	year = {2024},
	pages = {920--939},
}

@misc{zhang_towards_2024,
	title = {Towards the {Law} of {Capacity} {Gap} in {Distilling} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.07052},
	doi = {10.48550/arXiv.2311.07052},
	abstract = {Language model (LM) distillation is a trending area that aims to distil the knowledge residing in a large teacher LM to a small student one. While various methods have been proposed to maximize the effectiveness of the distillation, significant challenges persist, particularly when there is a substantial capacity gap between the teacher and student LMs. This issue, often referred to as the {\textbackslash}textit\{curse\} of capacity gap, suggests that a larger teacher does not necessarily result in a superior student compared to one distilled from a smaller teacher. In other words, there is likely an optimal teacher yielding the best student along the scaling course of the teacher. However, the curse of capacity gap can not be tackled without notable compute overhead, as indicated in previous studies. In the context of large LMs (LLMs), previously viable approaches become much less meaningful, as it is an impossible triangle to distill an expected student from an optimal teacher student with small compute overhead. Fortunately, the impossible triangle can fortunately be possible provided an inducted {\textbackslash}textit\{law\} of capacity gap. In this paper, we take the spirits of scaling law and reveal that the optimal teacher scale almost consistently follows a linear scaling with the student scale across different model architectures and data scales. The law later guides us to distil a 3B student LM (termed {\textbackslash}textsc\{MiniMA\}) from LLaMA2-7B. {\textbackslash}textsc\{MiniMA\} is demonstrated to outperform a wide range of 3B competitors and could even compete with several 7B models.},
	urldate = {2025-04-08},
	publisher = {arXiv},
	author = {Zhang, Chen and Song, Dawei and Ye, Zheyu and Gao, Yan},
	month = jul,
	year = {2024},
	note = {arXiv:2311.07052 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bai_qwen_2023,
	title = {Qwen {Technical} {Report}},
	url = {http://arxiv.org/abs/2309.16609},
	doi = {10.48550/arXiv.2309.16609},
	abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.},
	urldate = {2025-04-08},
	publisher = {arXiv},
	author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and Hui, Binyuan and Ji, Luo and Li, Mei and Lin, Junyang and Lin, Runji and Liu, Dayiheng and Liu, Gao and Lu, Chengqiang and Lu, Keming and Ma, Jianxin and Men, Rui and Ren, Xingzhang and Ren, Xuancheng and Tan, Chuanqi and Tan, Sinan and Tu, Jianhong and Wang, Peng and Wang, Shijie and Wang, Wei and Wu, Shengguang and Xu, Benfeng and Xu, Jin and Yang, An and Yang, Hao and Yang, Jian and Yang, Shusheng and Yao, Yang and Yu, Bowen and Yuan, Hongyi and Yuan, Zheng and Zhang, Jianwei and Zhang, Xingxuan and Zhang, Yichang and Zhang, Zhenru and Zhou, Chang and Zhou, Jingren and Zhou, Xiaohuan and Zhu, Tianhang},
	month = sep,
	year = {2023},
	note = {arXiv:2309.16609 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2025-04-08},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{li_blip-2_2023,
	series = {{ICML}'23},
	title = {{BLIP}-2: bootstrapping language-image pre-training with frozen image encoders and large language models},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pretrained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
        url = {https://proceedings.mlr.press/v202/li23q/li23q.pdf},
	year = {2023},
	note = {Place: Honolulu, Hawaii, USA},
}

@inproceedings{gulati_conformer_2020,
	title = {Conformer: {Convolution}-augmented {Transformer} for {Speech} {Recognition}},
	doi = {10.21437/Interspeech.2020-3015},
	booktitle = interspeech # " 2020",
	author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
	year = {2020},
	note = {ISSN: 2958-1796},
	pages = {5036--5040},
}

@article{hsu_hubert_2021,
	title = {{HuBERT}: {Self}-{Supervised} {Speech} {Representation} {Learning} by {Masked} {Prediction} of {Hidden} {Units}},
	volume = {29},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2021.3122291},
	doi = {10.1109/TASLP.2021.3122291},
	abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.{\textless}xref ref-type="fn" rid="fn1"{\textgreater}$^{\textrm{1}}${\textless}/xref{\textgreater}{\textless}xref ref-type="fn" rid="fn2"{\textgreater}$^{\textrm{2}}${\textless}/xref{\textgreater}},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = oct,
	year = {2021},
	note = {Publisher: IEEE Press},
	pages = {3451--3460},
}

@inproceedings{radford_robust_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/radford23a.html},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {28492--28518},
}

@inproceedings{franzini_connectionist_1990-1,
	address = {Albuquerque, NM, USA},
	title = {Connectionist {Viterbi} training: a new hybrid method for continuous speech recognition},
	shorttitle = {Connectionist {Viterbi} training},
	url = {http://ieeexplore.ieee.org/document/115733/},
	doi = {10.1109/ICASSP.1990.115733},
	urldate = {2025-04-07},
	booktitle = icassp # ", 1990",
	publisher = {IEEE},
	author = {Franzini, M. and Lee, K.-F. and Waibel, A.},
	year = {1990},
	pages = {425--428},
}

@article{waibel_phoneme_1989-1,
	title = {Phoneme recognition using time-delay neural networks},
	volume = {37},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/21701/},
	doi = {10.1109/29.21701},
	number = {3},
	urldate = {2025-04-07},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K.J.},
	month = mar,
	year = {1989},
	pages = {328--339},
}

@article{rabiner_tutorial_1989,
  author={Rabiner, Lawrence R.},
  journal={Proceedings of the IEEE}, 
  title={A tutorial on hidden Markov models and selected applications in speech recognition}, 
  year={1989},
  volume={77},
  number={2},
  pages={257-286},
  keywords={Tutorial;Hidden Markov models;Speech recognition},
  doi={10.1109/5.18626},
  url={https://ieeexplore.ieee.org/document/18626},
}

@article{licklider_process_1952,
	title = {On the {Process} of {Speech} {Perception}},
	volume = {24},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/24/6/590/618469/On-the-Process-of-Speech-Perception},
	doi = {10.1121/1.1906938},
	abstract = {The process of speech perception is analyzed into three main operations: (1) translation of the speech signal into form suitable for the nervous system, (2) identification of discrete speech elements, and (3) comprehension of meaning. The first operation appears to correspond roughly to the transformation made by the sound spectrograph. The second may be carried out by the neural equivalent of a set of matched filters. The third appears to involve a neural form of cross-correlation that exhibits some of the properties of the analogous electronic process.},
	language = {en},
	number = {6},
	urldate = {2025-04-07},
	journal = {The Journal of the Acoustical Society of America},
	author = {Licklider, J. C. R.},
	year = {1952},
	pages = {590--594},
}

@article{david_ears_1955,
	title = {Ears for computers},
	volume = {192},
	issn = {0036-8733},
	number = {2},
	journal = {Scientific American},
	author = {David, Edward E},
	year = {1955},
	note = {Publisher: JSTOR},
	pages = {92--99},
}

@book{bourlard_connectionist_1994,
	address = {Boston, MA},
	series = {The {Kluwer} {International} {Series} in {Engineering} and {Computer} {Science}, {VLSI}, {Computer} {Architecture} and {Digital} {Signal} {Processing}},
	title = {Connectionist {Speech} {Recognition}: {A} {Hybrid} {Approach}},
	isbn = {978-1-4615-3210-1},
	shorttitle = {Connectionist {Speech} {Recognition}},
	abstract = {Connectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state of the art continuous speech recognition systems based on hidden Markov models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e. HMM emission probability estimation and feature extraction. The book describes a successful five-year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical systems. Using standard databases and comparison with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. The book is also suitable as a text for advanced courses on neural networks or speech processing},
	language = {eng},
	number = {247},
	publisher = {Kluwer Academic Publishers},
	author = {Bourlard, Hervé A. and Morgan, Nelson},
	year = {1994},
	doi = {10.1007/978-1-4615-3210-1},
}

@misc{beltagy_longformer_2020,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      publisher = {arXiv},
      note = {arXiv:2004.05150 [cs.CL]},
      url={https://arxiv.org/abs/2004.05150}, 
}

@inproceedings{zhang_pegasus_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/zhang20ae.html},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {11328--11339},
}

@inproceedings{karlbom_abstract_2020,
	title = {Abstract {Podcast} {Summarization} using {BART} with {Longformer} {Attention}},
	url = {https://api.semanticscholar.org/CorpusID:233993149},
	booktitle = {Text {Retrieval} {Conference}},
	author = {Karlbom, Hannes and Clifton, A.},
	year = {2020},
}

@inproceedings{zechner_minimizing_2000,
	title = {Minimizing {Word} {Error} {Rate} in {Textual} {Summaries} of {Spoken} {Language}},
	url = {https://aclanthology.org/A00-2025/},
	booktitle = {1st {Meeting} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Zechner, Klaus and Waibel, Alex},
	year = {2000},
}

@article{ghosal_report_2022,
	title = {Report on the {SIGDial} 2021 special session on summarization of dialogues and multi-party meetings ({SummDial})},
	volume = {55},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3527546.3527561},
	doi = {10.1145/3527546.3527561},
	abstract = {The SummDial special session on summarization of dialogues and multi-party meetings was held virtually within the SIGDial 2021 conference on July 29, 2021. SummDial @ SIGDial 2021 aimed to bring together the speech, dialogue, and summarization communities to foster cross-pollination of ideas and fuel the discussions/collaborations to attempt this crucial and timely problem. When the pandemic has restricted most of our in-person interactions, the current scenario has forced people to go virtual, resulting in an information overload from frequent dialogues and meetings in the virtual environment. Summarization could help reduce the cognitive burden on the participants; however, multi-party speech summarization comes with its own set of challenges. The SummDial special session aimed to leverage the community intelligence to find effective solutions while also brainstorming the future of AI interventions in meetings and dialogues. We report the findings of the special session in this article. We organized the SummDial special session under the aegis of the EU-funded H2020 European Live Translator (ELITR) project.1Date: 29 July, 2021.Website: https://elitr.github.io/automatic-minuting/summdial.html.},
	number = {2},
	journal = {SIGIR Forum},
	author = {Ghosal, Tirthankar and Singh, Muskaan and Nedoluzhko, Anja and Bojar, Ondřej},
	month = mar,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
}

@inproceedings{clifton_100000_2020,
	address = {Barcelona, Spain (Online)},
	title = {100,000 {Podcasts}: {A} {Spoken} {English} {Document} {Corpus}},
	url = {https://aclanthology.org/2020.coling-main.519/},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Clifton, Ann and Reddy, Sravana and Yu, Yongze and Pappu, Aasish and Rezapour, Rezvaneh and Bonab, Hamed and Eskevich, Maria and Jones, Gareth and Karlgren, Jussi and Carterette, Ben and Jones, Rosie},
	month = dec,
	year = {2020},
	pages = {5903--5917},
}

@inproceedings{liu_what_2025,
    title = "What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations",
    author = "Liu, Dongqi  and
      Whitehouse, Chenxi  and
      Yu, Xi  and
      Mahon, Louis  and
      Saxena, Rohit  and
      Zhao, Zheng  and
      Qiu, Yifu  and
      Lapata, Mirella  and
      Demberg, Vera",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.310/",
    doi = "10.18653/v1/2025.acl-long.310",
    pages = "6187--6210",
    ISBN = "979-8-89176-251-0",
    abstract = "Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of our dataset. This study aims to pave the way for future research on scientific video-to-text summarization."
}

@inproceedings{murray_generating_2010,
	title = {Generating and {Validating} {Abstracts} of {Meeting} {Conversations}: a {User} {Study}},
	url = {https://aclanthology.org/W10-4211/},
	booktitle = {Proceedings of the 6th {International} {Natural} {Language} {Generation} {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Murray, Gabriel and Carenini, Giuseppe and Ng, Raymond},
	editor = {Kelleher, John and Namee, Brian Mac and Sluis, Ielka van der},
	month = jul,
	year = {2010},
}

@misc{bai_audiolog_2024,
	title = {{AudioLog}: {LLMs}-{Powered} {Long} {Audio} {Logging} with {Hybrid} {Token}-{Semantic} {Contrastive} {Learning}},
	shorttitle = {{AudioLog}},
	url = {http://arxiv.org/abs/2311.12371},
	doi = {10.48550/arXiv.2311.12371},
	abstract = {Previous studies in automated audio captioning have faced difficulties in accurately capturing the complete temporal details of acoustic scenes and events within long audio sequences. This paper presents AudioLog, a large language models (LLMs)-powered audio logging system with hybrid token-semantic contrastive learning. Specifically, we propose to fine-tune the pre-trained hierarchical token-semantic audio Transformer by incorporating contrastive learning between hybrid acoustic representations. We then leverage LLMs to generate audio logs that summarize textual descriptions of the acoustic environment. Finally, we evaluate the AudioLog system on two datasets with both scene and event annotations. Experiments show that the proposed system achieves exceptional performance in acoustic scene classification and sound event detection, surpassing existing methods in the field. Further analysis of the prompts to LLMs demonstrates that AudioLog can effectively summarize long audio sequences. To the best of our knowledge, this approach is the first attempt to leverage LLMs for summarizing long audio sequences.},
	urldate = {2025-03-28},
	publisher = {arXiv},
	author = {Bai, Jisheng and Yin, Han and Wang, Mou and Shi, Dongyuan and Gan, Woon-Seng and Chen, Jianfeng and Rahardja, Susanto},
	month = jan,
	year = {2024},
	note = {arXiv:2311.12371 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{nelson_cross-lingual_2024,
	title = {Cross-{Lingual} {Conversational} {Speech} {Summarization} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.06484},
	doi = {10.48550/arXiv.2408.06484},
	abstract = {Cross-lingual conversational speech summarization is an important problem, but suffers from a dearth of resources. While transcriptions exist for a number of languages, translated conversational speech is rare and datasets containing summaries are non-existent. We build upon the existing Fisher and Callhome Spanish-English Speech Translation corpus by supplementing the translations with summaries. The summaries are generated using GPT-4 from the reference translations and are treated as ground truth. The task is to generate similar summaries in the presence of transcription and translation errors. We build a baseline cascade-based system using open-source speech recognition and machine translation models. We test a range of LLMs for summarization and analyze the impact of transcription and translation errors. Adapting the Mistral-7B model for this task performs significantly better than off-the-shelf models and matches the performance of GPT-4.},
	urldate = {2025-03-28},
	publisher = {arXiv},
	author = {Nelson, Max and Wotherspoon, Shannon and Keith, Francis and Hartmann, William and Snover, Matthew},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06484 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{thulke_prompting_2024,
	title = {Prompting and {Fine}-{Tuning} of {Small} {LLMs} for {Length}-{Controllable} {Telephone} {Call} {Summarization}},
	url = {https://api.semanticscholar.org/CorpusID:273549679},
	journal = {2024 2nd International Conference on Foundation and Large Language Models (FLLM)},
	author = {Thulke, David and Gao, Yingbo and Jalota, Rricha and Dugast, Christian and Ney, Hermann},
	year = {2024},
	pages = {305--312},
}

@inproceedings{li_hierarchical_2021,
	address = {New York, NY, USA},
	series = {{UIST} '21},
	title = {Hierarchical {Summarization} for {Longform} {Spoken} {Dialog}},
	isbn = {978-1-4503-8635-7},
	url = {https://doi.org/10.1145/3472749.3474771},
	doi = {10.1145/3472749.3474771},
	abstract = {Every day we are surrounded by spoken dialog. This medium delivers rich diverse streams of information auditorily; however, systematically understanding dialog can often be non-trivial. Despite the pervasiveness of spoken dialog, automated speech understanding and quality information extraction remains markedly poor, especially when compared to written prose. Furthermore, compared to understanding text, auditory communication poses many additional challenges such as speaker disfluencies, informal prose styles, and lack of structure. These concerns all demonstrate the need for a distinctly speech tailored interactive system to help users understand and navigate the spoken language domain. While individual automatic speech recognition (ASR) and text summarization methods already exist, they are imperfect technologies; neither consider user purpose and intent nor address spoken language induced complications. Consequently, we design a two stage ASR and text summarization pipeline and propose a set of semantic segmentation and merging algorithms to resolve these speech modeling challenges. Our system enables users to easily browse and navigate content as well as recover from errors in these underlying technologies. Finally, we present an evaluation of the system which highlights user preference for hierarchical summarization as a tool to quickly skim audio and identify content of interest to the user.},
	booktitle = {The 34th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Li, Daniel and Chen, Thomas and Tung, Albert and Chilton, Lydia B},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {automatic speech recognition, information retrieval, machine learning applications, natural language interaction, summarization},
	pages = {582--597},
}

@inproceedings{fathullah_audiochatllama_2024,
	address = {Mexico City, Mexico},
	title = {{AudioChatLlama}: {Towards} {General}-{Purpose} {Speech} {Abilities} for {LLMs}},
	shorttitle = {{AudioChatLlama}},
	url = {https://aclanthology.org/2024.naacl-long.309/},
	doi = {10.18653/v1/2024.naacl-long.309},
	abstract = {In this work, we extend the instruction-tuned Llama-2 model with end-to-end general-purpose speech processing and reasoning abilities while maintaining the wide range of original LLM capabilities, without using any carefully curated paired data. The resulting end-to-end model, named AudioChatLlama, can utilize audio prompts as a replacement for text and sustain a conversation. Such a model also has extended cross-modal capabilities such as being able to perform spoken question answering (QA), speech translation, and audio summarization amongst many other closed and open-domain tasks. This is unlike prior approaches in speech, in which LLMs are extended to handle audio for a limited number of pre-designated tasks. On both synthesized and recorded speech QA test sets, evaluations show that our end-to-end approach is on par with or outperforms cascaded systems (speech recognizer + LLM) in terms of modelling the response to a prompt. Furthermore, unlike cascades, our approach can interchange text and audio modalities and intrinsically utilize prior context in a conversation to provide better results.},
	urldate = {2025-03-27},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fathullah, Yassir and Wu, Chunyang and Lakomkin, Egor and Li, Ke and Jia, Junteng and Shangguan, Yuan and Mahadeokar, Jay and Kalinli, Ozlem and Fuegen, Christian and Seltzer, Mike},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {5522--5532},
}

@misc{he_meralion-audiollm_2025,
	title = {{MERaLiON}-{AudioLLM}: {Bridging} {Audio} and {Language} with {Large} {Language} {Models}},
	shorttitle = {{MERaLiON}-{AudioLLM}},
	url = {http://arxiv.org/abs/2412.09818},
	doi = {10.48550/arXiv.2412.09818},
	abstract = {We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning in One Network), the first speech-text model tailored for Singapore's multilingual and multicultural landscape. Developed under the National Large Language Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates advanced speech and text processing to address the diverse linguistic nuances of local accents and dialects, enhancing accessibility and usability in complex, multilingual environments. Our results demonstrate improvements in both speech recognition and task-specific understanding, positioning MERaLiON-AudioLLM as a pioneering solution for region specific AI applications. We envision this release to set a precedent for future models designed to address localised linguistic and cultural contexts in a global framework.},
	urldate = {2025-03-27},
	publisher = {arXiv},
	author = {He, Yingxu and Liu, Zhuohan and Sun, Shuo and Wang, Bin and Zhang, Wenyu and Zou, Xunlong and Chen, Nancy F. and Aw, Ai Ti},
	month = jan,
	year = {2025},
	note = {arXiv:2412.09818 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{chen_train_2024,
	title = {Train {Long} and {Test} {Long}:{Leveraging} {Full} {Document} {Contexts} in {Speech} {Processing}},
	doi = {10.1109/ICASSP48485.2024.10446727},
	booktitle = icassp # ", 2024",
	author = {Chen, William and Kano, Takatomo and Ogawa, Atsunori and Delcroix, Marc and Watanabe, Shinji},
	year = {2024},
	keywords = {Adaptation models, Complexity theory, Data models, Encoding, Long-form ASR, Self-supervised Learning, Signal processing, Speech Summarization, Speech Translation, Training, Transformers},
	pages = {13066--13070},
}

@inproceedings{palaskar_multimodal_2021,
	title = {Multimodal {Speech} {Summarization} {Through} {Semantic} {Concept} {Learning}},
	doi = {10.21437/Interspeech.2021-1923},
	booktitle = interspeech # " 2021",
	author = {Palaskar, Shruti and Salakhutdinov, Ruslan and Black, Alan W. and Metze, Florian},
	year = {2021},
	note = {ISSN: 2958-1796},
	pages = {791--795},
}

@misc{eom_squba_2025,
	title = {{SQuBa}: {Speech} {Mamba} {Language} {Model} with {Querying}-{Attention} for {Efficient} {Summarization}},
	url = {https://openreview.net/forum?id=zOMa82W1HV},
	author = {Eom, SooHwan and Shim, Jay and Yoon, Eunseop and Yoon, Hee Suk and Ko, Hyeonmok and Hasegawa-Johnson, Mark A. and Yoo, Chang D.},
	year = {2025},
}

@inproceedings{miyazaki_exploring_2024,
	title = {Exploring the {Capability} of {Mamba} in {Speech} {Applications}},
	doi = {10.21437/Interspeech.2024-994},
	booktitle = interspeech # " 2024",
	author = {Miyazaki, Koichi and Masuyama, Yoshiki and Murata, Masato},
	year = {2024},
	note = {ISSN: 2958-1796},
	pages = {237--241},
}

@article{arnold_old_2004,
	title = {The {Old} and {Thee}, uh, {New}: {Disfluency} and {Reference} {Resolution}},
	volume = {15},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0956-7976, 1467-9280},
	shorttitle = {The {Old} and {Thee}, uh, {New}},
	url = {https://journals.sagepub.com/doi/10.1111/j.0956-7976.2004.00723.x},
	doi = {10.1111/j.0956-7976.2004.00723.x},
	abstract = {Most research on the rapid mental processes of online language processing has been limited to the study of idealized, fluent utterances. Yet speakers are often disfluent, for example, saying “thee, uh, candle” instead of “the candle.” By monitoring listeners' eye movements to objects in a display, we demonstrated that the fluency of an article (“thee uh” vs. “the”) affects how listeners interpret the following noun. With a fluent article, listeners were biased toward an object that had been mentioned previously, but with a disfluent article, they were biased toward an object that had not been mentioned. These biases were apparent as early as lexical information became available, showing that disfluency affects the basic processes of decoding linguistic input.},
	language = {en},
	number = {9},
	urldate = {2025-03-26},
	journal = {Psychological Science},
	author = {Arnold, Jennifer E. and Tanenhaus, Michael K. and Altmann, Rebecca J. and Fagnano, Maria},
	month = sep,
	year = {2004},
	pages = {578--582},
}

@inproceedings{stolcke_statistical_1996,
	title = {Statistical language modeling for speech disfluencies},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/541118/},
	urldate = {2025-03-26},
	booktitle = {1996 ieee international conference on acoustics, speech, and signal processing conference proceedings},
	publisher = {IEEE},
	author = {Stolcke, Andreas and Shriberg, Elizabeth},
	year = {1996},
	pages = {405--408},
}

@phdthesis{shriberg_preliminaries_1994,
	type = {{PhD} {Thesis}},
	title = {Preliminaries to a theory of speech disfluencies},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c0ca94051f549f08e0bb4be7694540460fd47f1b},
	urldate = {2025-03-26},
	school = {Citeseer},
	author = {Shriberg, Elizabeth Ellen},
	year = {1994},
}

@book{jurafsky_speech_2014,
	title = {{SPEECH} {AND} {LANGUAGE} {PROCESSING} {AN} {INTRODUCTION} {TO} {NATURAL} {LANGUAGE} {PROCESSING}, {2ND} {EDITION}},
	isbn = {978-93-325-1841-4},
	abstract = {Please Read Notes: Brand New, International Softcover Edition, Printed in black and white pages, minor self wear on the cover or pages, Sale restriction may be printed on the book, but Book name, contents, and author are exactly same as Hardcover Edition. Fast delivery through DHL/FedEx express.},
	language = {Englisch},
	publisher = {PEARSON INDIA},
	author = {JURAFSKY, JAMES H. MARTIN DANIEL},
	month = jan,
	year = {2014},
}

@article{fruchter_divas_nodate,
	title = {{DIVAS}: {MULTIMEDIA} {INFORMATION} {RETRIEVAL} {FROM} {MULTIMODAL} {DESIGN} {AND} {CONSTRUCTION} {KNOWLEDGE} {CORPORA}},
	shorttitle = {{DIVAS}},
	url = {https://itc.scix.net/pdfs/w78-2006-tf84.pdf},
	urldate = {2025-03-25},
	author = {Fruchter, Renate and Yin, Zhen},
}

@article{boletsis_review_2020,
	title = {A review of automated speech-based interaction for cognitive screening},
	volume = {4},
	url = {https://www.mdpi.com/2414-4088/4/4/93},
	number = {4},
	urldate = {2025-03-25},
	journal = {Multimodal Technologies and Interaction},
	author = {Boletsis, Costas},
	year = {2020},
	note = {Publisher: MDPI},
	pages = {93},
}

@book{tucker_spontaneous_2023,
	title = {Spontaneous speech},
	url = {https://www.cambridge.org/core/elements/spontaneous-speech/403C69D68C0BC2C702067F971E7DB47A},
	urldate = {2025-03-25},
	publisher = {Cambridge University Press},
	author = {Tucker, Benjamin V. and Mukai, Yoichi},
	year = {2023},
}

@book{tannen_talking_2007,
	title = {Talking voices: {Repetition}, dialogue, and imagery in conversational discourse},
	volume = {26},
	shorttitle = {Talking voices},
	url = {https://books.google.de/books?hl=de&lr=&id=Mm3Yh17h7SIC&oi=fnd&pg=PR1&dq=alking+Voices:+Repetition,+Dialogue,+and+Imagery+in+Conversational+Discourse.&ots=ran9za33kN&sig=JrV96AiRmDAMU6Rwz-QT4aZeybI},
	urldate = {2025-03-25},
	publisher = {Cambridge University Press},
	author = {Tannen, Deborah},
	year = {2007},
}

@article{remez_perceptual_1991,
	title = {On the perceptual differentiation of spontaneous and prepared speech},
	volume = {89},
	url = {https://pubs.aip.org/asa/jasa/article-abstract/89/4B_Supplement/2011/657066},
	number = {4B\_Supplement},
	urldate = {2025-03-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Remez, Robert E. and Berus, Stefanie M. and Nutter, Jennifer S. and Dang, Jessica M. and Davachi, Lila and Rubin, Philip E.},
	year = {1991},
	note = {Publisher: Acoustical Society of America},
	pages = {2011--2012},
}

@article{gatiyatullina_investigating_2020,
	title = {Investigating the differences between prepared and spontaneous speech characteristics: {Descriptive} approach},
	volume = {9},
	shorttitle = {Investigating the differences between prepared and spontaneous speech characteristics},
	url = {https://www.academia.edu/download/80810024/IJCSV9A319_Gatiyatullina.pdf},
	urldate = {2025-03-25},
	journal = {International Journal of Criminology and Sociology},
	author = {Gatiyatullina, Galia and Gorodetskaya, Ludmila and Solnyshkina, Marina and Gafiyatova, Elzara},
	year = {2020},
	pages = {2591--2598},
}

@article{bernstein_spontaneous_1985,
	title = {Spontaneous versus prepared speech},
	volume = {78},
	url = {https://pubs.aip.org/asa/jasa/article-abstract/78/S1/S37/684354},
	number = {S1},
	urldate = {2025-03-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bernstein, Jared and Baldwin, Gay},
	year = {1985},
	note = {Publisher: Acoustical Society of America},
	pages = {S37--S37},
}

@article{bernstein_spontaneous_2005,
	title = {Spontaneous versus prepared speech},
	volume = {78},
	issn = {0001-4966},
	url = {https://doi.org/10.1121/1.2022783},
	doi = {10.1121/1.2022783},
	abstract = {Some speakers use different forms when training a speech recognizer than when speaking spontaneously to the device—this could be called “enrollment diglossia.” As a preliminary study of this phenomenon, we compared selected phonological and prosodic features of spontaneous speech to read and recited versions of the same sentences and paragraphs. Three subjects were interviewed and were later asked to read or memorize and recite, at various nominal rates, portions of the material that they had originally spoken spontaneously. We made detailed measurements of /t/ allophonics, speech rate, and the forms of certain words. For some speakers, there are considerable differences between spontaneous and prepared renditions; e.g., one speaker produced 45\% vs 18\% of /t/s as flaps, another speaker varies local speech rate and produces more nonsyntactic pauses in spontaneous paragraphs, and one speaker invokes “fast speech” forms at slower speech rates for spontaneous speech than for prepared speech. [Work supported by DARPA.]},
	number = {S1},
	urldate = {2025-03-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bernstein, Jared and Baldwin, Gay},
	month = aug,
	year = {2005},
	pages = {S37},
}

@article{scheunemann_orality_1996,
	title = {Orality, literacy, and modern media},
	url = {https://cir.nii.ac.jp/crid/1130282272141595904},
	urldate = {2025-03-25},
	journal = {(No Title)},
	author = {Scheunemann, Dietrich},
	year = {1996},
}

@article{havelock_gesprochener_1999,
	title = {Gesprochener {Laut} und geschriebenes {Zeichen}},
	journal = {Pias, Claus/Vogl, Joseph/Engell, Lorenz/Fahle, Oliver/Neitzel, Britta (Hg.): Kursbuch Medienkultur. Die maßgeblichen Theorien von Brecht bis Baudrillard. Stuttgart: DVA},
	author = {Havelock, Eric A.},
	year = {1999},
	pages = {81--94},
}

@article{havelock_orality_1989,
	series = {Special {Issue} {Transformations} of the {Word}},
	title = {Orality and literacy, an overview},
	volume = {9},
	issn = {0271-5309},
	url = {https://www.sciencedirect.com/science/article/pii/0271530989900116},
	doi = {10.1016/0271-5309(89)90011-6},
	abstract = {The following is a transcript made from a tape-recording of the talk which Eric Havelock gave at the Vassar conference, ‘Transformations of the Word’, in June 1987. Professor Havelock was at the time recovering from a kidney operation and never regained his health sufficiently to be able to read and correct the transcript. We have made minor changes and deletions, but the text stands as close as possible to the words which Professor Havelock delivered. We have added a list of references at the end consisting of works and authors which Professor Havelock mentioned in the course of his talk.},
	number = {2},
	urldate = {2025-03-25},
	journal = {Language \& Communication},
	author = {Havelock, E. A.},
	month = jan,
	year = {1989},
	pages = {87--98},
}

@article{best_walter_2020,
	title = {Walter {J}. {Ong}, {Orality} and {Literacy} (1982)},
	volume = {32},
	url = {https://read.dukeupress.edu/public-culture/article-abstract/32/2%20(91)/431/166076?casa_token=imlODhwSf-IAAAAA:rwFCBjAeifZcmTVby7aGqklKs6OaSY_iab20eFisqAMYUqSvY9_B0-2pyLTvIqpGCrmy1kA0aA&casa_token=_tVuhq52DLEAAAAA:fXbraASVL-WIY2K3ipHFm9R-wgl9TNTglau7klCQrQZ5Fxyyrsz_kEIw7HFdWVfEBc0mssnVRA},
	number = {2},
	urldate = {2025-03-25},
	journal = {Public Culture},
	author = {Best, Stephen},
	year = {2020},
	note = {Publisher: Duke University Press},
	pages = {431--439},
}

@inproceedings{mukherjee_ectsum_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{ECTSum}: {A} {New} {Benchmark} {Dataset} {For} {Bullet} {Point} {Summarization} of {Long} {Earnings} {Call} {Transcripts}},
	shorttitle = {{ECTSum}},
	url = {https://aclanthology.org/2022.emnlp-main.748/},
	doi = {10.18653/v1/2022.emnlp-main.748},
	abstract = {Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial documents, discussing facts and figures, have largely been unexplored, majorly due to the unavailability of suitable datasets. In this work, we present ECTSum, a new dataset with transcripts of earnings calls (ECTs), hosted by publicly traded companies, as documents, and experts-written short telegram-style bullet point summaries derived from corresponding Reuters articles. ECTs are long unstructured documents without any prescribed length limit or format. We benchmark our dataset with state-of-the-art summarization methods across various metrics evaluating the content quality and factual consistency of the generated summaries. Finally, we present a simple yet effective approach, ECT-BPS, to generate a set of bullet points that precisely capture the important facts discussed in the calls.},
	urldate = {2025-03-25},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mukherjee, Rajdeep and Bohra, Abhinav and Banerjee, Akash and Sharma, Soumya and Hegde, Manjunath and Shaikh, Afreen and Shrivastava, Shivani and Dasgupta, Koustuv and Ganguly, Niloy and Ghosh, Saptarshi and Goyal, Pawan},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {10893--10906},
}

@article{sadia_meeting_2024,
	title = {Meeting the challenge: {A} benchmark corpus for automated {Urdu} meeting summarization},
	volume = {61},
	issn = {0306-4573},
	url = {https://doi.org/10.1016/j.ipm.2024.103734},
	doi = {10.1016/j.ipm.2024.103734},
	number = {4},
	journal = {Inf. Process. Manage.},
	author = {Sadia, Bareera and Adeeba, Farah and Shams, Sana and Javed, Kashif},
	month = jul,
	year = {2024},
	note = {Place: USA
Publisher: Pergamon Press, Inc.},
	keywords = {Abstractive text summarization, Deep learning, Fine-tuning, Meeting corpus, Natural language processing, Urdu},
}

@incollection{kallmeyer_sprachvergessenheit_2000,
	title = {Die {Sprachvergessenheit} der {Medientheorie}. {Ein} {Plädoyer} für das {Medium} {Sprache}},
	copyright = {https://www.degruyter.com/dg/page/free-access-policy},
	isbn = {978-3-11-062265-2},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110622652-003/html},
	urldate = {2025-03-25},
	booktitle = {Sprache und {Neue} {Medien}},
	publisher = {De Gruyter},
	author = {Jäger, Ludwig},
	editor = {Kallmeyer, Werner},
	month = dec,
	year = {2000},
	doi = {10.1515/9783110622652-003},
	pages = {9--30},
}

@article{crawford_generative_2024,
	title = {Generative {AI}’s environmental costs are soaring — and mostly secret},
	volume = {626},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/d41586-024-00478-x},
	doi = {10.1038/d41586-024-00478-x},
	language = {en},
	number = {8000},
	urldate = {2025-03-25},
	journal = {Nature},
	author = {Crawford, Kate},
	month = feb,
	year = {2024},
	pages = {693--693},
}

@article{baum_artificial_2023,
	title = {Artificial {Intelligence} {Needs} {Environmental} {Ethics}},
	volume = {26},
	issn = {2155-0085, 2155-0093},
	url = {https://www.tandfonline.com/doi/full/10.1080/21550085.2022.2076538},
	doi = {10.1080/21550085.2022.2076538},
	language = {en},
	number = {1},
	urldate = {2025-03-25},
	journal = {Ethics, Policy \& Environment},
	author = {Baum, Seth D. and Owe, Andrea},
	month = jan,
	year = {2023},
	pages = {139--143},
}

@article{dauvergne_is_2022,
	title = {Is artificial intelligence greening global supply chains? {Exposing} the political economy of environmental costs},
	volume = {29},
	issn = {0969-2290, 1466-4526},
	shorttitle = {Is artificial intelligence greening global supply chains?},
	url = {https://www.tandfonline.com/doi/full/10.1080/09692290.2020.1814381},
	doi = {10.1080/09692290.2020.1814381},
	language = {en},
	number = {3},
	urldate = {2025-03-25},
	journal = {Review of International Political Economy},
	author = {Dauvergne, Peter},
	month = may,
	year = {2022},
	pages = {696--718},
}

@inproceedings{xie_using_2010,
	address = {Los Angeles, California},
	title = {Using {Confusion} {Networks} for {Speech} {Summarization}},
	url = {https://aclanthology.org/N10-1006/},
	urldate = {2025-03-24},
	booktitle = {Human {Language} {Technologies}: {The} 2010 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Xie, Shasha and Liu, Yang},
	editor = {Kaplan, Ron and Burstein, Jill and Harper, Mary and Penn, Gerald},
	month = jun,
	year = {2010},
	pages = {46--54},
}

@misc{whatsapp_making_2022,
	type = {Blog post},
	title = {Making {Voice} {Messages} {Better}},
	url = {https://blog.whatsapp.com/making-voice-messages-better},
	urldate = {2025-03-24},
	author = {{WhatsApp}},
	month = mar,
	year = {2022},
	note = {Publisher: WhatsApp},
}

@misc{ceci_hours_2024,
	title = {Hours of video uploaded to {YouTube} every minute 2007-2022},
	url = {https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/},
	urldate = {2025-03-24},
	author = {Ceci, Laura},
	month = apr,
	year = {2024},
	note = {Publisher: Statista},
}

@misc{noauthor_youtube_nodate,
	title = {{YouTube}: hours of video uploaded every minute 2022},
	shorttitle = {{YouTube}},
	url = {https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/},
	abstract = {YouTube is among the leading social video platforms in the global market. Around 500 hours of video are uploaded on YouTube every minute},
	language = {en},
	urldate = {2025-03-24},
	journal = {Statista},
}

@misc{litterer_mapping_2024,
	title = {Mapping the {Podcast} {Ecosystem} with the {Structured} {Podcast} {Research} {Corpus}},
	url = {http://arxiv.org/abs/2411.07892},
	doi = {10.48550/arXiv.2411.07892},
	abstract = {Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but rather includes audio features and speaker turns for a subset of 370K episodes, and speaker role inferences and other metadata for all 1.1M episodes. Using this data, we also conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Litterer, Benjamin and Jurgens, David and Card, Dallas},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07892 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{wu_vcsum_2023,
	address = {Toronto, Canada},
	title = {{VCSUM}: {A} {Versatile} {Chinese} {Meeting} {Summarization} {Dataset}},
	shorttitle = {{VCSUM}},
	url = {https://aclanthology.org/2023.findings-acl.377/},
	doi = {10.18653/v1/2023.findings-acl.377},
	abstract = {Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research.},
	urldate = {2025-03-24},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Han and Zhan, Mingjie and Tan, Haochen and Hou, Zhaohui and Liang, Ding and Song, Linqi},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {6065--6079},
}

@inproceedings{monteiro_towards_2023,
	address = {Cham},
	title = {Towards {End}-to-{End} {Speech}-to-{Text} {Summarization}},
	isbn = {978-3-031-40498-6},
	doi = {10.1007/978-3-031-40498-6_27},
	abstract = {Speech-to-text (S2T) summarization is a time-saving technique for filtering and keeping up with the broadcast news uploaded online on a daily basis. The rise of large language models from deep learning with impressive text generation capabilities has placed the research focus on summarization systems that produce paraphrased compact versions of the document content, also known as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive summarization is a promising approach that offers the possibility of generating rich latent representations that leverage non-verbal and acoustic information, as opposed to the use of only linguistic information from automatically generated transcripts in cascade systems. However, the few literature on E2E modelling of this task fails on exploring different domains, namely broadcast news, which is challenging domain where large and diversified volumes of data are presented to the user every day. We model S2T summarization both with a cascade and an E2E system for a corpus of broadcast news in French. Our novel E2E model leverages external data by resorting to transfer learning from a pre-trained T2T summarizer. Experiments show that both our cascade and E2E abstractive summarizers are stronger than an extractive baseline. However, the performance of the E2E model still lies behind the cascade one, which is object of an extensive analysis that includes future directions to close that gap.},
	language = {en},
	booktitle = {Text, {Speech}, and {Dialogue}},
	publisher = {Springer Nature Switzerland},
	author = {Monteiro, Raul and Pernes, Diogo},
	editor = {Ekštein, Kamil and Pártl, František and Konopík, Miloslav},
	year = {2023},
	keywords = {Abstractive summarization, End-to-end, Speech-to-text summarization},
	pages = {304--316},
}

@article{waibe11_chil_2005,
	title = {{CHIL}: {Computers} in the human interaction loop},
	author = {Waibe11, Alex and Steusloff, Hartwig and Stiefelhagen, Rainer and {others}},
	year = {2005},
}

@inproceedings{stiefelhagen_estimating_2001,
	title = {Estimating focus of attention based on gaze and sound},
	booktitle = {Proceedings of the 2001 workshop on {Perceptive} user interfaces},
	author = {Stiefelhagen, Rainer and Yang, Jie and Waibel, Alex},
	year = {2001},
	pages = {1--9},
}

@inproceedings{gross_towards_2000,
	title = {Towards a multimodal meeting record},
	volume = {3},
	booktitle = {2000 {IEEE} {International} {Conference} on {Multimedia} and {Expo}. {ICME2000}. {Proceedings}. {Latest} {Advances} in the {Fast} {Changing} {World} of {Multimedia} ({Cat}. {No}. {00TH8532})},
	publisher = {IEEE},
	author = {Gross, Ralph and Bett, Michael and Yu, Hua and Zhu, Xiaojin and Pan, Yue and Yang, Jie and Waibel, Alex},
	year = {2000},
	pages = {1593--1596},
}

@inproceedings{bett_multimodal_2000,
	title = {Multimodal {Meeting} {Tracker}.},
	booktitle = {{RIAO}},
	publisher = {Paris, France},
	author = {Bett, Michael and Gross, Ralph and Yu, Hua and Zhu, Xiaojin and Pan, Yue and Yang, Jie and Waibel, Alex},
	year = {2000},
	pages = {32--45},
}

@inproceedings{yang_visual_1998,
	title = {Visual tracking for multimodal human computer interaction},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems},
	author = {Yang, Jie and Stiefelhagen, Rainer and Meier, Uwe and Waibel, Alex},
	year = {1998},
	pages = {140--147},
}

@inproceedings{stiefelhagen_modeling_1999,
	title = {Modeling focus of attention for meeting indexing},
	booktitle = {Proceedings of the seventh {ACM} international conference on {Multimedia} ({Part} 1)},
	author = {Stiefelhagen, Rainer and Yang, Jie and Waibel, Alex},
	year = {1999},
	pages = {3--10},
}

@inproceedings{huang_swing_2023,
	address = {Dubrovnik, Croatia},
	title = {{SWING}: {Balancing} {Coverage} and {Faithfulness} for {Dialogue} {Summarization}},
	shorttitle = {{SWING}},
	url = {https://aclanthology.org/2023.findings-eacl.37/},
	doi = {10.18653/v1/2023.findings-eacl.37},
	abstract = {Missing information is a common issue of dialogue summarization where some information in the reference summaries is not covered in the generated summaries. To address this issue, we propose to utilize natural language inference (NLI) models to improve coverage while avoiding introducing factual inconsistencies. Specifically, we use NLI to compute fine-grained training signals to encourage the model to generate content in the reference summaries that have not been covered, as well as to distinguish between factually consistent and inconsistent generated sentences. Experiments on the DialogSum and SAMSum datasets confirm the effectiveness of the proposed approach in balancing coverage and faithfulness, validated with automatic metrics and human evaluations. Additionally, we compute the correlation between commonly used automatic metrics with human judgments in terms of three different dimensions regarding coverage and factual consistency to provide insight into the most suitable metric for evaluating dialogue summaries.},
	urldate = {2025-03-21},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Kung-Hsiang and Singh, Siffi and Ma, Xiaofei and Xiao, Wei and Nan, Feng and Dingwall, Nicholas and Wang, William Yang and McKeown, Kathleen},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {512--525},
}

@inproceedings{zhu_factual_2025,
	address = {Abu Dhabi, UAE},
	title = {Factual {Dialogue} {Summarization} via {Learning} from {Large} {Language} {Models}},
	url = {https://aclanthology.org/2025.coling-main.302/},
	abstract = {Factual consistency is an important quality in dialogue summarization. Large language model (LLM)-based automatic text summarization models generate more factually consistent summaries compared to those by smaller pretrained language models, but they face deployment challenges in real-world applications due to privacy or resource constraints. In this paper, we investigate the use of symbolic knowledge distillation to improve the factual consistency of smaller pretrained models for dialogue summarization. We employ zero-shot learning to extract symbolic knowledge from LLMs, generating both factually consistent (positive) and inconsistent (negative) summaries. We then apply two contrastive learning objectives on these summaries to enhance smaller summarization models. Experiments with BART, PEGASUS, and Flan-T5 indicate that our approach surpasses strong baselines that rely on complex data augmentation strategies. Our approach demonstrates improved factual consistency while preserving coherence, fluency, and relevance, as verified by both automatic evaluation metrics and human assessments. We provide access to the data and code to facilitate future research.},
	urldate = {2025-03-21},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Rongxin and Lau, Jey Han and Qi, Jianzhong},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {4474--4492},
}

@inproceedings{lima_luiz_evaluating_2025,
	title = {Evaluating {Snippet} {Significance}: {A} {Framework} for {Audio} and {Text}-{Based} {Dialogue} {Summarization}},
	doi = {10.1109/ICASSP49660.2025.10888790},
	booktitle = icassp # ", 2025",
	author = {Lima Luiz, Anderson de and Boddu, Raviteja and Georges, Munir},
	year = {2025},
	keywords = {Accuracy, Acoustics, Benchmark testing, Data mining, Distance measurement, Multimodal, Robustness, Signal processing, Speech processing, Web conferencing, embedding, significance score},
	pages = {1--5},
}

@inproceedings{fu_tiny_2024,
	address = {Mexico City, Mexico},
	title = {Tiny {Titans}: {Can} {Smaller} {Large} {Language} {Models} {Punch} {Above} {Their} {Weight} in the {Real} {World} for {Meeting} {Summarization}?},
	shorttitle = {Tiny {Titans}},
	url = {https://aclanthology.org/2024.naacl-industry.33/},
	doi = {10.18653/v1/2024.naacl-industry.33},
	abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, Compact LLMs are a good alternative to the comparatively Larger LLMs to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (FLAN-T5, TinyLLaMA, LiteLLaMA, etc.) with zero-shot larger LLMs (LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which achieves performance on par with zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient LLM for real-world industrial deployment.},
	urldate = {2025-03-21},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 6: {Industry} {Track})},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Xue-Yong and Laskar, Md Tahmid Rahman and Khasanova, Elena and Chen, Cheng and Tn, Shashi},
	editor = {Yang, Yi and Davani, Aida and Sil, Avi and Kumar, Anoop},
	month = jun,
	year = {2024},
	pages = {387--394},
}

@article{kirstein_cads_2025,
	title = {{CADS}: {A} {Systematic} {Literature} {Review} on the {Challenges} of {Abstractive} {Dialogue} {Summarization}},
	volume = {82},
	issn = {1076-9757},
	url = {https://doi.org/10.1613/jair.1.16674},
	doi = {10.1613/jair.1.16674},
	abstract = {Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although focused reviews have been conducted on this topic, there is a lack of comprehensive work that details the core challenges of dialogue summarization, unifies the differing understanding of the task, and aligns proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. Recent advances in training methods have led to substantial improvements in language-related challenges. However, challenges such as comprehension, factuality, and salience remain difficult and present significant research opportunities. We further investigate how these approaches are typically analyzed, covering the datasets for the subdomains of dialogue (e.g., meeting, customer service, and medical), the established automatic metrics (e.g., ROUGE), and common human evaluation approaches for assigning scores and evaluating annotator agreement. We observe that only a few datasets (i.e., SAMSum, AMI, DialogSum) are widely used. Despite its limitations, the ROUGE metric is the most commonly used, while human evaluation, considered the gold standard, is frequently reported without sufficient detail on the inter-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that our described challenge taxonomy remains relevant despite a potential shift in relevance and difficulty.},
	journal = {J. Artif. Int. Res.},
	author = {Kirstein, Frederic and Wahle, Jan Philip and Gipp, Bela and Ruas, Terry},
	month = feb,
	year = {2025},
	note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
}

@article{zhong_dialoglm_2022,
	title = {{DialogLM}: {Pre}-trained {Model} for {Long} {Dialogue} {Understanding} and {Summarization}},
	volume = {36},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21432},
	doi = {10.1609/aaai.v36i10.21432},
	number = {10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhong, Ming and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
	month = jun,
	year = {2022},
	pages = {11765--11773},
}

@misc{chu_qwen2-audio_2024,
	title = {Qwen2-{Audio} {Technical} {Report}},
	url = {http://arxiv.org/abs/2407.10759},
	doi = {10.48550/arXiv.2407.10759},
	abstract = {We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10759 [eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{liu_g-eval_2023,
	address = {Singapore},
	title = {G-{Eval}: {NLG} {Evaluation} using {Gpt}-4 with {Better} {Human} {Alignment}},
	shorttitle = {G-{Eval}},
	url = {https://aclanthology.org/2023.emnlp-main.153},
	doi = {10.18653/v1/2023.emnlp-main.153},
	language = {en},
	urldate = {2025-03-20},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
	year = {2023},
	pages = {2511--2522},
}

@inproceedings{merine_risks_2022,
	address = {Rochester, MN, USA},
	title = {Risks and {Benefits} of {AI}-generated {Text} {Summarization} for {Expert} {Level} {Content} in {Graduate} {Health} {Informatics}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6845-9},
	url = {https://ieeexplore.ieee.org/document/9874678/},
	doi = {10.1109/ICHI54592.2022.00113},
	urldate = {2025-03-20},
	booktitle = {2022 {IEEE} 10th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	publisher = {IEEE},
	author = {Merine, Regina and Purkayastha, Saptarshi},
	month = jun,
	year = {2022},
	pages = {567--574},
}

@misc{liu_responsible_2023,
	title = {Responsible {AI} {Considerations} in {Text} {Summarization} {Research}: {A} {Review} of {Current} {Practices}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Responsible {AI} {Considerations} in {Text} {Summarization} {Research}},
	url = {https://arxiv.org/abs/2311.11103},
	doi = {10.48550/ARXIV.2311.11103},
	abstract = {AI and NLP publication venues have increasingly encouraged researchers to reflect on possible ethical considerations, adverse impacts, and other responsible AI issues their work might engender. However, for specific NLP tasks our understanding of how prevalent such issues are, or when and why these issues are likely to arise, remains limited. Focusing on text summarization -- a common NLP task largely overlooked by the responsible AI community -- we examine research and reporting practices in the current literature. We conduct a multi-round qualitative analysis of 333 summarization papers from the ACL Anthology published between 2020-2022. We focus on how, which, and when responsible AI issues are covered, which relevant stakeholders are considered, and mismatches between stated and realized research goals. We also discuss current evaluation practices and consider how authors discuss the limitations of both prior work and their own work. Overall, we find that relatively few papers engage with possible stakeholders or contexts of use, which limits their consideration of potential downstream adverse impacts or other responsible AI issues. Based on our findings, we make recommendations on concrete practices and research directions.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Liu, Yu Lu and Cao, Meng and Blodgett, Su Lin and Cheung, Jackie Chi Kit and Olteanu, Alexandra and Trischler, Adam},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{goyal_news_2023,
	title = {News {Summarization} and {Evaluation} in the {Era} of {GPT}-3},
	url = {http://arxiv.org/abs/2209.12356},
	doi = {10.48550/arXiv.2209.12356},
	abstract = {The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting. To support further research, we release: (a) a corpus of 10K generated summaries from fine-tuned and prompt-based models across 4 standard summarization benchmarks, (b) 1K human preference judgments comparing different systems for generic- and keyword-based summarization.},
	language = {en},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
	month = may,
	year = {2023},
	note = {arXiv:2209.12356 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mastropaolo_evaluating_2023,
	title = {Evaluating {Code} {Summarization} {Techniques}: {A} {New} {Metric} and an {Empirical} {Characterization}},
	shorttitle = {Evaluating {Code} {Summarization} {Techniques}},
	url = {http://arxiv.org/abs/2312.15475},
	doi = {10.48550/arXiv.2312.15475},
	abstract = {Several code summarization techniques have been proposed in the literature to automatically document a code snippet or a function. Ideally, software developers should be involved in assessing the quality of the generated summaries. However, in most cases, researchers rely on automatic evaluation metrics such as BLEU, ROUGE, and METEOR. These metrics are all based on the same assumption: The higher the textual similarity between the generated summary and a reference summary written by developers, the higher its quality. However, there are two reasons for which this assumption falls short: (i) reference summaries, e.g., code comments collected by mining software repositories, may be of low quality or even outdated; (ii) generated summaries, while using a different wording than a reference one, could be semantically equivalent to it, thus still being suitable to document the code snippet. In this paper, we perform a thorough empirical investigation on the complementarity of different types of metrics in capturing the quality of a generated summary. Also, we propose to address the limitations of existing metrics by considering a new dimension, capturing the extent to which the generated summary aligns with the semantics of the documented code snippet, independently from the reference summary. To this end, we present a new metric based on contrastive learning to capture said aspect. We empirically show that the inclusion of this novel dimension enables a more effective representation of developers’ evaluations regarding the quality of automatically generated summaries.},
	language = {en},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Mastropaolo, Antonio and Ciniselli, Matteo and Penta, Massimiliano Di and Bavota, Gabriele},
	month = dec,
	year = {2023},
	note = {arXiv:2312.15475 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{venkataramana_abstractive_2022,
	title = {Abstractive text summarization using bart},
	url = {https://ieeexplore.ieee.org/abstract/document/9972639/?casa_token=_gZ_gHPH5OoAAAAA:zd5vLvgu9Fsc87Kqb49DULBtkvkz0ulcFgW1doJsosTBnWsJGmOziE5mGXzfjKas177cthp4a8w},
	urldate = {2025-03-19},
	booktitle = {2022 {IEEE} 2nd {Mysore} {Sub} {Section} {International} {Conference} ({MysuruCon})},
	publisher = {IEEE},
	author = {Venkataramana, Attada and Srividya, K. and Cristin, R.},
	year = {2022},
	pages = {1--6},
}

@inproceedings{iwasaki_japanese_2019,
	title = {Japanese abstractive text summarization using {BERT}},
	url = {https://ieeexplore.ieee.org/abstract/document/8959920/?casa_token=6ItSLXuNC_4AAAAA:Dgazwd1IrBH5KkwvZip2Vr81S9A0Fl4e9gyLP_dgt6PqCb4kznIVBxojS-AjDd6C1jnz8StthDc},
	urldate = {2025-03-19},
	booktitle = {2019 {International} {Conference} on {Technologies} and {Applications} of {Artificial} {Intelligence} ({TAAI})},
	publisher = {IEEE},
	author = {Iwasaki, Yuuki and Yamashita, Akihiro and Konno, Yoko and Matsubayashi, Katsushi},
	year = {2019},
	pages = {1--5},
}

@misc{aksenov_abstractive_2020,
	title = {Abstractive {Text} {Summarization} based on {Language} {Model} {Conditioning} and {Locality} {Modeling}},
	url = {http://arxiv.org/abs/2003.13027},
	doi = {10.48550/arXiv.2003.13027},
	abstract = {We explore to what extent knowledge about the pre-trained language model that is used is beneficial for the task of abstractive summarization. To this end, we experiment with conditioning the encoder and decoder of a Transformer-based neural model on the BERT language model. In addition, we propose a new method of BERT-windowing, which allows chunk-wise processing of texts longer than the BERT window size. We also explore how locality modelling, i.e., the explicit restriction of calculations to the local context, can affect the summarization ability of the Transformer. This is done by introducing 2-dimensional convolutional self-attention into the first layers of the encoder. The results of our models are compared to a baseline and the state-of-the-art models on the CNN/Daily Mail dataset. We additionally train our model on the SwissText dataset to demonstrate usability on German. Both models outperform the baseline in ROUGE scores on two datasets and show its superiority in a manual qualitative analysis.},
	urldate = {2025-03-19},
	publisher = {arXiv},
	author = {Aksenov, Dmitrii and Moreno-Schneider, Julián and Bourgonje, Peter and Schwarzenberg, Robert and Hennig, Leonhard and Rehm, Georg},
	month = mar,
	year = {2020},
	note = {arXiv:2003.13027 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{kano_speech_2023,
	title = {Speech {Summarization} of {Long} {Spoken} {Document}: {Improving} {Memory} {Efficiency} of {Speech}/{Text} {Encoders}},
	doi = {10.1109/ICASSP49357.2023.10095019},
	booktitle = icassp # ", 2023",
	author = {Kano, Takatomo and Ogawa, Atsunori and Delcroix, Marc and Sharma, Roshan and Matsuura, Kohei and Watanabe, Shinji},
	year = {2023},
	keywords = {Computational efficiency, Computational modeling, Memory management, Signal processing, Speech processing, Task analysis, Video on demand, dual speech/text encoder, end-to-end modeling, long spoken document, memory efficient encoders},
	pages = {1--5},
}

@article{rath_formation_1961,
	title = {The formation of abstracts by the selection of sentences. {Part} {I}. {Sentence} selection by men and machines},
	volume = {12},
	number = {2},
	journal = {American Documentation},
	author = {Rath, G. J. and Resnick, A. and Savage, Terry R.},
	year = {1961},
	pages = {139--141},
}

@inproceedings{sharma_bass_2023,
	title = {{BASS}: {Block}-wise {Adaptation} for {Speech} {Summarization}},
	doi = {10.21437/Interspeech.2023-916},
	booktitle = interspeech # " 2023",
	author = {Sharma, Roshan and Arora, Siddhant and Zheng, Kenneth and Watanabe, Shinji and Singh, Rita and Raj, Bhiksha},
	year = {2023},
	note = {ISSN: 2958-1796},
	pages = {1454--1458},
}

@inproceedings{sharma_r-bass_2024,
	address = {Mexico City, Mexico},
	title = {R-{BASS} : {Relevance}-aided {Block}-wise {Adaptation} for {Speech} {Summarization}},
	url = {https://aclanthology.org/2024.findings-naacl.54/},
	doi = {10.18653/v1/2024.findings-naacl.54},
	abstract = {End-to-end speech summarization on long recordings is challenging because of the high computational cost. Block-wise Adaptation for Speech Summarization (BASS) summarizes arbitrarily long sequences by sequentially processing abutting chunks of audio. Despite the benefits of BASS, it has higher compute time due to sequential processing of all blocks, regardless of whether they are relevant to the final summary. In this paper, we propose R-BASS, a new relevance-aware block-wise adaptation method. First, we introduce two approaches to automatically estimate block relevance based on lexical and semantic similarity between the block-level transcript and the summary. Experiments on the How2 dataset show that using ground truth relevance during inference improves efficiency by 63.9 \% by dropping irrelevant blocks. Finally, we incorporate relevance scores into training using a novel relevance loss and relevance predictor, and the proposed R-BASS model makes it possible to drop 86.3 \% of the blocks while retaining comparable performance, resulting in a 2.2x speedup over BASS.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Roshan and Sharma, Ruchira and Dhamyal, Hira and Singh, Rita and Raj, Bhiksha},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {848--857},
}

@misc{noauthor_r-bass_nodate,
	title = {R-{BASS} : {Relevance}-aided {Block}-wise {Adaptation} for {Speech} {Summarization} - {ACL} {Anthology}},
	url = {https://aclanthology.org/2024.findings-naacl.54/},
	urldate = {2025-03-19},
}

@misc{microsoft_phi-4-mini_2025,
	title = {Phi-4-{Mini} {Technical} {Report}: {Compact} yet {Powerful} {Multimodal} {Language} {Models} via {Mixture}-of-{LoRAs}},
	shorttitle = {Phi-4-{Mini} {Technical} {Report}},
	url = {http://arxiv.org/abs/2503.01743},
	doi = {10.48550/arXiv.2503.01743},
	abstract = {We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.},
	urldate = {2025-03-19},
	publisher = {arXiv},
	author = {Microsoft and Abouelenin, Abdelrahman and Ashfaq, Atabak and Atkinson, Adam and Awadalla, Hany and Bach, Nguyen and Bao, Jianmin and Benhaim, Alon and Cai, Martin and Chaudhary, Vishrav and Chen, Congcong and Chen, Dong and Chen, Dongdong and Chen, Junkun and Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-ling and Dai, Qi and Dai, Xiyang and Fan, Ruchao and Gao, Mei and Gao, Min and Garg, Amit and Goswami, Abhishek and Hao, Junheng and Hendy, Amr and Hu, Yuxuan and Jin, Xin and Khademi, Mahmoud and Kim, Dongwoo and Kim, Young Jin and Lee, Gina and Li, Jinyu and Li, Yunsheng and Liang, Chen and Lin, Xihui and Lin, Zeqi and Liu, Mengchen and Liu, Yang and Lopez, Gilsinia and Luo, Chong and Madan, Piyush and Mazalov, Vadim and Mitra, Arindam and Mousavi, Ali and Nguyen, Anh and Pan, Jing and Perez-Becker, Daniel and Platin, Jacob and Portet, Thomas and Qiu, Kai and Ren, Bo and Ren, Liliang and Roy, Sambuddha and Shang, Ning and Shen, Yelong and Singhal, Saksham and Som, Subhojit and Song, Xia and Sych, Tetyana and Vaddamanu, Praneetha and Wang, Shuohang and Wang, Yiming and Wang, Zhenghao and Wu, Haibin and Xu, Haoran and Xu, Weijian and Yang, Yifan and Yang, Ziyi and Yu, Donghan and Zabir, Ishmam and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yunan and Zhou, Xiren},
	month = mar,
	year = {2025},
	note = {arXiv:2503.01743 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{liu_adapting_2020,
	address = {Online},
	title = {Adapting {End}-to-{End} {Speech} {Recognition} for {Readable} {Subtitles}},
	url = {https://aclanthology.org/2020.iwslt-1.30/},
	doi = {10.18653/v1/2020.iwslt-1.30},
	abstract = {Automatic speech recognition (ASR) systems are primarily evaluated on transcription accuracy. However, in some use cases such as subtitling, verbatim transcription would reduce output readability given limited screen size and reading time. Therefore, this work focuses on ASR with output compression, a task challenging for supervised approaches due to the scarcity of training data. We first investigate a cascaded system, where an unsupervised compression model is used to post-edit the transcribed speech. We then compare several methods of end-to-end speech recognition under output length constraints. The experiments show that with limited data far less than needed for training a model from scratch, we can adapt a Transformer-based ASR model to incorporate both transcription and compression capabilities. Furthermore, the best performance in terms of WER and ROUGE scores is achieved by explicitly modeling the length constraints within the end-to-end ASR system.},
	urldate = {2025-03-18},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Spoken} {Language} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Danni and Niehues, Jan and Spanakis, Gerasimos},
	editor = {Federico, Marcello and Waibel, Alex and Knight, Kevin and Nakamura, Satoshi and Ney, Hermann and Niehues, Jan and Stüker, Sebastian and Wu, Dekai and Mariani, Joseph and Yvon, Francois},
	month = jul,
	year = {2020},
	pages = {247--256},
}

@book{goodman_sonic_2012,
	address = {Cambridge, Mass. London},
	series = {Technologies of lived abstraction},
	title = {Sonic warfare: sound, affect, and the ecology of fear},
	isbn = {978-0-262-51795-9 978-0-262-01347-5},
	shorttitle = {Sonic warfare},
	language = {eng},
	publisher = {MIT Press},
	author = {Goodman, Steve},
	year = {2012},
}

@phdthesis{akiyama_phongraphic_2014,
	type = {{PhD} thesis},
	title = {The {Phongraphic} {Memory}. {A} {History} of {Sound} {Recording} in the {Field}},
	url = {https://escholarship.mcgill.ca/concern/theses/dr26z121r},
	urldate = {2025-03-06},
	school = {McGill University},
	author = {Akiyama, Mitchell},
	year = {2014},
}

@article{jelinek_continuous_1976,
	title = {Continuous speech recognition by statistical methods},
	volume = {64},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1454428/},
	doi = {10.1109/PROC.1976.10159},
	number = {4},
	urldate = {2025-03-10},
	journal = {Proceedings of the IEEE},
	author = {Jelinek, Frederick},
	year = {1976},
	pages = {532--556},
}

@article{jelinek_continuous_1976-1,
	title = {Continuous speech recognition by statistical methods},
	volume = {64},
	doi = {10.1109/PROC.1976.10159},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Jelinek, F.},
	year = {1976},
	keywords = {Acoustic devices, Automatic speech recognition, Decoding, Loudspeakers, Natural languages, Signal processing, Speech processing, Speech recognition, Statistical analysis, Statistics},
	pages = {532--556},
}

@inproceedings{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2014, {December} 8-13 2014, {Montreal}, {Quebec}, {Canada}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
	year = {2014},
	pages = {3104--3112},
}

@inproceedings{zhang_bertscore_2020,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	url = {https://openreview.net/forum?id=SkeHuCVFDr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	year = {2020},
}

@article{adelmann_4_2024,
	title = {4. {Medienwissenschaftliches} {Symposium}-{Filter}},
	url = {https://mediarep.org/entities/misc/c3e086a7-382f-4afe-8a15-f6bda5fba6d5},
	urldate = {2025-03-18},
	author = {Adelmann, Ralf and Matzner, Tobias},
	year = {2024},
	note = {Publisher: Universität Paderborn},
}

@book{noauthor_filter_2024,
	title = {Filter – {Medienwissenschaftliche} {Symposien} der {DFG}},
	url = {https://ris.uni-paderborn.de/record/51764},
	language = {ger},
	urldate = {2025-03-18},
	year = {2024},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013/},
	urldate = {2025-03-17},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
}

@article{baum_maximization_1970,
	title = {A maximization technique occurring in the statistical analysis of probabilistic functions of {Markov} chains},
	volume = {41},
	issn = {0003-4851},
	number = {1},
	journal = {The annals of mathematical statistics},
	author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
	year = {1970},
	note = {Publisher: JSTOR},
	pages = {164--171},
        url = {https://www.jstor.org/stable/2239727},
}

@article{zhang_benchmarking_2024,
	title = {Benchmarking {Large} {Language} {Models} for {News} {Summarization}},
	volume = {12},
	url = {https://aclanthology.org/2024.tacl-1.3/},
	doi = {10.1162/tacl_a_00632},
	abstract = {Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM`s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.},
	urldate = {2025-03-17},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang, Percy and McKeown, Kathleen and Hashimoto, Tatsunori B.},
	year = {2024},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {39--57},
}

@article{singhal_toward_2025,
	title = {Toward expert-level medical question answering with large language models},
	copyright = {2025 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-024-03423-7},
	doi = {10.1038/s41591-024-03423-7},
	abstract = {Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the first to exceed a ‘passing’ score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workflows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fine-tuning and new strategies for improving reasoning and grounding through ensemble refinement and chain of retrieval. Med-PaLM 2 scores up to 86.5\% on the MedQA dataset, improving upon Med-PaLM by over 19\%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates significant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P {\textless} 0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65\% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications.},
	language = {en},
	urldate = {2025-03-17},
	journal = {Nature Medicine},
	author = {Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Amin, Mohamed and Hou, Le and Clark, Kevin and Pfohl, Stephen R. and Cole-Lewis, Heather and Neal, Darlene and Rashid, Qazi Mamunur and Schaekermann, Mike and Wang, Amy and Dash, Dev and Chen, Jonathan H. and Shah, Nigam H. and Lachgar, Sami and Mansfield, Philip Andrew and Prakash, Sushant and Green, Bradley and Dominowska, Ewa and Agüera y Arcas, Blaise and Tomašev, Nenad and Liu, Yun and Wong, Renee and Semturs, Christopher and Mahdavi, S. Sara and Barral, Joelle K. and Webster, Dale R. and Corrado, Greg S. and Matias, Yossi and Azizi, Shekoofeh and Karthikesalingam, Alan and Natarajan, Vivek},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Health care, Medical research},
	pages = {1--8},
}

@misc{dubey_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-03-17},
	publisher = {arXiv},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{nguyen_msa-asr_2025,
	title = {{MSA}-{ASR}: {Efficient} {Multilingual} {Speaker} {Attribution} with frozen {ASR} {Models}},
	shorttitle = {{MSA}-{ASR}},
	url = {http://arxiv.org/abs/2411.18152},
	doi = {10.48550/arXiv.2411.18152},
	abstract = {Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe speech while assigning transcripts to the corresponding speakers accurately. Existing methods often rely on complex modular systems or require extensive fine-tuning of joint modules, limiting their adaptability and general efficiency. This paper introduces a novel approach, leveraging a frozen multilingual ASR model to incorporate speaker attribution into the transcriptions, using only standard monolingual ASR datasets. Our method involves training a speaker module to predict speaker embeddings based on weak labels without requiring additional ASR model modifications. Despite being trained exclusively with non-overlapping monolingual data, our approach effectively extracts speaker attributes across diverse multilingual datasets, including those with overlapping speech. Experimental results demonstrate competitive performance compared to strong baselines, highlighting the model's robustness and potential for practical applications.},
	urldate = {2025-03-17},
	publisher = {arXiv},
	author = {Nguyen, Thai-Binh and Waibel, Alexander},
	month = jan,
	year = {2025},
	note = {arXiv:2411.18152 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{noauthor_241118152_nodate,
	title = {[2411.18152] {MSA}-{ASR}: {Efficient} {Multilingual} {Speaker} {Attribution} with frozen {ASR} {Models}},
	url = {https://arxiv.org/abs/2411.18152},
	urldate = {2025-03-17},
}

@inproceedings{nguyen_super-human_2021,
	title = {Super-{Human} {Performance} in {Online} {Low}-{Latency} {Recognition} of {Conversational} {Speech}},
	doi = {10.21437/Interspeech.2021-1114},
	booktitle = interspeech # " 2021",
	author = {Nguyen, Thai-Son and Stüker, Sebastian and Waibel, Alex},
	year = {2021},
	note = {ISSN: 2958-1796},
	pages = {1762--1766},
}

@inproceedings{hori_automatic_2002,
	title = {Automatic speech summarization applied to {English} broadcast news speech},
	volume = {1},
	booktitle = {2002 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hori, Chiori and Furui, Sadaoki and Malkin, Rob and Yu, Hua and Waibel, Alex},
	year = {2002},
	pages = {I--9},
}

@inproceedings{devlin_bert_2019,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	url = {https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC},
	urldate = {2025-03-15},
	booktitle = {Proceedings of the 2019 conference of the {North} {American} chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{devlin_bert_2019-1,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	url = {https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC},
	urldate = {2025-03-15},
	booktitle = {Proceedings of the 2019 conference of the {North} {American} chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{devlin_bert_2019-2,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	url = {https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC},
	urldate = {2025-03-15},
	booktitle = {Proceedings of the 2019 conference of the {North} {American} chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2025-03-14},
	journal = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{nallapati_abstractive_2016,
	title = {Abstractive {Text} {Summarization} {Using} {Sequence}-to-{Sequence} {RNNs} and {Beyond}},
	url = {http://arxiv.org/abs/1602.06023},
	doi = {10.48550/arXiv.1602.06023},
	abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Nallapati, Ramesh and Zhou, Bowen and santos, Cicero Nogueira dos and Gulcehre, Caglar and Xiang, Bing},
	month = aug,
	year = {2016},
	note = {arXiv:1602.06023 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_fast_2018,
	title = {Fast {Abstractive} {Summarization} with {Reinforce}-{Selected} {Sentence} {Rewriting}},
	url = {http://arxiv.org/abs/1805.11080},
	doi = {10.48550/arXiv.1805.11080},
	abstract = {Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Chen, Yen-Chun and Bansal, Mohit},
	month = may,
	year = {2018},
	note = {arXiv:1805.11080 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{nallapati_abstractive_2016-1,
	title = {Abstractive {Text} {Summarization} {Using} {Sequence}-to-{Sequence} {RNNs} and {Beyond}},
	url = {http://arxiv.org/abs/1602.06023},
	doi = {10.48550/arXiv.1602.06023},
	abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Nallapati, Ramesh and Zhou, Bowen and santos, Cicero Nogueira dos and Gulcehre, Caglar and Xiang, Bing},
	month = aug,
	year = {2016},
	note = {arXiv:1602.06023 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{chen_integrating_2012,
	title = {Integrating intra-speaker topic modeling and temporal-based inter-speaker topic modeling in random walk for improved multi-party meeting summarization},
	url = {https://kilthub.cmu.edu/articles/Integrating_Intra-Speaker_Topic_Modeling_and_Temporal-Based_Inter-Speaker_Topic_Modeling_in_Random_Walk_for_Improved_Multi-Party_Meeting_Summarization/6473453/files/11903057.pdf},
	urldate = {2025-03-14},
	author = {Chen, Yun-Nung and Metze, Florian},
	year = {2012},
	note = {Publisher: Carnegie Mellon University},
}

@incollection{philo_hunting_2004,
	address = {Hoboken},
	series = {Critical {Geographies}},
	title = {‘{Hunting} with the camera’ {Photography}, {Wildlife} and {Colonialism} in {Africa}},
	isbn = {978-0-203-00488-3 978-0-415-19846-2},
	language = {eng},
	booktitle = {Animal {Spaces}, {Beastly} {Places}},
	publisher = {Taylor and Francis},
	author = {Ryan, James},
	editor = {Philo, Chris and Wilbert, Chris},
	year = {2004},
	pages = {205--222},
}

@inproceedings{hirao_extracting_2002,
	title = {Extracting important sentences with support vector machines},
	url = {https://aclanthology.org/C02-1053.pdf},
	urldate = {2025-03-14},
	booktitle = {{COLING} 2002: the 19th international conference on computational linguistics},
	author = {Hirao, Tsutomu and Isozaki, Hideki and Maeda, Eisaku and Matsumoto, Yuji},
	year = {2002},
}

@inproceedings{maskey_comparing_2005,
	title = {Comparing lexical, acoustic/prosodic, structural and discourse features for speech summarization.},
	url = {http://webcluster.cs.columbia.edu/~julia/files/eurospeech05_vfinal.pdf},
	urldate = {2025-03-14},
	booktitle = interspeech # " 2005",
	author = {Maskey, Sameer and Hirschberg, Julia},
	year = {2005},
	pages = {621--624},
}

@article{koumpis_automatic_2005,
	title = {Automatic summarization of voicemail messages using lexical and prosodic features},
	volume = {2},
	issn = {1550-4875, 1550-4883},
	url = {https://dl.acm.org/doi/10.1145/1075389.1075390},
	doi = {10.1145/1075389.1075390},
	abstract = {This aticle presents trainable methods for extracting principal content words from voicemail messages. The short text summaries generated are suitable for mobile messaging applications. The system uses a set of classifiers to identify the summary words with each word described by a vector of lexical and prosodic features. We use an ROC-based algorithm, Parcel, to select input features (and classifiers). We have performed a series of objective and subjective evaluations using unseen data from two different speech recognition systems as well as human transcriptions of voicemail speech.},
	language = {en},
	number = {1},
	urldate = {2025-03-14},
	journal = {ACM Transactions on Speech and Language Processing},
	author = {Koumpis, Konstantinos and Renals, Steve},
	month = feb,
	year = {2005},
	pages = {1},
}

@inproceedings{chin-yew_rouge_2004,
	title = {Rouge: {A} package for automatic evaluation of summaries},
	shorttitle = {Rouge},
	url = {https://cir.nii.ac.jp/crid/1571417125576321408},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {Workshop} on {Text} {Summarization} {Branches} {Out}, 2004},
	author = {Chin-Yew, Lin},
	year = {2004},
}

@inproceedings{hori_speech_2003,
	title = {Speech summarization using weighted finite-state transducers.},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3f402c84b6f4e011fc76cff8856f8c14e06e4ae2},
	urldate = {2025-03-14},
	booktitle = interspeech # " 2003",
	publisher = {Citeseer},
	author = {Hori, Takaaki and Hori, Chiori and Minami, Yasuhiro},
	year = {2003},
	pages = {2817--2820},
}

@inproceedings{mckeown_text_2005,
	title = {From text to speech summarization},
	volume = {5},
	url = {https://ieeexplore.ieee.org/abstract/document/1416474/?casa_token=GNuig5G9Q3MAAAAA:zMt353GcAPVj-N-z5zDreBVe6ZeYwE8qVKwQILMVlXrGvgzLfhEX4EnzRcrlYB6-nkfIj_pCJ4Y},
	urldate = {2025-03-14},
	booktitle = icassp # ", 2005",
	publisher = {IEEE},
	author = {McKeown, Kathleen and Hirschberg, Julia and Galley, Michel and Maskey, Sameer},
	year = {2005},
	pages = {v--997},
}

@article{jelinek_design_1975,
	title = {Design of a linguistic statistical decoder for the recognition of continuous speech},
	volume = {21},
	url = {https://ieeexplore.ieee.org/abstract/document/1055384/?casa_token=QH6vjppJV1gAAAAA:D5z1UV4rXrJ494uAoTgXEjbjcyJHXijWRESxpDZ-lbRWjWu4XPDPKGR0_3Sq7juHn5ZEw2li2Uk},
	number = {3},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Information Theory},
	author = {Jelinek, Frederick and Bahl, Lalit and Mercer, Robert},
	year = {1975},
	note = {Publisher: IEEE},
	pages = {250--256},
}

@inproceedings{chen_use_1992,
	address = {San Francisco, CA, USA},
	title = {The use of emphasis to automatically summarize a spoken discourse},
	isbn = {978-0-7803-0532-8},
	url = {http://ieeexplore.ieee.org/document/225930/},
	doi = {10.1109/ICASSP.1992.225930},
	urldate = {2025-03-14},
	booktitle = icassp # ", 1992",
	publisher = {IEEE},
	author = {Chen, F.R. and Withgott, M.},
	year = {1992},
	pages = {229--232 vol.1},
}

@inproceedings{whittaker_scan_1999,
	title = {{SCAN}: {Designing} and evaluating user interfaces to support retrieval from speech archives},
	author = {Whittaker, Steve and Hirschberg, Julia and Choi, John and Hindle, Don and Pereira, Fernando and Singhal, Amit},
	year = {1999},
        booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {26--33},
        series = {SIGIR '99},
        url = {https://doi.org/10.1145/312624.312639},
}

@inproceedings{gee_tipster_1998,
	address = {Baltimore, Maryland},
	title = {The {TIPSTER} text program overview},
	booktitle = {{TIPSTER} {Text} {Program} {Phase} {III}: {Proceedings} of a {Workshop} held at {Baltimore}, {Maryland}, {October} 13-15, 1998},
	author = {Gee, F Ruth},
	year = {1998},
	pages = {3--5},
        url = {https://dl.acm.org/doi/abs/10.3115/1119089.1119091},
}

@inproceedings{zhang_exploratory_2021,
	address = {Punta Cana, Dominican Republic},
	title = {An {Exploratory} {Study} on {Long} {Dialogue} {Summarization}: {What} {Works} and {What}`s {Next}},
	shorttitle = {An {Exploratory} {Study} on {Long} {Dialogue} {Summarization}},
	url = {https://aclanthology.org/2021.findings-emnlp.377/},
	doi = {10.18653/v1/2021.findings-emnlp.377},
	abstract = {Dialogue summarization helps readers capture salient information from long conversations in meetings, interviews, and TV series. However, real-world dialogues pose a great challenge to current summarization models, as the dialogue length typically exceeds the input limits imposed by recent transformer-based pre-trained models, and the interactive nature of dialogues makes relevant information more context-dependent and sparsely distributed than news articles. In this work, we perform a comprehensive study on long dialogue summarization by investigating three strategies to deal with the lengthy input problem and locate relevant information: (1) extended transformer models such as Longformer, (2) retrieve-then-summarize pipeline models with several dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding models such as HMNet. Our experimental results on three long dialogue datasets (QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline models yield the best performance. We also demonstrate that the summary quality can be further improved with a stronger retrieval model and pretraining on proper external summarization datasets.},
	urldate = {2025-03-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yusen and Ni, Ansong and Yu, Tao and Zhang, Rui and Zhu, Chenguang and Deb, Budhaditya and Celikyilmaz, Asli and Awadallah, Ahmed Hassan and Radev, Dragomir},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4426--4433},
}

@inproceedings{zhu_hierarchical_2020,
	address = {Online},
	title = {A {Hierarchical} {Network} for {Abstractive} {Meeting} {Summarization} with {Cross}-{Domain} {Pretraining}},
	url = {https://aclanthology.org/2020.findings-emnlp.19/},
	doi = {10.18653/v1/2020.findings-emnlp.19},
	abstract = {With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66\% to 46.28\%.},
	urldate = {2025-03-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Chenguang and Xu, Ruochen and Zeng, Michael and Huang, Xuedong},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {194--203},
}

@inproceedings{matsuura_transfer_2023,
	title = {Transfer {Learning} from {Pre}-trained {Language} {Models} {Improves} {End}-to-{End} {Speech} {Summarization}},
	doi = {10.21437/Interspeech.2023-1307},
	booktitle = interspeech # " 2023",
	author = {Matsuura, Kohei and Ashihara, Takanori and Moriya, Takafumi and Tanaka, Tomohiro and Kano, Takatomo and Ogawa, Atsunori and Delcroix, Marc},
	year = {2023},
	note = {ISSN: 2958-1796},
	pages = {2943--2947},
}

@inproceedings{matsuura_leveraging_2023,
	title = {Leveraging {Large} {Text} {Corpora} {For} {End}-{To}-{End} {Speech} {Summarization}},
	doi = {10.1109/ICASSP49357.2023.10094993},
	booktitle = icassp # ", 2023",
	author = {Matsuura, Kohei and Ashihara, Takanori and Moriya, Takafumi and Tanaka, Tomohiro and Ogawa, Atsunori and Delcroix, Marc and Masumura, Ryo},
	year = {2023},
	keywords = {Acoustics, Automatic speech recognition, End-to-end speech summarization, How2 dataset, Measurement, Signal processing, Speech processing, Training, Training data, multi-modal data augmentation, synthetic data augmentation},
	pages = {1--5},
}

@inproceedings{sharma_end--end_2022,
	title = {End-to-{End} {Speech} {Summarization} {Using} {Restricted} {Self}-{Attention}},
	doi = {10.1109/ICASSP43922.2022.9747320},
	booktitle = icassp # ", 2022",
	author = {Sharma, Roshan and Palaskar, Shruti and Black, Alan W and Metze, Florian},
	year = {2022},
	keywords = {Computational modeling, Memory management, Predictive models, Signal processing, Speech recognition, Text recognition, Transformers, concept learning, end-to-end, long sequence modeling, speech summarization},
	pages = {8072--8076},
}

@article{furui_speech--text_2004,
	title = {Speech-to-text and speech-to-speech summarization of spontaneous speech},
	volume = {12},
	doi = {10.1109/TSA.2004.828699},
	number = {4},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Furui, S. and Kikuchi, T. and Shinnaka, Y. and Hori, C.},
	year = {2004},
	keywords = {Broadcasting, Compaction, Concatenated codes, Data mining, Laboratories, Natural languages, Speech recognition, Speech synthesis, Synthesizers, Text recognition},
	pages = {401--408},
}

@inproceedings{wang_essumm_2022,
	title = {{ESSumm}: {Extractive} {Speech} {Summarization} from {Untranscribed} {Meeting}},
	url = {https://doi.org/10.21437/Interspeech.2022-945},
	doi = {10.21437/INTERSPEECH.2022-945},
	booktitle = interspeech # " 2022",
	publisher = {ISCA},
	author = {Wang, Jun},
	editor = {Ko, Hanseok and Hansen, John H. L.},
	year = {2022},
	pages = {3243--3247},
}

@inproceedings{jorgensen_cross-lingual_2025,
	address = {Abu Dhabi, UAE},
	title = {Cross-{Lingual} {Sentence} {Compression} for {Length}-{Constrained} {Subtitles} in {Low}-{Resource} {Settings}},
	url = {https://aclanthology.org/2025.coling-main.429/},
	abstract = {This paper explores the joint task of machine translation and sentence compression, emphasizing its application in subtitle generation for broadcast and live media for low-resource languages and hardware. We develop CLSC (Cross-Lingual Sentence Compression), a system trained on openly available parallel corpora organized by compression ratios, where the target length is constrained to a fraction of the source sentence length. We present two training methods: 1) Multiple Models (MM), where individual models are trained separately for each compression ratio, and 2) a Controllable Model (CM), a single model per language using a compression token to encode length constraints. We evaluate both subtitle data and transcriptions from the EuroParl corpus. To accommodate low-resource settings, we constrain data sampling for training and show results for transcriptions in French, Hungarian, Lithuanian, and Polish and subtitles in Albanian, Basque, Malay, and Norwegian. Our models preserve high semantic meaning and metric evaluations for compressed contexts.},
	urldate = {2025-03-13},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jørgensen, Tollef Emil and Mengshoel, Ole Jakob},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {6447--6458},
}

@inproceedings{jung_augsumm_2024,
	title = {{AugSumm}: {Towards} {Generalizable} {Speech} {Summarization} {Using} {Synthetic} {Labels} from {Large} {Language} {Models}},
	url = {https://doi.org/10.1109/ICASSP48485.2024.10447328},
	doi = {10.1109/ICASSP48485.2024.10447328},
	booktitle = icassp # ", 2024",
	publisher = {IEEE},
	author = {Jung, Jee-Weon and Sharma, Roshan S. and Chen, William and Raj, Bhiksha and Watanabe, Shinji},
	year = {2024},
	pages = {12071--12075},
}

@article{apostolidis_video_2021,
	title = {Video {Summarization} {Using} {Deep} {Neural} {Networks}: {A} {Survey}},
	volume = {109},
	url = {https://doi.org/10.1109/JPROC.2021.3117472},
	doi = {10.1109/JPROC.2021.3117472},
	number = {11},
	journal = {Proc. IEEE},
	author = {Apostolidis, Evlampios and Adamantidou, Eleni and Metsai, Alexandros I. and Mezaris, Vasileios and Patras, Ioannis},
	year = {2021},
	pages = {1838--1863},
}

@misc{apostolidis_video_2021-1,
	title = {Video {Summarization} {Using} {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Video {Summarization} {Using} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2101.06072},
	doi = {10.48550/arXiv.2101.06072},
	abstract = {Video summarization technologies aim to create a concise and complete synopsis by selecting the most informative parts of the video content. Several approaches have been developed over the last couple of decades and the current state of the art is represented by methods that rely on modern deep neural network architectures. This work focuses on the recent advances in the area and provides a comprehensive survey of the existing deep-learning-based methods for generic video summarization. After presenting the motivation behind the development of technologies for video summarization, we formulate the video summarization task and discuss the main characteristics of a typical deep-learning-based analysis pipeline. Then, we suggest a taxonomy of the existing algorithms and provide a systematic review of the relevant literature that shows the evolution of the deep-learning-based video summarization technologies and leads to suggestions for future developments. We then report on protocols for the objective evaluation of video summarization algorithms and we compare the performance of several deep-learning-based approaches. Based on the outcomes of these comparisons, as well as some documented considerations about the amount of annotated data and the suitability of evaluation protocols, we indicate potential future research directions.},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Apostolidis, Evlampios and Adamantidou, Eleni and Metsai, Alexandros I. and Mezaris, Vasileios and Patras, Ioannis},
	month = sep,
	year = {2021},
	note = {arXiv:2101.06072 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@article{ling_enhancing_nodate,
	title = {Enhancing {Factual} {Consistency} in {Text} {Summarization} via {Counterfactual} {Debiasing}},
	abstract = {Despite significant progress in abstractive text summarization aimed at generating fluent and informative outputs, how to ensure the factual consistency of generated summaries remains a crucial and challenging issue. In this study, drawing inspiration from advancements in causal inference, we construct causal graphs to analyze the process of abstractive text summarization methods and identify intrinsic causes of factual inconsistency, specifically language bias and irrelevancy bias, and we propose COFACTSUM, a novel framework that mitigates the causal effects of these biases through counterfactual estimation for enhancing the factual consistency of the generated content. COFACTSUM provides two counterfactual estimation strategies, including Explicit Counterfactual Masking, which employs a dynamic masking approach, and Implicit Counterfactual Training, which utilizes a discriminative cross-attention mechanism. Besides, we propose a Debiasing Degree Adjustment mechanism to dynamically calibrate the level of debiasing at each decoding step. Extensive experiments conducted on two widely used summarization datasets demonstrate the effectiveness and advantages of the proposed COFACTSUM in enhancing the factual consistency of generated summaries, outperforming several baseline methods.},
	language = {en},
	author = {Ling, Zhenqing and Xie, Yuexiang and Dong, Chenhe and Shen, Ying},
}

@inproceedings{maynez_faithfulness_2020,
	address = {Online},
	title = {On {Faithfulness} and {Factuality} in {Abstractive} {Summarization}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.173},
	doi = {10.18653/v1/2020.acl-main.173},
	language = {en},
	urldate = {2025-03-13},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
	year = {2020},
	pages = {1906--1919},
}

@misc{miller_leveraging_2019,
	title = {Leveraging {BERT} for {Extractive} {Text} {Summarization} on {Lectures}},
	url = {http://arxiv.org/abs/1906.04165},
	doi = {10.48550/arXiv.1906.04165},
	abstract = {In the last two decades, automatic extractive text summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content. However, many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. Recently, new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models. This paper reports on the project called Lecture Summarization Service, a python based RESTful service that utilizes the BERT model for text embeddings and KMeans clustering to identify sentences closes to the centroid for summary selection. The purpose of the service was to provide students a utility that could summarize lecture content, based on their desired number of sentences. On top of the summary work, the service also includes lecture and summary management, storing content on the cloud which can be used for collaboration. While the results of utilizing BERT for extractive summarization were promising, there were still areas where the model struggled, providing feature research opportunities for further improvement.},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Miller, Derek},
	month = jun,
	year = {2019},
	note = {arXiv:1906.04165 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{xie_using_2025,
	title = {Using {LLM}-supported lecture summarization system to improve knowledge recall and student satisfaction},
	volume = {269},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742403238X},
	doi = {https://doi.org/10.1016/j.eswa.2024.126371},
	abstract = {Large Language Models (LLMs) are a useful summarization tool for helping students remember the main ideas and central claims of a lecture. However, due to limitations in current techniques, they are unable to deal with lecture’s complex structures, which contain multiple knowledge themes. To address this issue, we proposed an original theme-based lecture summary (TLS) system that combines graph-based theme segmentation algorithms and LLMs, consisting of data preparation, theme segmentation, summary generation, and consistency evaluation stages. To assess the effectiveness of the system, an education intervention was implemented using a mixed-methods research design. Forty-six undergraduate students took part in an authentic lecture context for eight weeks. The quantitative study used a 2 × 2 factorial design, with intervention conditions including lecture type (conceptual vs. procedural) and TLS-generated summaries (with vs. without). Then, eight of the participants were chosen for a focus-group interview to learn about student satisfaction. The study finds that: 1) TLS-generated summaries have a significant positive influence on knowledge recall; 2) lecture type has no effect on the intervention outcomes of TLS-generated summaries, regardless of whether it is conceptually or procedurally oriented; and 3) TLS-generated summaries can increase student satisfaction. The current study broadens the use of LLMs in authentic educational settings to promote student success.},
	journal = {Expert Systems with Applications},
	author = {Xie, Tao and Kuang, Yuanyuan and Tang, Ying and Liao, Jian and Yang, Yunong},
	year = {2025},
	keywords = {Knowledge recall, Large language models, Lecture summaries, Student satisfaction, Undergraduate students},
	pages = {126371},
}

@misc{noauthor_using_nodate,
	title = {Using {LLM}-supported lecture summarization system to improve knowledge recall and student satisfaction - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742403238X?via%3Dihub},
	urldate = {2025-03-13},
}

@inproceedings{song_towards_2022,
	address = {Dublin, Ireland},
	title = {Towards {Abstractive} {Grounded} {Summarization} of {Podcast} {Transcripts}},
	url = {https://aclanthology.org/2022.acl-long.302/},
	doi = {10.18653/v1/2022.acl-long.302},
	abstract = {Podcasts have shown a recent rise in popularity. Summarization of podcasts is of practical benefit to both content providers and consumers. It helps people quickly decide whether they will listen to a podcast and/or reduces the cognitive load of content providers to write summaries. Nevertheless, podcast summarization faces significant challenges including factual inconsistencies of summaries with respect to the inputs. The problem is exacerbated by speech disfluencies and recognition errors in transcripts of spoken language. In this paper, we explore a novel abstractive summarization method to alleviate these issues. Our approach learns to produce an abstractive summary while grounding summary segments in specific regions of the transcript to allow for full inspection of summary details. We conduct a series of analyses of the proposed approach on a large podcast dataset and show that the approach can achieve promising results. Grounded summaries bring clear benefits in locating the summary and transcript segments that contain inconsistent information, and hence improve summarization quality in terms of automatic and human evaluation.},
	urldate = {2025-03-13},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Song, Kaiqiang and Li, Chen and Wang, Xiaoyang and Yu, Dong and Liu, Fei},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {4407--4418},
}

@article{klatt_review_1977,
	title = {Review of the {ARPA} {Speech} {Understanding} {Project}},
	volume = {62},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/62/6/1345/744517/Review-of-the-ARPA-Speech-Understanding-Project},
	doi = {10.1121/1.381666},
	abstract = {In September of 1976, four speech understanding systems were demonstrated, signifying the end of a five-year program of research and development sponsored by the Advanced Research Projects Agency (ARPA). The best performance was displayed by the Harpy system developed at Carnegie–Mellon University. Harpy satisfied a set of design goals that were specified at the beginning of the program, including the gal of understanding over 90\% of a set of naturally spoken sentences composed from a 1000-word lexicon. After defining the nature of the speech understanding problem, the four systems are described and critically evaluated. Based on this review, a structure for a next-generation speech understanding system is proposed and parts of it are considered as a possible model of the early stages of speech perception. The perceptual model addresses the issue of lexical access and includes a decoding network composed of expected spectral sequences for all word strings of English.},
	language = {en},
	number = {6},
	urldate = {2025-03-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Klatt, Dennis H.},
	month = dec,
	year = {1977},
	pages = {1345--1366},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2025-03-13},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{renals_connectionist_1994,
	title = {Connectionist probability estimators in {HMM} speech recognition},
	volume = {2},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1063-6676},
	url = {http://ieeexplore.ieee.org/document/260359/},
	doi = {10.1109/89.260359},
	number = {1},
	urldate = {2025-03-13},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Renals, S. and Morgan, N. and Bourlard, H. and Cohen, M. and Franco, H.},
	month = jan,
	year = {1994},
	pages = {161--174},
}

@inproceedings{kirstein_whats_2025,
	address = {Abu Dhabi, UAE},
	title = {What`s {Wrong}? {Refining} {Meeting} {Summaries} with {LLM} {Feedback}},
	url = {https://aclanthology.org/2025.coling-main.143/},
	abstract = {Meeting summarization has become a critical task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context understanding compared to traditional methods. However, they still struggle to maintain relevance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence. This post-hoc refinement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar complex text generation tasks requiring robustness, action planning, and discussion towards a goal.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kirstein, Frederic Thomas and Lima Ruas, Terry and Gipp, Bela},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {2100--2120},
}

@inproceedings{kirstein_is_2025,
	address = {Abu Dhabi, UAE},
	title = {Is my {Meeting} {Summary} {Good}? {Estimating} {Quality} with a {Multi}-{LLM} {Evaluator}},
	url = {https://aclanthology.org/2025.coling-industry.48/},
	abstract = {The quality of meeting summaries generated by natural language generation (NLG) systems is hard to measure automatically. Established metrics such as ROUGE and BERTScore have a relatively low correlation with human judgments and fail to capture nuanced errors. Recent studies suggest using large language models (LLMs), which have the benefit of better context understanding and adaption of error definitions without training on a large number of human preference judgments. However, current LLM-based evaluators risk masking errors and can only serve as a weak proxy, leaving human evaluation the gold standard despite being costly and hard to compare across studies. In this work, we present MESA, an LLM-based framework employing a three-step assessment of individual error types, multi-agent discussion for decision refinement, and feedback-based self-training to refine error definition understanding and alignment with human judgment. We show that MESA`s components enable thorough error detection, consistent rating, and adaptability to custom error guidelines. Using GPT-4o as its backbone, MESA achieves mid to high Point-Biserial correlation with human judgment in error detection and mid Spearman and Kendall correlation in reflecting error impact on summary quality, on average 0.25 higher than previous methods. The framework`s flexibility in adapting to custom error guidelines makes it suitable for various tasks with limited human-labeled data.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Kirstein, Frederic Thomas and Lima Ruas, Terry and Gipp, Bela},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven and Darwish, Kareem and Agarwal, Apoorv},
	month = jan,
	year = {2025},
	pages = {561--574},
}

@inproceedings{kirstein_whats_2024,
	address = {Miami, Florida, USA},
	title = {What`s under the hood: {Investigating} {Automatic} {Metrics} on {Meeting} {Summarization}},
	url = {https://aclanthology.org/2024.findings-emnlp.393/},
	doi = {10.18653/v1/2024.findings-emnlp.393},
	abstract = {Meeting summarization has become a critical task considering the increase in online interactions. Despite new techniques being proposed regularly, the evaluation of meeting summarization techniques relies on metrics not tailored to capture meeting-specific errors, leading to ineffective assessment. This paper explores what established automatic metrics capture and the errors they mask by correlating metric scores with human evaluations across a comprehensive error taxonomy. We start by reviewing the literature on English meeting summarization to identify key challenges, such as speaker dynamics and contextual turn-taking, and error types, including missing information and linguistic inaccuracy, concepts previously loosely defined in the field. We then examine the relationship between these challenges and errors using human annotated transcripts and summaries from encoder-decoder-based and autoregressive Transformer models on the QMSum dataset. Experiments reveal that different model architectures respond variably to the challenges, resulting in distinct links between challenges and errors. Current established metrics struggle to capture the observable errors, showing weak to moderate correlations, with a third of the correlations indicating error masking. Only a subset of metrics accurately reacts to specific errors, while most correlations show either unresponsiveness or failure to reflect the error`s impact on summary quality.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Kirstein, Frederic and Wahle, Jan Philip and Ruas, Terry and Gipp, Bela},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {6709--6723},
}

@misc{noauthor_whats_nodate,
	title = {What’s under the hood: {Investigating} {Automatic} {Metrics} on {Meeting} {Summarization} - {ACL} {Anthology}},
	url = {https://aclanthology.org/2024.findings-emnlp.393/},
	urldate = {2025-03-12},
}

@inproceedings{murray_summarizing_2008,
	address = {Honolulu, Hawaii},
	title = {Summarizing {Spoken} and {Written} {Conversations}},
	url = {https://aclanthology.org/D08-1081/},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 2008 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Murray, Gabriel and Carenini, Giuseppe},
	editor = {Lapata, Mirella and Ng, Hwee Tou},
	month = oct,
	year = {2008},
	pages = {773--782},
}

@inproceedings{murray_summarizing_2008-1,
	address = {Honolulu, Hawaii},
	title = {Summarizing {Spoken} and {Written} {Conversations}},
	url = {https://aclanthology.org/D08-1081/},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 2008 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Murray, Gabriel and Carenini, Giuseppe},
	editor = {Lapata, Mirella and Ng, Hwee Tou},
	month = oct,
	year = {2008},
	pages = {773--782},
}

@misc{savelieva_abstractive_2020,
	title = {Abstractive {Summarization} of {Spoken} and {Written} {Instructions} with {BERT}},
	url = {http://arxiv.org/abs/2008.09676},
	doi = {10.48550/arXiv.2008.09676},
	abstract = {Summarization of speech is a difficult problem due to the spontaneity of the flow, disfluencies, and other issues that are not usually encountered in written texts. Our work presents the first application of the BERTSum model to conversational language. We generate abstractive summaries of narrated instructional videos across a wide variety of topics, from gardening and cooking to software configuration and sports. In order to enrich the vocabulary, we use transfer learning and pretrain the model on a few large cross-domain datasets in both written and spoken English. We also do preprocessing of transcripts to restore sentence segmentation and punctuation in the output of an ASR system. The results are evaluated with ROUGE and Content-F1 scoring for the How2 and WikiHow datasets. We engage human judges to score a set of summaries randomly selected from a dataset curated from HowTo100M and YouTube. Based on blind evaluation, we achieve a level of textual fluency and utility close to that of summaries written by human content creators. The model beats current SOTA when applied to WikiHow articles that vary widely in style and topic, while showing no performance regression on the canonical CNN/DailyMail dataset. Due to the high generalizability of the model across different styles and domains, it has great potential to improve accessibility and discoverability of internet content. We envision this integrated as a feature in intelligent virtual assistants, enabling them to summarize both written and spoken instructional content upon request.},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Savelieva, Alexandra and Au-Yeung, Bryan and Ramani, Vasanth},
	month = aug,
	year = {2020},
	note = {arXiv:2008.09676 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{tang_tofueval_2024,
	address = {Mexico City, Mexico},
	title = {{TofuEval}: {Evaluating} {Hallucinations} of {LLMs} on {Topic}-{Focused} {Dialogue} {Summarization}},
	shorttitle = {{TofuEval}},
	url = {https://aclanthology.org/2024.naacl-long.251/},
	doi = {10.18653/v1/2024.naacl-long.251},
	abstract = {Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model`s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Liyan and Shalyminov, Igor and Wong, Amy and Burnsky, Jon and Vincent, Jake and Yang, Yu'an and Singh, Siffi and Feng, Song and Song, Hwanjun and Su, Hang and Sun, Lijia and Zhang, Yi and Mansour, Saab and McKeown, Kathleen},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {4455--4480},
}

@inproceedings{shang_end--end_2024,
	title = {An {End}-to-{End} {Speech} {Summarization} {Using} {Large} {Language} {Model}},
	doi = {10.21437/Interspeech.2024-1428},
	booktitle = interspeech # " 2024",
	author = {Shang, Hengchao and Li, Zongyao and Guo, Jiaxin and Li, Shaojun and Rao, Zhiqiang and Luo, Yuanchang and Wei, Daimeng and Yang, Hao},
	year = {2024},
	note = {ISSN: 2958-1796},
	pages = {1950--1954},
}

@inproceedings{kang_prompting_2024,
	title = {Prompting {Large} {Language} {Models} with {Audio} for {General}-{Purpose} {Speech} {Summarization}},
	doi = {10.21437/Interspeech.2024-2213},
	booktitle = interspeech # " 2024",
	author = {Kang, Wonjune and Roy, Deb},
	year = {2024},
	note = {ISSN: 2958-1796},
	pages = {1955--1959},
}

@inproceedings{kang_prompting_2024-1,
	title = {Prompting {Large} {Language} {Models} with {Audio} for {General}-{Purpose} {Speech} {Summarization}},
	url = {http://arxiv.org/abs/2406.05968},
	doi = {10.21437/Interspeech.2024-2213},
	abstract = {In this work, we introduce a framework for speech summarization that leverages the processing and reasoning capabilities of large language models (LLMs). We propose an end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret. Using a dataset with paired speech-text data, the overall system is trained to generate consistent responses to prompts with the same semantic information regardless of the input modality. The resulting framework allows the LLM to process speech inputs in the same way as text, enabling speech summarization by simply prompting the LLM. Unlike prior approaches, our method is able to summarize spoken content from any arbitrary domain, and it can produce summaries in different styles by varying the LLM prompting strategy. Experiments demonstrate that our approach outperforms a cascade baseline of speech recognition followed by LLM text processing.},
	urldate = {2025-03-12},
	booktitle = interspeech # " 2024",
	author = {Kang, Wonjune and Roy, Deb},
	month = sep,
	year = {2024},
	note = {arXiv:2406.05968 [eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1955--1959},
}

@inproceedings{mullenbach_clip_2021,
	address = {Online},
	title = {{CLIP}: {A} {Dataset} for {Extracting} {Action} {Items} for {Physicians} from {Hospital} {Discharge} {Notes}},
	shorttitle = {{CLIP}},
	url = {https://aclanthology.org/2021.acl-long.109/},
	doi = {10.18653/v1/2021.acl-long.109},
	abstract = {Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting, and improved information sharing can help. To share information, caregivers write discharge notes containing action items to share with patients and their future caregivers, but these action items are easily lost due to the lengthiness of the documents. In this work, we describe our creation of a dataset of clinical action items annotated over MIMIC-III, the largest publicly available dataset of real clinical notes. This dataset, which we call CLIP, is annotated by physicians and covers 718 documents representing 100K sentences. We describe the task of extracting the action items from these documents as multi-aspect extractive summarization, with each aspect representing a type of action to be taken. We evaluate several machine learning models on this task, and show that the best models exploit in-domain language model pre-training on 59K unannotated documents, and incorporate context from neighboring sentences. We also propose an approach to pre-training data selection that allows us to explore the trade-off between size and domain-specificity of pre-training datasets for this task.},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mullenbach, James and Pruksachatkun, Yada and Adler, Sean and Seale, Jennifer and Swartz, Jordan and McKelvey, Greg and Dai, Hui and Yang, Yi and Sontag, David},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {1365--1378},
}

@inproceedings{purver_detecting_2007,
	address = {Antwerp, Belgium},
	title = {Detecting and {Summarizing} {Action} {Items} in {Multi}-{Party} {Dialogue}},
	url = {https://aclanthology.org/2007.sigdial-1.4/},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 8th {SIGdial} {Workshop} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Purver, Matthew and Dowding, John and Niekrasz, John and Ehlen, Patrick and Noorbaloochi, Sharareh and Peters, Stanley},
	editor = {Bunt, Harry and Keizer, Simon and Paek, Tim},
	month = sep,
	year = {2007},
	pages = {18--25},
}

@misc{asthana_summaries_2024,
	title = {Summaries, {Highlights}, and {Action} items: {Design}, implementation and evaluation of an {LLM}-powered meeting recap system},
	shorttitle = {Summaries, {Highlights}, and {Action} items},
	url = {http://arxiv.org/abs/2307.15793},
	doi = {10.48550/arXiv.2307.15793},
	abstract = {Meetings play a critical infrastructural role in the coordination of work. In recent years, due to shift to hybrid and remote work, more meetings are moving to online Computer Mediated Spaces. This has led to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context. To address these gaps, we design, implement and evaluate in-context a meeting recap system. We first conceptualize two salient recap representations -- important highlights, and a structured, hierarchical minutes view. We develop a system to operationalize the representations with dialogue summarization as its building blocks. Finally, we evaluate the effectiveness of the system with seven users in the context of their work meetings. Our findings show promise in using LLM-based dialogue summarization for meeting recap and the need for both representations in different contexts. However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics. We identify collaboration opportunities such as a shared recap document that a high quality recap enables. We report on implications for designing AI systems to partner with users to learn and improve from natural interactions to overcome the limitations related to personal relevance and summarization quality.},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Asthana, Sumit and Hilleli, Sagih and He, Pengcheng and Halfaker, Aaron},
	month = aug,
	year = {2024},
	note = {arXiv:2307.15793 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval},
}

@misc{dubois_length-controlled_2025,
	title = {Length-{Controlled} {AlpacaEval}: {A} {Simple} {Way} to {Debias} {Automatic} {Evaluators}},
	shorttitle = {Length-{Controlled} {AlpacaEval}},
	url = {http://arxiv.org/abs/2404.04475},
	doi = {10.48550/arXiv.2404.04475},
	abstract = {LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instructiontuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: "What would the preference be if the model’s and baseline’s output had the same length?" To achieve this, we first fit a generalized linear model to predict the biased auto-annotator’s preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98. We release the code and resulting leaderboard.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Dubois, Yann and Galambosi, Balázs and Liang, Percy and Hashimoto, Tatsunori B.},
	month = mar,
	year = {2025},
	note = {arXiv:2404.04475 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dubois_alpacafarm_nodate,
	title = {{AlpacaFarm}: {A} {Simulation} {Framework} for {Methods} that {Learn} from {Human} {Feedback}},
	abstract = {Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowd-workers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that the rankings of models trained in AlpacaFarm match the rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10\% improvement in win-rate against Davinci003. We release AlpacaFarm at https://github.com/tatsu-lab/alpaca\_farm.},
	language = {en},
	author = {Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan},
}

@inproceedings{bhandari_metrics_2020,
	address = {Barcelona, Spain (Online)},
	title = {Metrics also {Disagree} in the {Low} {Scoring} {Range}: {Revisiting} {Summarization} {Evaluation} {Metrics}},
	shorttitle = {Metrics also {Disagree} in the {Low} {Scoring} {Range}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.501},
	doi = {10.18653/v1/2020.coling-main.501},
	language = {en},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Bhandari, Manik and Gour, Pranav Narayan and Ashfaq, Atabak and Liu, Pengfei},
	year = {2020},
	pages = {5702--5711},
}

@article{qiu_mmsum_nodate,
	title = {{MMSum}: {A} {Dataset} for {Multimodal} {Summarization} and {Thumbnail} {Generation} of {Videos}},
	abstract = {Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient maintenance, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MMSum dataset. Our new dataset features (1) Humanvalidated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess various tasks and methods, including video summarization, text summarization, and multimodal summarization. To champion accessibility and collaboration, we released the MMSum dataset and the data collection tool as fully open-source resources, fostering transparency and accelerating future developments, at https://mmsum-dataset.github.io/.},
	language = {en},
	author = {Qiu, Jielin and Zhu, Jiacheng and Han, William and Kumar, Aditesh and Mittal, Karthik and Jin, Claire and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Zhao, Ding and Li, Bo and Wang, Lijuan},
}

@misc{thakur_judging_2025,
	title = {Judging the {Judges}: {Evaluating} {Alignment} and {Vulnerabilities} in {LLMs}-as-{Judges}},
	shorttitle = {Judging the {Judges}},
	url = {http://arxiv.org/abs/2406.12624},
	doi = {10.48550/arXiv.2406.12624},
	abstract = {Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges1 Investigating thirteen judge models of different model sizes and families, judging answers of nine different ‘exam-taker models’ – both base and instruction-tuned – we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
	month = jan,
	year = {2025},
	note = {arXiv:2406.12624 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{mei_automated_2022,
	title = {Automated audio captioning: an overview of recent progress and new challenges},
	volume = {2022},
	issn = {1687-4722},
	shorttitle = {Automated audio captioning},
	url = {https://doi.org/10.1186/s13636-022-00259-2},
	doi = {10.1186/s13636-022-00259-2},
	abstract = {Automated audio captioning is a cross-modal translation task that aims to generate natural language descriptions for given audio clips. This task has received increasing attention with the release of freely available datasets in recent years. The problem has been addressed predominantly with deep learning techniques. Numerous approaches have been proposed, such as investigating different neural network architectures, exploiting auxiliary information such as keywords or sentence information to guide caption generation, and employing different training strategies, which have greatly facilitated the development of this field. In this paper, we present a comprehensive review of the published contributions in automated audio captioning, from a variety of existing approaches to evaluation metrics and datasets. We also discuss open challenges and envisage possible future research directions.},
	number = {1},
	urldate = {2025-03-11},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Mei, Xinhao and Liu, Xubo and Plumbley, Mark D. and Wang, Wenwu},
	month = oct,
	year = {2022},
	keywords = {Audio captioning, Audio processing, Deep learning, Encoder-decoder framework, Natural language processing},
	pages = {26},
}

@inproceedings{banerjee_generating_2015,
	address = {New York, NY, USA},
	series = {{DocEng} '15},
	title = {Generating {Abstractive} {Summaries} from {Meeting} {Transcripts}},
	isbn = {978-1-4503-3307-8},
	url = {https://doi.org/10.1145/2682571.2797061},
	doi = {10.1145/2682571.2797061},
	abstract = {Summaries of meetings are very important as they convey the essential content of discussions in a concise form. Both participants and non-participants are interested in the summaries of meetings to plan for their future work. Generally, it is time consuming to read and understand the whole documents. Therefore, summaries play an important role as the readers are interested in only the important context of discussions. In this work, we address the task of meeting document summarization. Automatic summarization systems on meeting conversations developed so far have been primarily extractive, resulting in unacceptable summaries that are hard to read. The extracted utterances contain disfluencies that affect the quality of the extractive summaries. To make summaries much more readable, we propose an approach to generating abstractive summaries by fusing important content from several utterances. We first separate meeting transcripts into various topic segments, and then identify the important utterances in each segment using a supervised learning approach.The important utterances are then combined together to generate a one-sentence summary. In the text generation step, the dependency parses of the utterances in each segment are combined together to create a directed graph. The most informative and well-formed sub-graph obtained by integer linear programming (ILP) is selected to generate a one-sentence summary for each topic segment. The ILP formulation reduces disfluencies by leveraging grammatical relations that are more prominent in non-conversational style of text, and therefore generates summaries that is comparable to human-written abstractive summaries. Experimental results show that our method can generate more informative summaries than the baselines. In addition, readability assessments by human judges as well as log-likelihood estimates obtained from the dependency parser show that our generated summaries are significantly readable and well-formed.},
	booktitle = {Proceedings of the 2015 {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Banerjee, Siddhartha and Mitra, Prasenjit and Sugiyama, Kazunari},
	year = {2015},
	note = {event-place: Lausanne, Switzerland},
	keywords = {abstractive meeting summarization, integer linear programming, topic segmentation},
	pages = {51--60},
}

@inproceedings{tang_investigating_2022,
	address = {Seattle, United States},
	title = {Investigating {Crowdsourcing} {Protocols} for {Evaluating} the {Factual} {Consistency} of {Summaries}},
	url = {https://aclanthology.org/2022.naacl-main.417/},
	doi = {10.18653/v1/2022.naacl-main.417},
	abstract = {Current pre-trained models applied for summarization are prone to factual inconsistencies that misrepresent the source text. Evaluating the factual consistency of summaries is thus necessary to develop better models. However, the human evaluation setup for evaluating factual consistency has not been standardized. To determine the factors that affect the reliability of the human evaluation, we crowdsource evaluations for factual consistency across state-of-the-art models on two news summarization datasets using the rating-based Likert Scale and ranking-based Best-Worst Scaling. Our analysis reveals that the ranking-based Best-Worst Scaling offers a more reliable measure of summary quality across datasets and that the reliability of Likert ratings highly depends on the target dataset and the evaluation design. To improve crowdsourcing reliability, we extend the scale of the Likert rating and present a scoring algorithm for Best-Worst Scaling that we call value learning. Our crowdsourcing guidelines will be publicly available to facilitate future work on factual consistency in summarization.},
	urldate = {2025-03-11},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Xiangru and Fabbri, Alexander and Li, Haoran and Mao, Ziming and Adams, Griffin and Wang, Borui and Celikyilmaz, Asli and Mehdad, Yashar and Radev, Dragomir},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {5680--5692},
}

@inproceedings{iskender_best_2020,
	address = {Online},
	title = {Best {Practices} for {Crowd}-based {Evaluation} of {German} {Summarization}: {Comparing} {Crowd}, {Expert} and {Automatic} {Evaluation}},
	shorttitle = {Best {Practices} for {Crowd}-based {Evaluation} of {German} {Summarization}},
	url = {https://aclanthology.org/2020.eval4nlp-1.16/},
	doi = {10.18653/v1/2020.eval4nlp-1.16},
	abstract = {One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with human quality ratings. As a solution, we propose crowdsourcing as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of summarization by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, BLEU, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure \& coherence, summary usefulness, and summary informativeness.},
	urldate = {2025-03-11},
	booktitle = {Proceedings of the {First} {Workshop} on {Evaluation} and {Comparison} of {NLP} {Systems}},
	publisher = {Association for Computational Linguistics},
	author = {Iskender, Neslihan and Polzehl, Tim and Möller, Sebastian},
	editor = {Eger, Steffen and Gao, Yang and Peyrard, Maxime and Zhao, Wei and Hovy, Eduard},
	month = nov,
	year = {2020},
	pages = {164--175},
}

@inproceedings{bhandari_metrics_2020-1,
	address = {Barcelona, Spain (Online)},
	title = {Metrics also {Disagree} in the {Low} {Scoring} {Range}: {Revisiting} {Summarization} {Evaluation} {Metrics}},
	shorttitle = {Metrics also {Disagree} in the {Low} {Scoring} {Range}},
	url = {https://aclanthology.org/2020.coling-main.501/},
	doi = {10.18653/v1/2020.coling-main.501},
	abstract = {In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Bhandari, Manik and Gour, Pranav Narayan and Ashfaq, Atabak and Liu, Pengfei},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {5702--5711},
}

@inproceedings{peyrard_studying_2019,
	address = {Florence, Italy},
	title = {Studying {Summarization} {Evaluation} {Metrics} in the {Appropriate} {Scoring} {Range}},
	url = {https://aclanthology.org/P19-1502/},
	doi = {10.18653/v1/P19-1502},
	abstract = {In summarization, automatic evaluation metrics are usually compared based on their ability to correlate with human judgments. Unfortunately, the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC/TAC shared tasks. However, modern systems are typically better than the best systems submitted at the time of these shared tasks. We show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate. It is problematic because metrics disagree yet we can`t decide which one to trust. This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust. This would also be greatly beneficial to further improve summarization systems and metrics alike.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Peyrard, Maxime},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {5093--5100},
}

@misc{lu_mutual_2025,
	title = {Mutual {Reinforcement} of {LLM} {Dialogue} {Synthesis} and {Summarization} {Capabilities} for {Few}-{Shot} {Dialogue} {Summarization}},
	url = {http://arxiv.org/abs/2502.17328},
	doi = {10.48550/arXiv.2502.17328},
	abstract = {In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM{\textbackslash}'s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5\% increase in ROUGE scores and a 0.3\% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Lu, Yen-Ju and Hu, Ting-Yao and Koppula, Hema Swetha and Pouransari, Hadi and Chang, Jen-Hao Rick and Xia, Yin and Kong, Xiang and Zhu, Qi and Wang, Simon and Tuzel, Oncel and Vemulapalli, Raviteja},
	month = feb,
	year = {2025},
	note = {arXiv:2502.17328 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{dubois_alpacafarm_2023,
	title = {{AlpacaFarm}: {A} {Simulation} {Framework} for {Methods} that {Learn} from {Human} {Feedback}},
	url = {http://papers.nips.cc/paper\_files/paper/2023/hash/5fc47800ee5b30b8777fdd30abcaaf3b-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 36: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2023, {NeurIPS} 2023, {New} {Orleans}, {LA}, {USA}, {December} 10 - 16, 2023},
	author = {Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
	editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
	year = {2023},
}

@misc{dubois_length-controlled_2024,
	title = {Length-{Controlled} {AlpacaEval}: {A} {Simple} {Way} to {Debias} {Automatic} {Evaluators}},
	url = {https://doi.org/10.48550/arXiv.2404.04475},
	doi = {10.48550/ARXIV.2404.04475},
	author = {Dubois, Yann and Galambosi, Balázs and Liang, Percy and Hashimoto, Tatsunori B.},
	year = {2024},
  publisher = {arXiv},
  note = {arXiv:2404.04475 [cs.LG]},
}

@inproceedings{kryscinski_evaluating_2020,
	address = {Online},
	title = {Evaluating the {Factual} {Consistency} of {Abstractive} {Text} {Summarization}},
	url = {https://aclanthology.org/2020.emnlp-main.750/},
	doi = {10.18653/v1/2020.emnlp-main.750},
	abstract = {The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {9332--9346},
}

@inproceedings{durmus_spurious_2022,
	address = {Dublin, Ireland},
	title = {Spurious {Correlations} in {Reference}-{Free} {Evaluation} of {Text} {Generation}},
	url = {https://aclanthology.org/2022.acl-long.102/},
	doi = {10.18653/v1/2022.acl-long.102},
	abstract = {Model-based, reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation(NLG) systems. Despite promising recentresults, we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word overlap,perplexity, and length. We further observethat for text summarization, these metrics havehigh error rates when ranking current state-ofthe-art abstractive summarization systems. Wedemonstrate that these errors can be mitigatedby explicitly designing evaluation metrics toavoid spurious features in reference-free evaluation.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Durmus, Esin and Ladhak, Faisal and Hashimoto, Tatsunori},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {1443--1454},
}

@inproceedings{tuggener_are_2021,
	address = {Online and in Dominican Republic},
	title = {Are {We} {Summarizing} the {Right} {Way}? {A} {Survey} of {Dialogue} {Summarization} {Data} {Sets}},
	shorttitle = {Are {We} {Summarizing} the {Right} {Way}?},
	url = {https://aclanthology.org/2021.newsum-1.12/},
	doi = {10.18653/v1/2021.newsum-1.12},
	abstract = {Dialogue summarization is a long-standing task in the field of NLP, and several data sets with dialogues and associated human-written summaries of different styles exist. However, it is unclear for which type of dialogue which type of summary is most appropriate. For this reason, we apply a linguistic model of dialogue types to derive matching summary items and NLP tasks. This allows us to map existing dialogue summarization data sets into this model and identify gaps and potential directions for future work. As part of this process, we also provide an extensive overview of existing dialogue summarization data sets.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the {Third} {Workshop} on {New} {Frontiers} in {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Tuggener, Don and Mieskes, Margot and Deriu, Jan and Cieliebak, Mark},
	editor = {Carenini, Giuseppe and Cheung, Jackie Chi Kit and Dong, Yue and Liu, Fei and Wang, Lu},
	month = nov,
	year = {2021},
	pages = {107--118},
}

@misc{hardt_patterns_2021,
	title = {Patterns, predictions, and actions: {A} story about machine learning},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Patterns, predictions, and actions},
	url = {https://arxiv.org/abs/2102.05242},
	doi = {10.48550/ARXIV.2102.05242},
	abstract = {This graduate textbook on machine learning tells a story of how patterns in data support predictions and consequential actions. Starting with the foundations of decision making, we cover representation, optimization, and generalization as the constituents of supervised learning. A chapter on datasets as benchmarks examines their histories and scientific bases. Self-contained introductions to causality, the practice of causal inference, sequential decision making, and reinforcement learning equip the reader with concepts and tools to reason about actions and their consequences. Throughout, the text discusses historical context and societal impact. We invite readers from all backgrounds; some experience with probability, calculus, and linear algebra suffices.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Hardt, Moritz and Recht, Benjamin},
	year = {2021},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{li_theres_2023,
	title = {“{There}’s {No} {Data} {Like} {More} {Data}”: {Automatic} {Speech} {Recognition} and the {Making} of {Algorithmic} {Culture}},
	volume = {38},
	issn = {0369-7827, 1933-8287},
	shorttitle = {“{There}’s {No} {Data} {Like} {More} {Data}”},
	url = {https://www.journals.uchicago.edu/doi/10.1086/725132},
	doi = {10.1086/725132},
	language = {en},
	urldate = {2025-03-10},
	journal = {Osiris},
	author = {Li, Xiaochang},
	month = jul,
	year = {2023},
	pages = {165--182},
}

@incollection{pinch_signals_2012,
	address = {New York},
	series = {Oxford handbooks},
	title = {Do {Signals} {Have} {Politics}? {Inscribing} {Abilities} in {Cochlear} {Implants}},
	isbn = {9780195388947 9780199995813},
	abstract = {Written by the world's leading scholars and researchers in the emerging field of sound studies, The Oxford Handbook of Sound Studies offers new and fully engaging perspectives on the significance of sound in its material and cultural forms. The book considers sounds and music as experienced in such diverse settings as shop floors, laboratories, clinics, design studios, homes, and clubs, across an impressively broad range of historical periods and national and cultural contexts. Science has traditionally been understood as a visual matter, a study which has historically been undertaken with optical technologies such as slides, graphs, and telescopes. This book questions that notion powerfully by showing how listening has contributed to scientific practice. Sounds have always been a part of human experience, shaping and transforming the world in which we live in ways that often go unnoticed. Sounds and music, the authors argue, are embedded in the fabric of everyday life, art, commerce, and politics in ways which impact our perception of the world. Through an extraordinarily diverse set of case studies, authors illustrate how sounds--from the sounds of industrialization, to the sounds of automobiles, to sounds in underwater music and hip-hop, to the sounds of nanotechnology--give rise to new forms listening practices. In addition, the book discusses the rise of new public problems such as noise pollution, hearing loss, and the "end" of the amateur musician that stem from the spread and appropriation of new sound and music-related technologies, analog and digital, in many domains of life},
	language = {eng},
	booktitle = {The {Oxford} handbook of sound studies},
	publisher = {Oxford University Press},
	author = {Mills, Mara},
	editor = {Pinch, Trevor and Bijsterveld, Karin},
	year = {2012},
	pages = {320--246},
}

@article{li_vocal_2019,
	title = {Vocal {Features}: {From} {Voice} {Identification} to {Speech} {Recognition} by {Machine}},
	volume = {60},
	issn = {1097-3729},
	shorttitle = {Vocal {Features}},
	url = {https://muse.jhu.edu/article/727301},
	doi = {10.1353/tech.2019.0066},
	language = {en},
	number = {2S},
	urldate = {2025-03-10},
	journal = {Technology and Culture},
	author = {Li, Xiaochang and Mills, Mara},
	year = {2019},
	pages = {S129--S160},
}

@article{luhn_automatic_1958,
	title = {The automatic creation of literature abstracts},
	volume = {2},
	issn = {0018-8646},
	number = {2},
	journal = {IBM Journal of research and development},
	author = {Luhn, Hans Peter},
	year = {1958},
        URL = {https://ieeexplore.ieee.org/document/5392672},
	pages = {159--165},
}

@book{philo_animal_2004,
	address = {Hoboken},
	series = {Critical {Geographies}},
	title = {Animal {Spaces}, {Beastly} {Places}},
	isbn = {9780203004883 9780415198462},
	abstract = {This book explores the variations on the human-animal spatial orderings. It develops new ways of thinking about human animal interactions and encourges us to find new ways for humans and animals to live together},
	language = {eng},
	publisher = {Taylor and Francis},
	author = {Philo, Chris and Wilbert, Chris},
	year = {2004},
}

@inproceedings{nihei_fusing_2018,
	address = {New York, NY, USA},
	series = {{GIFT}'18},
	title = {Fusing {Verbal} and {Nonverbal} {Information} for {Extractive} {Meeting} {Summarization}},
	isbn = {978-1-4503-6077-7},
	url = {https://doi.org/10.1145/3279981.3279987},
	doi = {10.1145/3279981.3279987},
	abstract = {Automatic meeting summarization would reduce the cost of producing minutes during or after a meeting. With the goal of establishing a method for extractive meeting summarization, we propose a multimodal fusion model that identifies the important utterances that should be included in meeting extracts of group discussions. The proposed multimodal model fuses audio, visual, motion, and linguistic unimodal models that are trained by employing a convolutional neural network approach. The performance of the verbal and nonverbal fusion model presented an F-measure of 0.831. We also discuss the characteristics of verbal and nonverbal models and demonstrate that they complement each other.},
	booktitle = {Proceedings of the {Group} {Interaction} {Frontiers} in {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Nihei, Fumio and Nakano, Yukiko I. and Takase, Yutaka},
	year = {2018},
	note = {event-place: Boulder, CO, USA},
	keywords = {Deep neural network, Important utterances for meeting summarization, Multimodal fusion},
}

@inproceedings{khullar_mast_2020,
	address = {Online},
	title = {{MAST}: {Multimodal} {Abstractive} {Summarization} with {Trimodal} {Hierarchical} {Attention}},
	url = {https://aclanthology.org/2020.nlpbt-1.7/},
	doi = {10.18653/v1/2020.nlpbt-1.7},
	abstract = {This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities – text, audio and video – in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Natural} {Language} {Processing} {Beyond} {Text}},
	publisher = {Association for Computational Linguistics},
	author = {Khullar, Aman and Arora, Udit},
	editor = {Castellucci, Giuseppe and Filice, Simone and Poria, Soujanya and Cambria, Erik and Specia, Lucia},
	month = nov,
	year = {2020},
	pages = {60--69},
}

@misc{noauthor_mast_nodate,
	title = {{MAST}: {Multimodal} {Abstractive} {Summarization} with {Trimodal} {Hierarchical} {Attention} - {ACL} {Anthology}},
	url = {https://aclanthology.org/2020.nlpbt-1.7/},
	urldate = {2025-03-07},
}

@misc{noauthor_multimodal_nodate,
	title = {Multimodal {Abstractive} {Summarization} for {How2} {Videos} - {ACL} {Anthology}},
	url = {https://aclanthology.org/P19-1659/},
	urldate = {2025-03-07},
}

@inproceedings{li_keep_2019,
	address = {Florence, Italy},
	title = {Keep {Meeting} {Summaries} on {Topic}: {Abstractive} {Multi}-{Modal} {Meeting} {Summarization}},
	shorttitle = {Keep {Meeting} {Summaries} on {Topic}},
	url = {https://aclanthology.org/P19-1210/},
	doi = {10.18653/v1/P19-1210},
	abstract = {Transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make Natural Language Generation models for generating summaries unfocused. We develop an abstractive meeting summarizer from both videos and audios of meeting recordings. Specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. To narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. In addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures.},
	urldate = {2025-03-07},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Manling and Zhang, Lingyu and Ji, Heng and Radke, Richard J.},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {2190--2196},
}

@inproceedings{li_multi-modal_2017,
	address = {Copenhagen, Denmark},
	title = {Multi-modal {Summarization} for {Asynchronous} {Collection} of {Text}, {Image}, {Audio} and {Video}},
	url = {https://aclanthology.org/D17-1114/},
	doi = {10.18653/v1/D17-1114},
	abstract = {The rapid increase of the multimedia data over the Internet necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For audio information, we design an approach to selectively use its transcription. For vision information, we learn joint representations of texts and images using a neural network. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, non-redundancy, readability and coverage through budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our method outperforms other competitive baseline methods.},
	urldate = {2025-03-07},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Haoran and Zhu, Junnan and Ma, Cong and Zhang, Jiajun and Zong, Chengqing},
	editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
	month = sep,
	year = {2017},
	pages = {1092--1102},
}

@article{hovy_automated_nodate,
	title = {Automated {Summarization} {Evaluation} with {Basic} {Elements}},
	abstract = {As part of evaluating a summary automatically, it is usual to determine how much of the contents of one or more humanproduced ‘ideal’ summaries it contains. Previous automated methods such as ROUGE compare using fixed word ngrams, which are not ideal for a variety of reasons. In this paper we describe a framework in which summary evaluation measures can be instantiated and compared, and we implement a specific evaluation method using very small units of content, called Basic Elements, that address some of the shortcomings of ngrams. This method is tested on DUC 2003, 2004, and 2005 systems and produces very good correlations with human judgments.},
	language = {en},
	author = {Hovy, Eduard and Lin, Chin-Yew and Zhou, Liang and Fukumoto, Junichi},
}

@inproceedings{raina_is_2024,
	address = {Miami, Florida, USA},
	title = {Is {LLM}-as-a-{Judge} {Robust}? {Investigating} {Universal} {Adversarial} {Attacks} on {Zero}-shot {LLM} {Assessment}},
	shorttitle = {Is {LLM}-as-a-{Judge} {Robust}?},
	url = {https://aclanthology.org/2024.emnlp-main.427},
	doi = {10.18653/v1/2024.emnlp-main.427},
	language = {en},
	urldate = {2025-03-07},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Raina, Vyas and Liusie, Adian and Gales, Mark},
	year = {2024},
	pages = {7499--7517},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https: //github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	language = {en},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{zheng_judging_2023-1,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {46595--46623},
}

@article{lin_rouge_nodate,
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.},
	language = {en},
	author = {Lin, Chin-Yew},
}

@inproceedings{post_call_2018,
	address = {Belgium, Brussels},
	title = {A {Call} for {Clarity} in {Reporting} {BLEU} {Scores}},
	url = {http://aclweb.org/anthology/W18-6319},
	doi = {10.18653/v1/W18-6319},
	abstract = {The ﬁeld of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to “the” BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to ﬁnd, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, ﬁnding differences as high as 1.8 between commonly used conﬁgurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU,1 to facilitate this.},
	language = {en},
	urldate = {2025-03-07},
	booktitle = {Proceedings of the {Third} {Conference} on {Machine} {Translation}: {Research} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Post, Matt},
	year = {2018},
	pages = {186--191},
}

@inproceedings{zhao_moverscore_2019,
	address = {Hong Kong, China},
	title = {{MoverScore}: {Text} {Generation} {Evaluating} with {Contextualized} {Embeddings} and {Earth} {Mover} {Distance}},
	shorttitle = {{MoverScore}},
	url = {https://www.aclweb.org/anthology/D19-1053},
	doi = {10.18653/v1/D19-1053},
	language = {en},
	urldate = {2025-03-07},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M. and Eger, Steffen},
	year = {2019},
	pages = {563--578},
}

@article{noauthor_proceedings_nodate,
	title = {Proceedings of the...},
	abstract = {We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.},
	language = {en},
}

@inproceedings{maekawa_corpus_2003,
	title = {Corpus of spontaneous {Japanese}: its design and evaluation},
	shorttitle = {Corpus of spontaneous {Japanese}},
	url = {https://www.isca-archive.org/sspr_2003/maekawa03_sspr.html},
	urldate = {2025-03-06},
	author = {Maekawa, Kikuo},
	year = {2003},
	pages = {paper MMO2},
        booktitle = {ISCA/IEEE Workshop on Spontaneous Speech Processing and Recognition},
}

@book{salton_introduction_1983,
	title = {Introduction to {Modern} {Information} {Retrieval}},
	isbn = {978-0-07-054484-0},
	abstract = {Examines Concepts, Functions \& Processes of Information Retrieval Systems},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Salton, Gerard and McGill, Michael J.},
	year = {1983},
	note = {Google-Books-ID: 7f5TAAAAMAAJ},
	keywords = {Computers / System Administration / Storage \& Retrieval},
}

@inproceedings{chen_places_2023,
	address = {Dubrovnik, Croatia},
	title = {{PLACES}: {Prompting} {Language} {Models} for {Social} {Conversation} {Synthesis}},
	shorttitle = {{PLACES}},
	url = {https://aclanthology.org/2023.findings-eacl.63/},
	doi = {10.18653/v1/2023.findings-eacl.63},
	abstract = {Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.},
	urldate = {2025-03-05},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Maximillian and Papangelis, Alexandros and Tao, Chenyang and Kim, Seokhwan and Rosenbaum, Andy and Liu, Yang and Yu, Zhou and Hakkani-Tur, Dilek},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {844--868},
}

@inproceedings{vico_ted_2022,
	title = {{TED} {Talk} {Teaser} {Generation} with {Pre}-{Trained} {Models}},
	url = {https://ieeexplore.ieee.org/document/9746700},
	doi = {10.1109/ICASSP43922.2022.9746700},
	abstract = {While we have seen significant advances in automatic summarization for text, research on speech summarization is still limited. In this work, we address the challenge of automatically generating teasers for TED talks. In the first step, we create a corpus for automatic summarization of TED and TEDx talks consisting of the talks' recording, their transcripts and their descriptions. The corpus is used to build a speech summarization system for the task. We adapt and combine pre-trained models for automatic speech recognition (ASR) and text summarization using the collected data. This initial work shows that is more important to adapt the summarization model to the ASR transcripts than to adapt the ASR model to the talks.},
	urldate = {2025-03-04},
	booktitle = icassp # ", 2022",
	author = {Vico, Gianluca and Niehues, Jan},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Acoustic beams, Acoustics, Adaptation models, Conferences, Data models, Decoding, Signal processing, abstractive summarization, automatic speech recognition, speech summarization},
	pages = {8067--8071},
}

@inproceedings{ghosal_overview_2023,
	address = {Prague, Czechia},
	title = {Overview of the {Second} {Shared} {Task} on {Automatic} {Minuting} ({AutoMin}) at {INLG} 2023},
	url = {https://aclanthology.org/2023.inlg-genchal.19/},
	abstract = {In this article, we report the findings of the second shared task on Automatic Minuting (AutoMin) held as a Generation Challenge at the 16th International Natural Language Generation (INLG) Conference 2023. The second Automatic Minuting shared task is a successor to the first AutoMin which took place in 2021. The primary objective of the AutoMin shared task is to garner participation of the speech and natural language processing and generation community to create automatic methods for generating minutes from multi-party meetings. Five teams from diverse backgrounds participated in the shared task this year. A lot has changed in the Generative AI landscape since the last AutoMin especially with the emergence and wide adoption of Large Language Models (LLMs) to different downstream tasks. Most of the contributions are based on some form of an LLM and we are also adding current outputs of GPT4 as a benchmark. Furthermore, we examine the applicability of GPT-4 for automatic scoring of minutes. Compared to the previous instance of AutoMin, we also add another domain, the minutes for EU Parliament sessions, and we experiment with a more fine-grained manual evaluation. More details on the event can be found at https://ufal.github.io/automin-2023/.},
	urldate = {2025-03-04},
	booktitle = {Proceedings of the 16th {International} {Natural} {Language} {Generation} {Conference}: {Generation} {Challenges}},
	publisher = {Association for Computational Linguistics},
	author = {Ghosal, Tirthankar and Bojar, Ondřej and Hledíková, Marie and Kocmi, Tom and Nedoluzhko, Anna},
	editor = {Mille, Simon},
	month = sep,
	year = {2023},
	pages = {138--167},
}

@inproceedings{manakul_cued_speech_2020,
	series = {{NIST} {Special} {Publication}},
	title = {{CUED}\_SPEECH at {TREC} 2020 {Podcast} {Summarisation} {Track}},
	volume = {1266},
	url = {https://trec.nist.gov/pubs/trec29/papers/cued\_speech.P.pdf},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {Text} {REtrieval} {Conference}, {TREC} 2020, {Virtual} {Event} [{Gaithersburg}, {Maryland}, {USA}], {November} 16-20, 2020},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Manakul, Potsawee and Gales, Mark J. F.},
	editor = {Voorhees, Ellen M. and Ellis, Angela},
	year = {2020},
}

@article{jangra_survey_2023,
	title = {A {Survey} on {Multi}-modal {Summarization}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3584700},
	doi = {10.1145/3584700},
	abstract = {The new era of technology has brought us to the point where it is convenient for people to share their opinions over an abundance of platforms. These platforms have a provision for the users to express themselves in multiple forms of representations, including text, images, videos, and audio. This, however, makes it difficult for users to obtain all the key information about a topic, making the task of automatic multi-modal summarization (MMS) essential. In this article, we present a comprehensive survey of the existing research in the area of MMS, covering various modalities such as text, image, audio, and video. Apart from highlighting the different evaluation metrics and datasets used for the MMS task, our work also discusses the current challenges and future directions in this field.},
	number = {13s},
	journal = {ACM Comput. Surv.},
	author = {Jangra, Anubhav and Mukherjee, Sourajit and Jatowt, Adam and Saha, Sriparna and Hasanuzzaman, Mohammad},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Summarization, multi-modal content processing, neural networks},
}

@misc{noauthor_survey_nodate,
	title = {A {Survey} on {Multi}-modal {Summarization} {\textbar} {ACM} {Computing} {Surveys}},
	url = {https://dl.acm.org/doi/10.1145/3584700},
	urldate = {2025-03-03},
}

@misc{noauthor_210905199_nodate,
	title = {[2109.05199] {A} {Survey} on {Multi}-modal {Summarization}},
	url = {https://arxiv.org/abs/2109.05199},
	urldate = {2025-03-03},
}

@inproceedings{zechner_diasumm_2000,
	title = {{DIASUMM}: {Flexible} {Summarization} of {Spontaneous} {Dialogues} in {Unrestricted} {Domains}},
	shorttitle = {{DIASUMM}},
	url = {https://aclanthology.org/C00-2140/},
	urldate = {2025-03-03},
	booktitle = {{COLING} 2000 {Volume} 2: {The} 18th {International} {Conference} on {Computational} {Linguistics}},
	author = {Zechner, Klaus and Waibel, Alex},
	year = {2000},
}

@inproceedings{hu_meetingbank_2023,
	address = {Toronto, Canada},
	title = {{MeetingBank}: {A} {Benchmark} {Dataset} for {Meeting} {Summarization}},
	shorttitle = {{MeetingBank}},
	url = {https://aclanthology.org/2023.acl-long.906/},
	doi = {10.18653/v1/2023.acl-long.906},
	abstract = {As the number of recorded meetings increases, it becomes increasingly important to utilize summarization technology to create useful summaries of these recordings. However, there is a crucial lack of annotated meeting corpora for developing this technology, as it can be hard to collect meetings, especially when the topics discussed are confidential. Furthermore, meeting summaries written by experienced writers are scarce, making it hard for abstractive summarizers to produce sensible output without a reliable reference. This lack of annotated corpora has hindered the development of meeting summarization technology. In this paper, we present MeetingBank, a new benchmark dataset of city council meetings over the past decade. MeetingBank is unique among other meeting corpora due to its divide-and-conquer approach, which involves dividing professionally written meeting minutes into shorter passages and aligning them with specific segments of the meeting. This breaks down the process of summarizing a lengthy meeting into smaller, more manageable tasks. The dataset provides a new testbed of various meeting summarization systems and also allows the public to gain insight into how council decisions are made. We make the collection, including meeting video links, transcripts, reference summaries, agenda, and other metadata, publicly available to facilitate the development of better meeting summarization techniques.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Yebowen and Ganter, Timothy and Deilamsalehy, Hanieh and Dernoncourt, Franck and Foroosh, Hassan and Liu, Fei},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {16409--16423},
}

@misc{zufle_nutshell_2025,
	title = {{NUTSHELL}: {A} {Dataset} for {Abstract} {Generation} from {Scientific} {Talks}},
	shorttitle = {{NUTSHELL}},
	url = {http://arxiv.org/abs/2502.16942},
	doi = {10.48550/arXiv.2502.16942},
	abstract = {Scientific communication is receiving increasing attention in natural language processing, especially to help researches access, summarize, and generate content. One emerging application in this area is Speech-to-Abstract Generation (SAG), which aims to automatically generate abstracts from recorded scientific presentations. SAG enables researchers to efficiently engage with conference talks, but progress has been limited by a lack of large-scale datasets. To address this gap, we introduce NUTSHELL, a novel multimodal dataset of *ACL conference talks paired with their corresponding abstracts. We establish strong baselines for SAG and evaluate the quality of generated abstracts using both automatic metrics and human judgments. Our results highlight the challenges of SAG and demonstrate the benefits of training on NUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to advance research in SAG and foster the development of improved models and evaluation methods.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Züfle, Maike and Papi, Sara and Savoldi, Beatrice and Gaido, Marco and Bentivogli, Luisa and Niehues, Jan},
	month = feb,
	year = {2025},
	note = {arXiv:2502.16942 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{qiu_mmsum_2024,
	title = {{MMSum}: {A} {Dataset} for {Multimodal} {Summarization} and {Thumbnail} {Generation} of {Videos}},
	shorttitle = {{MMSum}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Qiu_MMSum_A_Dataset_for_Multimodal_Summarization_and_Thumbnail_Generation_of_CVPR_2024_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Qiu, Jielin and Zhu, Jiacheng and Han, William and Kumar, Aditesh and Mittal, Karthik and Jin, Claire and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Zhao, Ding and Li, Bo and Wang, Lijuan},
	year = {2024},
	pages = {21909--21921},
        booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2024, Seattle, WA, USA, June 16-22, 2024},
}

@inproceedings{zhu_msmo_2018,
	address = {Brussels, Belgium},
	title = {{MSMO}: {Multimodal} {Summarization} with {Multimodal} {Output}},
	shorttitle = {{MSMO}},
	url = {https://aclanthology.org/D18-1448/},
	doi = {10.18653/v1/D18-1448},
	abstract = {Multimodal summarization has drawn much attention due to the rapid growth of multimedia data. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this task, we first collect a large-scale dataset for MSMO research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (MMAE) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of MMAE.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Junnan and Li, Haoran and Liu, Tianshang and Zhou, Yu and Zhang, Jiajun and Zong, Chengqing},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {4154--4164},
}

@inproceedings{chen_dialogsum_2021,
	address = {Online},
	title = {{DialogSum}: {A} {Real}-{Life} {Scenario} {Dialogue} {Summarization} {Dataset}},
	shorttitle = {{DialogSum}},
	url = {https://aclanthology.org/2021.findings-acl.449/},
	doi = {10.18653/v1/2021.findings-acl.449},
	urldate = {2025-02-28},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Yulong and Liu, Yang and Chen, Liang and Zhang, Yue},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {5062--5074},
}

@inproceedings{cho_streamhover_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{StreamHover}: {Livestream} {Transcript} {Summarization} and {Annotation}},
	shorttitle = {{StreamHover}},
	url = {https://aclanthology.org/2021.emnlp-main.520/},
	doi = {10.18653/v1/2021.emnlp-main.520},
	abstract = {With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present StreamHover, a framework for annotating and summarizing livestream transcripts. With a total of over 500 hours of videos annotated with both extractive and abstractive summaries, our benchmark dataset is significantly larger than currently existing annotated corpora. We explore a neural extractive summarization model that leverages vector-quantized variational autoencoder to learn latent vector representations of spoken utterances and identify salient utterances from the transcripts to form summaries. We show that our model generalizes better and improves performance over strong baselines. The results of this study provide an avenue for future research to improve summarization solutions for efficient browsing of livestreams.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Sangwoo and Dernoncourt, Franck and Ganter, Tim and Bui, Trung and Lipka, Nedim and Chang, Walter and Jin, Hailin and Brandt, Jonathan and Foroosh, Hassan and Liu, Fei},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {6457--6474},
}

@inproceedings{chen_summscreen_2022,
	address = {Dublin, Ireland},
	title = {{SummScreen}: {A} {Dataset} for {Abstractive} {Screenplay} {Summarization}},
	shorttitle = {{SummScreen}},
	url = {https://aclanthology.org/2022.acl-long.589/},
	doi = {10.18653/v1/2022.acl-long.589},
	abstract = {We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Mingda and Chu, Zewei and Wiseman, Sam and Gimpel, Kevin},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {8602--8615},
}

@inproceedings{garmash_cem_2023,
	address = {Cham},
	title = {Cem {Mil} {Podcasts}: {A} {Spoken} {Portuguese} {Document} {Corpus} for {Multi}-modal, {Multi}-lingual and {Multi}-dialect {Information} {Access} {Research}},
	isbn = {978-3-031-42448-9},
	shorttitle = {Cem {Mil} {Podcasts}},
	doi = {10.1007/978-3-031-42448-9_5},
	abstract = {In this paper we describe the Portuguese-language podcast dataset we have released for academic research purposes. We give an overview of how the data was sampled, descriptive statistics over the collection, as well as information about the distribution over Brazilian and Portuguese dialects.},
	language = {en},
	booktitle = {Experimental {IR} {Meets} {Multilinguality}, {Multimodality}, and {Interaction}},
	publisher = {Springer Nature Switzerland},
	author = {Garmash, Ekaterina and Tanaka, Edgar and Clifton, Ann and Correia, Joana and Jat, Sharmistha and Zhu, Winstead and Jones, Rosie and Karlgren, Jussi},
	editor = {Arampatzis, Avi and Kanoulas, Evangelos and Tsikrika, Theodora and Vrochidis, Stefanos and Giachanou, Anastasia and Li, Dan and Aliannejadi, Mohammad and Vlachos, Michalis and Faggioli, Guglielmo and Ferro, Nicola},
	year = {2023},
	keywords = {Dataset, Multi-modal, Podcast, Speech retrieval, Spoken audio, Summarization},
	pages = {48--59},
}

@misc{manakul_podcast_2022,
	title = {Podcast {Summary} {Assessment}: {A} {Resource} for {Evaluating} {Summary} {Assessment} {Methods}},
	shorttitle = {Podcast {Summary} {Assessment}},
	url = {http://arxiv.org/abs/2208.13265},
	doi = {10.48550/arXiv.2208.13265},
	abstract = {Automatic summary assessment is useful for both machine-generated and human-produced summaries. Automatically evaluating the summary text given the document enables, for example, summary generation system development and detection of inappropriate summaries. Summary assessment can be run in a number of modes: ranking summary generation systems; ranking summaries of a particular document; and estimating the quality of a document-summary pair on an absolute scale. Existing datasets with annotation for summary assessment are usually based on news summarization datasets such as CNN/DailyMail or XSum. In this work, we describe a new dataset, the podcast summary assessment corpus, a collection of podcast summaries that were evaluated by human experts at TREC2020. Compared to existing summary assessment data, this dataset has two unique aspects: (i) long-input, speech podcast based, documents; and (ii) an opportunity to detect inappropriate reference summaries in podcast corpus. First, we examine existing assessment methods, including model-free and model-based methods, and provide benchmark results for this long-input summary assessment dataset. Second, with the aim of filtering reference summary-document pairings for training, we apply summary assessment for data selection. The experimental results on these two aspects provide interesting insights on the summary assessment and generation tasks. The podcast summary assessment data is available.},
	urldate = {2025-02-28},
	publisher = {arXiv},
	author = {Manakul, Potsawee and Gales, Mark J. F.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13265 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{gong_cream_2024,
	title = {{CREAM}: {Comparison}-{Based} {Reference}-{Free} {ELO}-{Ranked} {Automatic} {Evaluation} for {Meeting} {Summarization}},
	shorttitle = {{CREAM}},
	url = {http://arxiv.org/abs/2409.10883},
	doi = {10.48550/arXiv.2409.10883},
	abstract = {Large Language Models (LLMs) have spurred interest in automatic evaluation methods for summarization, offering a faster, more cost-effective alternative to human evaluation. However, existing methods often fall short when applied to complex tasks like long-context summarizations and dialogue-based meeting summarizations. In this paper, we introduce CREAM (Comparison-Based Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a novel framework that addresses the unique challenges of evaluating meeting summaries. CREAM leverages a combination of chain-of-thought reasoning and key facts alignment to assess conciseness and completeness of model-generated summaries without requiring reference. By employing an ELO ranking system, our approach provides a robust mechanism for comparing the quality of different models or prompt configurations.},
	urldate = {2025-02-28},
	publisher = {arXiv},
	author = {Gong, Ziwei and Ai, Lin and Deshpande, Harshsaiprasad and Johnson, Alexander and Phung, Emmy and Wu, Zehui and Emami, Ahmad and Hirschberg, Julia},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10883 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{zhong_towards_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Towards a {Unified} {Multi}-{Dimensional} {Evaluator} for {Text} {Generation}},
	url = {https://aclanthology.org/2022.emnlp-main.131/},
	doi = {10.18653/v1/2022.emnlp-main.131},
	abstract = {Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UniEval for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UniEval correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UniEval achieves a 23\% higher correlation on text summarization, and over 43\% on dialogue response generation. Also, UniEval demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data, and all pre-trained evaluators are available at https://github.com/maszhongming/UniEval.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Ming and Liu, Yang and Yin, Da and Mao, Yuning and Jiao, Yizhu and Liu, Pengfei and Zhu, Chenguang and Ji, Heng and Han, Jiawei},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {2023--2038},
}

@inproceedings{akula_sentence_2022,
	address = {Marseille, France},
	title = {Sentence {Pair} {Embeddings} {Based} {Evaluation} {Metric} for {Abstractive} and {Extractive} {Summarization}},
	url = {https://aclanthology.org/2022.lrec-1.646/},
	abstract = {The development of an automatic evaluation metric remains an open problem in text generation. Widely used evaluation metrics, like ROUGE and BLEU, are based on exact word matching and fail to capture semantic similarity. Recent works, such as BERTScore, MoverScore and, Sentence Mover`s Similarity, are an improvement over these standard metrics as they use the contextualized word or sentence embeddings to capture semantic similarity. We in this work, propose a novel evaluation metric, Sentence Pair EmbEDdings (SPEED) Score, for text generation which is based on semantic similarity between sentence pairs as opposed to earlier approaches. To find semantic similarity between a pair of sentences, we obtain sentence-level embeddings from multiple transformer models pre-trained specifically on various sentence pair tasks such as Paraphrase Detection (PD), Semantic Text Similarity (STS), and Natural Language Inference (NLI). As these sentence pair tasks involve capturing the semantic similarity between a pair of input texts, we leverage these models in our metric computation. Our proposed evaluation metric shows an impressive performance in evaluating both abstractive and extractive summarization models and achieves state-of-the-art results on the SummEval dataset, demonstrating the effectiveness of our approach. Also, we perform the run-time analysis to show that our proposed metric is faster than the current state-of-the-art.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Akula, Ramya and Garibay, Ivan},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {6009--6017},
}

@inproceedings{inoue_improvement_2004,
	title = {Improvement of speech summarization using prosodic information},
	url = {https://www.isca-archive.org/speechprosody_2004/inoue04_speechprosody.html},
	doi = {10.21437/SpeechProsody.2004-138},
	language = {en},
	urldate = {2025-02-28},
	booktitle = {Speech {Prosody} 2004},
	publisher = {ISCA},
	author = {Inoue, Akira and Mikami, Takayoshi and Yamashita, Yoichi},
	month = mar,
	year = {2004},
	pages = {599--602},
}

@article{gambhir_recent_2017,
	title = {Recent automatic text summarization techniques: a survey},
	volume = {47},
	url = {https://doi.org/10.1007/s10462-016-9475-9},
	doi = {10.1007/S10462-016-9475-9},
	number = {1},
	journal = {Artif. Intell. Rev.},
	author = {Gambhir, Mahak and Gupta, Vishal},
	year = {2017},
	pages = {1--66},
}

@article{el-kassas_automatic_2021,
	title = {Automatic text summarization: {A} comprehensive survey},
	volume = {165},
	issn = {0957-4174},
	shorttitle = {Automatic text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	doi = {10.1016/j.eswa.2020.113679},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.},
	urldate = {2025-02-28},
	journal = {Expert Systems with Applications},
	author = {El-Kassas, Wafaa S. and Salama, Cherif R. and Rafea, Ahmed A. and Mohamed, Hoda K.},
	month = mar,
	year = {2021},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization evaluation, Text summarization techniques},
	pages = {113679},
}

@misc{noauthor_recent_nodate,
	title = {Recent automatic text summarization techniques: a survey: {Artificial} {Intelligence} {Review}: {Vol} 47, {No} 1},
	url = {https://dl.acm.org/doi/10.1007/s10462-016-9475-9},
	urldate = {2025-02-28},
}

@article{supriyono_survey_2024,
	title = {A survey of text summarization: {Techniques}, evaluation and challenges},
	volume = {7},
	issn = {2949-7191},
	shorttitle = {A survey of text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719124000189},
	doi = {10.1016/j.nlp.2024.100070},
	abstract = {This paper explores the complex field of text summarization in Natural Language Processing (NLP), with particular attention to the development and importance of semantic understanding. Text summarization is a crucial component of natural language processing (NLP), which helps to translate large amounts of textual data into clear and understandable representations. As the story progresses, it demonstrates the dynamic transition from simple syntactic structures to sophisticated models with semantic comprehension. In order to effectively summarize, syntactic, semantic, and pragmatic concerns become crucial, highlighting the necessity of capturing not only grammar but also the context and underlying meaning. It examines the wide range of summarization models, from conventional extractive techniques to state-of-the-art tools like pre-trained models. Applications are found in many different fields, demonstrating how versatile summarizing techniques are. Semantic drift and domain-specific knowledge remain obstacles, despite progress. In the future, the study predicts developments like artificial intelligence integration and transfer learning, which motivates academics to investigate these prospects for advancement. The approach, which is based on the PRISMA framework, emphasizes a methodical and open literature review. The work attempts to further natural language processing (NLP) and text summarization by combining various research findings and suggesting future research directions in this dynamic subject.},
	urldate = {2025-02-28},
	journal = {Natural Language Processing Journal},
	author = {{Supriyono} and Wibawa, Aji Prasetya and {Suyono} and Kurniawan, Fachrul},
	month = jun,
	year = {2024},
	keywords = {Natural language processing, PRISMA, Semantic, Syntactic, Text summarization, Transfer learning},
	pages = {100070},
}

@misc{rezazadegan_automatic_2020,
	title = {Automatic {Speech} {Summarisation}: {A} {Scoping} {Review}},
	shorttitle = {Automatic {Speech} {Summarisation}},
	url = {http://arxiv.org/abs/2008.11897},
	doi = {10.48550/arXiv.2008.11897},
	abstract = {Speech summarisation techniques take human speech as input and then output an abridged version as text or speech. Speech summarisation has applications in many domains from information technology to health care, for example improving speech archives or reducing clinical documentation burden. This scoping review maps the speech summarisation literature, with no restrictions on time frame, language summarised, research method, or paper type. We reviewed a total of 110 papers out of a set of 153 found through a literature search and extracted speech features used, methods, scope, and training corpora. Most studies employ one of four speech summarisation architectures: (1) Sentence extraction and compaction; (2) Feature extraction and classification or rank-based sentence selection; (3) Sentence compression and compression summarisation; and (4) Language modelling. We also discuss the strengths and weaknesses of these different methods and speech features. Overall, supervised methods (e.g. Hidden Markov support vector machines, Ranking support vector machines, Conditional random fields) performed better than unsupervised methods. As supervised methods require manually annotated training data which can be costly, there was more interest in unsupervised methods. Recent research into unsupervised methods focusses on extending language modelling, for example by combining Uni-gram modelling with deep neural networks. Protocol registration: The protocol for this scoping review is registered at https://osf.io.},
	urldate = {2025-02-28},
	publisher = {arXiv},
	author = {Rezazadegan, Dana and Berkovsky, Shlomo and Quiroz, Juan C. and Kocaballi, A. Baki and Wang, Ying and Laranjo, Liliana and Coiera, Enrico},
	month = aug,
	year = {2020},
	note = {arXiv:2008.11897 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@incollection{sudmann_current_2023,
	address = {Bielefeld, Germany},
	edition = {1},
	title = {The current state of summarization},
	isbn = {978-3-8376-6766-0 978-3-8394-6766-4},
	url = {https://www.transcript-open.de/doi/10.14361/9783839467664-016},
	language = {en},
	urldate = {2024-02-08},
	booktitle = {Beyond {Quantity}. {Research} with {Subsymbolic} {AI}},
	publisher = {transcript Verlag},
	author = {Retkowski, Fabian},
	editor = {Sudmann, Andreas and Echterhölter, Anna and Ramsauer, Markus and Retkowski, Fabian and Schröter, Jens and Waibel, Alexander},
	month = nov,
	year = {2023},
	doi = {10.14361/9783839467664-016},
	pages = {291--312},
}

@misc{lv_vt-ssum_2021,
	title = {{VT}-{SSum}: {A} {Benchmark} {Dataset} for {Video} {Transcript} {Segmentation} and {Summarization}},
	shorttitle = {{VT}-{SSum}},
	url = {http://arxiv.org/abs/2106.05606},
	doi = {10.48550/arXiv.2106.05606},
	abstract = {Video transcript summarization is a fundamental task for video understanding. Conventional approaches for transcript summarization are usually built upon the summarization data for written language such as news articles, while the domain discrepancy may degrade the model performance on spoken text. In this paper, we present VT-SSum, a benchmark dataset with spoken language for video transcript segmentation and summarization, which includes 125K transcript-summary pairs from 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET by leveraging the slides content as the weak supervision to generate the extractive summary for video transcripts. Experiments with a state-of-the-art deep learning approach show that the model trained with VT-SSum brings a significant improvement on the AMI spoken text summarization benchmark. VT-SSum is publicly available at https://github.com/Dod-o/VT-SSum to support the future research of video transcript segmentation and summarization tasks.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Lv, Tengchao and Cui, Lei and Vasilijevic, Momcilo and Wei, Furu},
	month = jul,
	year = {2021},
	note = {arXiv:2106.05606 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{retkowski_text_2024,
	address = {St. Julian's, Malta},
	title = {From {Text} {Segmentation} to {Smart} {Chaptering}: {A} {Novel} {Benchmark} for {Structuring} {Video} {Transcriptions}},
	shorttitle = {From {Text} {Segmentation} to {Smart} {Chaptering}},
	url = {https://aclanthology.org/2024.eacl-long.25/},
	abstract = {Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical “smart chaptering” task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.},
	urldate = {2025-02-27},
	booktitle = {Proceedings of the 18th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Retkowski, Fabian and Waibel, Alexander},
	editor = {Graham, Yvette and Purver, Matthew},
	month = mar,
	year = {2024},
	pages = {406--419},
}

@inproceedings{gliwa_samsum_2019,
	address = {Hong Kong, China},
	title = {{SAMSum} {Corpus}: {A} {Human}-annotated {Dialogue} {Dataset} for {Abstractive} {Summarization}},
	shorttitle = {{SAMSum} {Corpus}},
	url = {https://aclanthology.org/D19-5409/},
	doi = {10.18653/v1/D19-5409},
	abstract = {This paper introduces the SAMSum Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news – in contrast with human evaluators' judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.},
	urldate = {2025-02-27},
	booktitle = {Proceedings of the 2nd {Workshop} on {New} {Frontiers} in {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
	editor = {Wang, Lu and Cheung, Jackie Chi Kit and Carenini, Giuseppe and Liu, Fei},
	month = nov,
	year = {2019},
	pages = {70--79},
}

@inproceedings{matsuura_sentence-wise_2024,
	title = {Sentence-wise {Speech} {Summarization}: {Task}, {Datasets}, and {End}-to-{End} {Modeling} with {LM} {Knowledge} {Distillation}},
	booktitle = interspeech # " 2024",
	author = {Matsuura, Kohei and Ashihara, Takanori and Moriya, Takafumi and Mimura, Masato and Kano, Takatomo and Ogawa, Atsunori and Delcroix, Marc},
	year = {2024},
	pages = {1945--1949},
        url = {https://www.isca-archive.org/interspeech_2024/matsuura24_interspeech.pdf},
}

@inproceedings{lev_talksumm_2019,
	address = {Florence, Italy},
	title = {{TalkSumm}: {A} {Dataset} and {Scalable} {Annotation} {Method} for {Scientific} {Paper} {Summarization} {Based} on {Conference} {Talks}},
	shorttitle = {{TalkSumm}},
	url = {https://aclanthology.org/P19-1204/},
	doi = {10.18653/v1/P19-1204},
	abstract = {Currently, no large-scale training data is available for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers' content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts.},
	urldate = {2025-02-27},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lev, Guy and Shmueli-Scheuer, Michal and Herzig, Jonathan and Jerbi, Achiya and Konopnicki, David},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {2125--2131},
}
@inproceedings{le-duc_real-time_2024,
	title = {Real-time {Speech} {Summarization} for {Medical} {Conversations}},
	url = {https://www.isca-archive.org/interspeech_2024/leduc24_interspeech.html},
	doi = {10.21437/Interspeech.2024-2250},
	language = {en},
	urldate = {2025-02-27},
	booktitle = interspeech # " 2024",
	publisher = {ISCA},
	author = {Le-Duc, Khai and Nguyen, Khai-Nguyen and Vo-Dang, Long and Hy, Truong-Son},
	month = sep,
	year = {2024},
	pages = {1960--1964},
}

@misc{le-duc_real-time_2024-1,
	title = {Real-time {Speech} {Summarization} for {Medical} {Conversations}},
	url = {http://arxiv.org/abs/2406.15888},
	doi = {10.48550/arXiv.2406.15888},
	abstract = {In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: https://github.com/leduckhai/MultiMed},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Le-Duc, Khai and Nguyen, Khai-Nguyen and Vo-Dang, Long and Hy, Truong-Son},
	month = jun,
	year = {2024},
	note = {arXiv:2406.15888 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{shon_slue_2023,
	address = {Toronto, Canada},
	title = {{SLUE} {Phase}-2: {A} {Benchmark} {Suite} of {Diverse} {Spoken} {Language} {Understanding} {Tasks}},
	shorttitle = {{SLUE} {Phase}-2},
	url = {https://aclanthology.org/2023.acl-long.496/},
	doi = {10.18653/v1/2023.acl-long.496},
	abstract = {Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models' performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models.},
	urldate = {2025-02-27},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shon, Suwon and Arora, Siddhant and Lin, Chyi-Jiunn and Pasad, Ankita and Wu, Felix and Sharma, Roshan and Wu, Wei-Lun and Lee, Hung-yi and Livescu, Karen and Watanabe, Shinji},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {8906--8937},
}

@inproceedings{palaskar_multimodal_2019,
	address = {Florence, Italy},
	title = {Multimodal {Abstractive} {Summarization} for {How2} {Videos}},
	url = {https://aclanthology.org/P19-1659/},
	doi = {10.18653/v1/P19-1659},
	abstract = {In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to “compress” text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.},
	urldate = {2025-02-26},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Palaskar, Shruti and Libovický, Jindřich and Gella, Spandana and Metze, Florian},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {6587--6596},
}

@inproceedings{lin_csds_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{CSDS}: {A} {Fine}-{Grained} {Chinese} {Dataset} for {Customer} {Service} {Dialogue} {Summarization}},
	shorttitle = {{CSDS}},
	url = {https://aclanthology.org/2021.emnlp-main.365/},
	doi = {10.18653/v1/2021.emnlp-main.365},
	abstract = {Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer`s issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers' viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies.},
	urldate = {2025-02-26},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Haitao and Ma, Liqun and Zhu, Junnan and Xiang, Lu and Zhou, Yu and Zhang, Jiajun and Zong, Chengqing},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4436--4451},
}

@inproceedings{feigenblat_tweetsumm_2021,
	address = {Punta Cana, Dominican Republic},
	title = {{TWEETSUMM} - {A} {Dialog} {Summarization} {Dataset} for {Customer} {Service}},
	url = {https://aclanthology.org/2021.findings-emnlp.24/},
	doi = {10.18653/v1/2021.findings-emnlp.24},
	abstract = {In a typical customer service chat scenario, customers contact a support center to ask for help or raise complaints, and human agents try to solve the issues. In most cases, at the end of the conversation, agents are asked to write a short summary emphasizing the problem and the proposed solution, usually for the benefit of other agents that may have to deal with the same customer or issue. The goal of the present article is advancing the automation of this task. We introduce the first large scale, high quality, customer care dialog summarization dataset with close to 6500 human annotated summaries. The data is based on real-world customer support dialogs and includes both extractive and abstractive summaries. We also introduce a new unsupervised, extractive summarization method specific to dialogs.},
	urldate = {2025-02-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Feigenblat, Guy and Gunasekara, Chulaka and Sznajder, Benjamin and Joshi, Sachindra and Konopnicki, David and Aharonov, Ranit},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {245--260},
}

@inproceedings{zhu_mediasum_2021,
	address = {Online},
	title = {{MediaSum}: {A} {Large}-scale {Media} {Interview} {Dataset} for {Dialogue} {Summarization}},
	shorttitle = {{MediaSum}},
	url = {https://aclanthology.org/2021.naacl-main.474/},
	doi = {10.18653/v1/2021.naacl-main.474},
	abstract = {This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model`s performance on other dialogue summarization tasks.},
	urldate = {2025-02-26},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Chenguang and Liu, Yang and Mei, Jie and Zeng, Michael},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {5927--5934},
}

@inproceedings{nedoluzhko_elitr_2022,
	address = {Marseille, France},
	title = {{ELITR} {Minuting} {Corpus}: {A} {Novel} {Dataset} for {Automatic} {Minuting} from {Multi}-{Party} {Meetings} in {English} and {Czech}},
	shorttitle = {{ELITR} {Minuting} {Corpus}},
	url = {https://aclanthology.org/2022.lrec-1.340/},
	abstract = {Taking minutes is an essential component of every meeting, although the goals, style, and procedure of this activity (“minuting” for short) can vary. Minuting is a rather unstructured writing activity and is affected by who is taking the minutes and for whom the intended minutes are. With the rise of online meetings, automatic minuting would be an important benefit for the meeting participants as well as for those who might have missed the meeting. However, automatically generating meeting minutes is a challenging problem due to a variety of factors including the quality of automatic speech recorders (ASRs), availability of public meeting data, subjective knowledge of the minuter, etc. In this work, we present the first of its kind dataset on Automatic Minuting. We develop a dataset of English and Czech technical project meetings which consists of transcripts generated from ASRs, manually corrected, and minuted by several annotators. Our dataset, AutoMin, consists of 113 (English) and 53 (Czech) meetings, covering more than 160 hours of meeting content. Upon acceptance, we will publicly release (aaa.bbb.ccc) the dataset as a set of meeting transcripts and minutes, excluding the recordings for privacy reasons. A unique feature of our dataset is that most meetings are equipped with more than one minute, each created independently. Our corpus thus allows studying differences in what people find important while taking the minutes. We also provide baseline experiments for the community to explore this novel problem further. To the best of our knowledge AutoMin is probably the first resource on minuting in English and also in a language other than English (Czech).},
	urldate = {2025-02-26},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Nedoluzhko, Anna and Singh, Muskaan and Hledíková, Marie and Ghosal, Tirthankar and Bojar, Ondřej},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {3174--3182},
}

@inproceedings{janin_icsi_2003,
	title = {The {ICSI} {Meeting} {Corpus}},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/1198793},
	doi = {10.1109/ICASSP.2003.1198793},
	abstract = {We have collected a corpus of data from natural meetings that occurred at the International Computer Science Institute (ICSI) in Berkeley, California over the last three years. The corpus contains audio recorded simultaneously from head-worn and table-top microphones, word-level transcripts of meetings, and various metadata on participants, meetings, and hardware. Such a corpus supports work in automatic speech recognition, noise robustness, dialog modeling, prosody, rich transcription, information retrieval, and more. We present details on the contents of the corpus, as well as rationales for the decisions that led to its configuration. The corpus were delivered to the Linguistic Data Consortium (LDC).},
	urldate = {2025-02-26},
	booktitle = icassp # ", 2003",
	author = {Janin, A. and Baron, D. and Edwards, J. and Ellis, D. and Gelbart, D. and Morgan, N. and Peskin, B. and Pfau, T. and Shriberg, E. and Stolcke, A. and Wooters, C.},
	month = apr,
	year = {2003},
	note = {ISSN: 1520-6149},
	keywords = {Audio recording, Microphones, Speech processing, Speech recognition},
	pages = {I--I},
}

@inproceedings{carletta_ami_2006,
	address = {Berlin, Heidelberg},
	title = {The {AMI} {Meeting} {Corpus}: {A} {Pre}-announcement},
	isbn = {978-3-540-32550-5},
	shorttitle = {The {AMI} {Meeting} {Corpus}},
	doi = {10.1007/11677482_3},
	abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
	language = {en},
	booktitle = {Machine {Learning} for {Multimodal} {Interaction}},
	publisher = {Springer},
	author = {Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre},
	editor = {Renals, Steve and Bengio, Samy},
	year = {2006},
	keywords = {Acoustic Model, Design Team, Industrial Designer, Marketing Expert, Pronunciation Variation},
	pages = {28--39},
}

@misc{rasheed_can_2024,
	title = {Can {Large} {Language} {Models} {Serve} as {Data} {Analysts}? {A} {Multi}-{Agent} {Assisted} {Approach} for {Qualitative} {Data} {Analysis}},
	shorttitle = {Can {Large} {Language} {Models} {Serve} as {Data} {Analysts}?},
	url = {http://arxiv.org/abs/2402.01386},
	doi = {10.48550/arXiv.2402.01386},
	abstract = {Recent advancements in Large Language Models (LLMs) have enabled collaborative human-bot interactions in Software Engineering (SE), similar to many other professions. However, the potential benefits and implications of incorporating LLMs into qualitative data analysis in SE have not been completely explored. For instance, conducting qualitative data analysis manually can be a time-consuming, effort-intensive, and error-prone task for researchers. LLM-based solutions, such as generative AI models trained on massive datasets, can be utilized to automate tasks in software development as well as in qualitative data analysis. To this end, we utilized LLMs to automate and expedite the qualitative data analysis processes. We employed a multi-agent model, where each agent was tasked with executing distinct, individual research related activities. Our proposed model interpreted large quantities of textual documents and interview transcripts to perform several common tasks used in qualitative analysis. The results show that this technical assistant speeds up significantly the data analysis process, enabling researchers to manage larger datasets much more effectively. Furthermore, this approach introduces a new dimension of scalability and accuracy in qualitative research, potentially transforming data interpretation methodologies in SE.},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {Rasheed, Zeeshan and Waseem, Muhammad and Ahmad, Aakash and Kemell, Kai-Kristian and Xiaofeng, Wang and Duc, Anh Nguyen and Abrahamsson, Pekka},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01386 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{liu_lost_2024,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	volume = {12},
	shorttitle = {Lost in the {Middle}},
	url = {https://aclanthology.org/2024.tacl-1.9/},
	doi = {10.1162/tacl_a_00638},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
	urldate = {2025-02-20},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	year = {2024},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {157--173},
}

@article{xi_rise_2025,
	title = {The rise and potential of large language model based agents: a survey},
	volume = {68},
	issn = {1869-1919},
	shorttitle = {The rise and potential of large language model based agents},
	url = {https://doi.org/10.1007/s11432-024-4222-0},
	doi = {10.1007/s11432-024-4222-0},
	abstract = {For a long time, researchers have sought artificial intelligence (AI) that matches or exceeds human intelligence. AI agents, which are artificial entities capable of sensing the environment, making decisions, and taking actions, are seen as a means to achieve this goal. Extensive efforts have been made to develop AI agents, with a primary focus on refining algorithms or training strategies to enhance specific skills or particular task performance. The field, however, lacks a sufficiently general and powerful model to serve as a foundation for building general agents adaptable to diverse scenarios. With their versatile capabilities, large language models (LLMs) pave a promising path for the development of general AI agents, and substantial progress has been made in the realm of LLM-based agents. In this article, we conduct a comprehensive survey on LLM-based agents, covering their construction frameworks, application scenarios, and the exploration of societies built upon LLM-based agents. We also conclude some potential future directions and open problems in this flourishing field.},
	language = {en},
	number = {2},
	urldate = {2025-02-19},
	journal = {Science China Information Sciences},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Zhang, Qi and Gui, Tao},
	month = jan,
	year = {2025},
	keywords = {AI agents, Artificial Intelligence, LLM-based agents, agent society, large language models, natural language processing},
	pages = {121101},
}

@misc{xi_rise_2023,
	title = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}: {A} {Survey}},
	shorttitle = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}},
	url = {http://arxiv.org/abs/2309.07864},
	doi = {10.48550/arXiv.2309.07864},
	abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@book{tuschling_chatgpt_2023,
	title = {{ChatGPT} und andere »{Quatschmaschinen}«: {Gespräche} mit {Künstlicher} {Intelligenz}},
	shorttitle = {{ChatGPT} und andere »{Quatschmaschinen}«},
	url = {https://library.oapen.org/handle/20.500.12657/87490},
	language = {German},
	urldate = {2025-02-19},
	publisher = {transcript Verlag},
	editor = {Tuschling, Anna and Sudmann, Andreas and Dotzler, Bernhard J.},
	year = {2023},
	doi = {10.14361/9783839469088},
	note = {Accepted: 2024-02-02T16:04:26Z},
	keywords = {Ai, Artificial Intelligence, Chat, ChatGPT, Computer, Computer Sciences, Conversation, Culture, Dialog, Dialoque, Digital Media, Digitale Medien, Digitalisierung, Digitalization, Gespräch, Informatik, KI, Kultur, Künstliche Intelligenz, Language, Machine, Machine Learning, Maschine, Maschinelles Lernen, Media, Media Studies, Medien, Medienwissenschaft, Social Media, Sprache, Sprachmodell, Technik, Technology, thema EDItEUR::J Society and Social Sciences::JB Society and culture: general::JBC Cultural and media studies::JBCT Media studies},
}

@book{denzin_sage_2023,
	address = {Los Angeles London New Delhi Singapore Washington DC Melbourne},
	edition = {6},
	title = {The {SAGE} {Handbook} of {Qualitative} {Research}},
	isbn = {978-1-07-183674-3},
	abstract = {This new edition of the SAGE Handbook of Qualitative Researchrepresents the sixth generation of the ongoing conversation about the discipline, practice, and conduct of qualitative inquiry. As with earlier editions, the Sixth Edition is virtually a new volume, with 27 of the 34 chapters representing new topics or approaches not seen in the previous edition, including intersectionality; critical disability research; postcolonial and decolonized knowledge; diffraction and intra-action; social media methodologies; thematic analysis, collaborative inquiry from the borderlands; qualitative inquiry and public health science; co-production and the politics of impact; publishing qualitative research; and academic survival. Authors in the Sixth Edition engage with questions of ontology and epistemology, the politics of the research act, the changing landscape of higher education, and the role qualitative researchers play in contributing to a more just, egalitarian society. To mark the Handbook s 30-year history, we are pleased to offer a bonus PART VI in the eBook versions of the Sixth Edition: this additional section brings together and reprints ten of the most famous or game-changing contributions from the previous five editions. You can bundle the print + eBook version with bundle ISBN: 978-1-0719-2874-5.},
	language = {Englisch},
	publisher = {SAGE Publications, Inc},
	author = {Denzin, Norman K. and Lincoln, Yvonna S. and Giardina, Michael Donald and Cannella, Gaile S.},
	month = jun,
	year = {2023},
}

@book{creswell_qualitative_2017,
	edition = {4},
	title = {Qualitative {Inquiry} and {Research} {Design}: {Choosing} {Among} {Five} {Approaches}},
	isbn = {978-1-5063-3020-4},
	shorttitle = {Qualitative {Inquiry} and {Research} {Design}},
	abstract = {In the revised Fourth Edition of the best-selling text, John W. Creswell and new co-author Cheryl N. Poth explore the philosophical underpinnings, history, and key elements of five qualitative inquiry approaches: narrative research, phenomenology, grounded theory, ethnography, and case study. Preserving Creswell's signature writing style, the authors compare the approaches and relate research designs to each of the traditions of inquiry in a highly accessible manner. Featuring new content, articles, pedagogy, references, and expanded coverage of ethics throughout, the Fourth Edition is an ideal introduction to the theories, strategies, and practices of qualitative inquiry.},
	language = {Englisch},
	publisher = {SAGE Publications, Inc},
	author = {Creswell, John W. and Poth, Cheryl N.},
	month = jan,
	year = {2017},
}

@article{ziems_can_2024,
	title = {Can {Large} {Language} {Models} {Transform} {Computational} {Social} {Science}?},
	volume = {50},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00502},
	doi = {10.1162/coli_a_00502},
	abstract = {Large language models (LLMs) are capable of successfully performing many language
processing tasks zero-shot (without training data). If zero-shot LLMs can also
reliably classify and explain social phenomena like persuasiveness and political
ideology, then LLMs could augment the computational social science (CSS)
pipeline in important ways. This work provides a road map for using LLMs as CSS
tools. Towards this end, we contribute a set of prompting best practices and an
extensive evaluation pipeline to measure the zero-shot performance of 13
language models on 25 representative English CSS benchmarks. On taxonomic
labeling tasks (classification), LLMs fail to outperform the best fine-tuned
models but still achieve fair levels of agreement with humans. On free-form
coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references.
We conclude that the performance of today’s LLMs can augment the CSS
research pipeline in two ways: (1) serving as zero-shot data annotators on human
annotation teams, and (2) bootstrapping challenging creative generation tasks
(e.g., explaining the underlying attributes of a text). In summary, LLMs are
posed to meaningfully participate in social science analysis in partnership with
humans.},
	number = {1},
	urldate = {2025-02-15},
	journal = {Computational Linguistics},
	author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	month = mar,
	year = {2024},
	pages = {237--291},
}

@misc{qin_is_2023,
	title = {Is {ChatGPT} a {General}-{Purpose} {Natural} {Language} {Processing} {Task} {Solver}?},
	url = {http://arxiv.org/abs/2302.06476},
	doi = {10.48550/arXiv.2302.06476},
	abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	month = nov,
	year = {2023},
	note = {arXiv:2302.06476 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{qin_is_2023-1,
	title = {Is {ChatGPT} a {General}-{Purpose} {Natural} {Language} {Processing} {Task} {Solver}?},
	url = {https://openreview.net/forum?id=u03xn1COsO},
	abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot---i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
	language = {en},
	urldate = {2025-02-15},
	author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	month = dec,
	year = {2023},
}

@misc{qin_is_2023-2,
	title = {Is {ChatGPT} a {General}-{Purpose} {Natural} {Language} {Processing} {Task} {Solver}?},
	url = {http://arxiv.org/abs/2302.06476},
	doi = {10.48550/arXiv.2302.06476},
	abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	month = nov,
	year = {2023},
	note = {arXiv:2302.06476 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bang_multitask_2023,
	title = {A {Multitask}, {Multilingual}, {Multimodal} {Evaluation} of {ChatGPT} on {Reasoning}, {Hallucination}, and {Interactivity}},
	url = {http://arxiv.org/abs/2302.04023},
	doi = {10.48550/arXiv.2302.04023},
	abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
	month = nov,
	year = {2023},
	note = {arXiv:2302.04023 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{garcia_if_2023,
	title = {If the {Sources} {Could} {Talk}: {Evaluating} {Large} {Language} {Models} for {Research} {Assistance} in {History}},
	shorttitle = {If the {Sources} {Could} {Talk}},
	url = {http://arxiv.org/abs/2310.10808},
	doi = {10.48550/arXiv.2310.10808},
	abstract = {The recent advent of powerful Large-Language Models (LLM) provides a new conversational form of inquiry into historical memory (or, training data, in this case). We show that by augmenting such LLMs with vector embeddings from highly specialized academic sources, a conversational methodology can be made accessible to historians and other researchers in the Humanities. Concretely, we evaluate and demonstrate how LLMs have the ability of assisting researchers while they examine a customized corpora of different types of documents, including, but not exclusive to: (1). primary sources, (2). secondary sources written by experts, and (3). the combination of these two. Compared to established search interfaces for digital catalogues, such as metadata and full-text search, we evaluate the richer conversational style of LLMs on the performance of two main types of tasks: (1). question-answering, and (2). extraction and organization of data. We demonstrate that LLMs semantic retrieval and reasoning abilities on problem-specific tasks can be applied to large textual archives that have not been part of the its training data. Therefore, LLMs can be augmented with sources relevant to specific research projects, and can be queried privately by researchers.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Garcia, Giselle Gonzalez and Weilbach, Christian},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10808 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@article{borko_automatic_1963,
	title = {Automatic {Document} {Classification}},
	volume = {10},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/321160.321165},
	doi = {10.1145/321160.321165},
	language = {en},
	number = {2},
	urldate = {2025-02-13},
	journal = {Journal of the ACM},
	author = {Borko, Harold and Bernick, Myrna},
	month = apr,
	year = {1963},
	pages = {151--162},
}

@article{salton_vector_1975,
	title = {A vector space model for automatic indexing},
	volume = {18},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/361219.361220},
	doi = {10.1145/361219.361220},
	abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
	language = {en},
	number = {11},
	urldate = {2025-02-13},
	journal = {Communications of the ACM},
	author = {Salton, G. and Wong, A. and Yang, C. S.},
	month = nov,
	year = {1975},
	pages = {613--620},
}

@misc{noauthor_vector_nodate,
	title = {A {Vector} {Space} {Model} for {Automatic} {Indexing} - {Google} {Suche}},
	url = {https://www.google.com/search?client=firefox-b-d&q=A+Vector+Space+Model+for+Automatic+Indexing},
	urldate = {2025-02-13},
}

@book{ritchie_qualitative_2014,
	title = {Qualitative {Research} {Practice}: {A} {Guide} for {Social} {Science} {Students} and {Researchers}},
	isbn = {978-1-4462-0911-0},
	shorttitle = {Qualitative {Research} {Practice}},
	abstract = {Why use qualitative methods? What kinds of questions can qualitative methods help you answer? How do you actually do rigorous and reflective qualitative research in the real world?   Written by a team of leading researchers associated with NatCen Social Research (the National Centre for Social Research) this textbook leads students and researchers through the entire process of qualitative research from beginning to end - moving through design, sampling, data collection, analysis and reporting. In this fully revised second edition you will find:  A practical account of how to carry out qualitative research which recognises a range of current approaches and applications  A brand new chapter on ethics  A brand new chapter on observational research  Updated advice on using software when analysing your qualitative data  New case studies which illustrate issues you may encounter and how problems have been tackled by other researchers.   This book is an ideal guide for students, practitioners and researchers faced with the challenges of doing qualitative research in both applied and academic settings in messy real-life contexts.},
	language = {en},
	publisher = {SAGE Publications},
	author = {Ritchie, Jane and Lewis, Jane and Nicholls, Carol McNaughton and Ormston, Rachel},
	month = jan,
	year = {2014},
	note = {Google-Books-ID: zkITlwEACAAJ},
	keywords = {Social Science / Research},
}

@misc{noauthor_qualitative_2024,
	title = {Qualitative {Research} {Practice}},
	url = {https://us.sagepub.com/en-us/nam/qualitative-research-practice/book237434},
	abstract = {A Guide for Social Science Students and Researchers},
	language = {en},
	urldate = {2025-02-13},
	journal = {SAGE Publications Inc},
	month = dec,
	year = {2024},
}

@article{sebastiani_machine_2002,
	title = {Machine learning in automated text categorization},
	volume = {34},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/505282.505283},
	doi = {10.1145/505282.505283},
	abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.},
	language = {en},
	number = {1},
	urldate = {2025-02-13},
	journal = {ACM Computing Surveys},
	author = {Sebastiani, Fabrizio},
	month = mar,
	year = {2002},
	pages = {1--47},
}

@article{christou_use_2023,
	title = {The {Use} of {Artificial} {Intelligence} ({Ai}) in {Qualitative} {Research} for {Theory} {Development} (1st edition)},
	volume = {28},
	number = {9},
	journal = {The Qualitative Report},
	author = {Christou, Prokopis A.},
	year = {2023},
	pages = {2739--2755},
}

@incollection{franken_ai_2025,
	address = {Wiesbaden},
	title = {{AI} in and for {Qualitative} {Research}},
	isbn = {978-3-658-08460-8},
	url = {https://doi.org/10.1007/978-3-658-08460-8_98-1},
	abstract = {This chapter discusses current potentials and limitations of the application of AI in qualitative research processes. While AI can help shed light on aspects that might otherwise go undiscovered, it can also exclude ambiguities that qualitative researchers often seek to explore. Therefore, AI should be seen as a tool to augment, rather than simply replace, human capabilities, forming new researcher–AI collaborations. Researchers are encouraged to embrace the additional layers of complexity introduced with AI as insightful opportunities for deeper understanding of the studied phenomenon.},
	language = {de},
	urldate = {2025-02-13},
	booktitle = {Handbuch {Soziale} {Praktiken} und {Digitale} {Alltagswelten}},
	publisher = {Springer Fachmedien},
	author = {Franken, Lina and Vepřek, Libuše Hannah},
	editor = {Friese, Heidrun and Nolden, Marcus and Schreiter, Miriam},
	year = {2025},
	doi = {10.1007/978-3-658-08460-8_98-1},
	pages = {1--9},
}

@article{christou_use_2023-1,
	title = {The use of artificial intelligence ({AI}) in qualitative research for theory development},
	url = {https://philpapers.org/rec/CHRTUO-3},
	urldate = {2025-02-13},
	author = {Christou, Prokopis A.},
	year = {2023},
}

@article{morgan_exploring_2023,
	title = {Exploring the {Use} of {Artificial} {Intelligence} for {Qualitative} {Data} {Analysis}: {The} {Case} of {ChatGPT}},
	volume = {22},
	issn = {1609-4069},
	shorttitle = {Exploring the {Use} of {Artificial} {Intelligence} for {Qualitative} {Data} {Analysis}},
	url = {https://doi.org/10.1177/16094069231211248},
	doi = {10.1177/16094069231211248},
	abstract = {The potential use of artificial intelligence programs such as a ChatGPT to analyze qualitative data raises any number of questions, most notably whether it is possible to produce similar results without the demanding process of manual coding. In addition, there are questions about both the simplicity of using ChatGPT for qualitative data analysis and the potential time savings that it might provide This article addresses these questions by using ChatGPT to reinvestigate two qualitative datasets that were previously analyzed by more traditional methods. In particular, it examines the extent to which the responses from ChatGPT can recreate the themes that were originally chosen to summarize the two previous analyses. The results show that ChatGPT performed reasonably well, but in both cases it was less successful at locating subtle, interpretive themes, and more successful at reproducing concrete, descriptive themes. In doing so, the program was quite easy to use and required very little effort in comparison to approaches that rely on manual coding. It is important to recognize, however, that both coding and approaches based on artificial intelligence are simply tools that must be applied within a larger analytic process. Overall, this exploration suggests that artificial intelligence may well have the power to disrupt the coding of data segments as a dominant paradigm for qualitative data analysis.},
	language = {en},
	urldate = {2025-02-13},
	journal = {International Journal of Qualitative Methods},
	author = {Morgan, David L.},
	month = oct,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	pages = {16094069231211248},
}

@inproceedings{petersen-frey_qualitative_2023,
	address = {Ingolstadt, Germany},
	title = {From {Qualitative} to {Quantitative} {Research}: {Semi}-{Automatic} {Annotation} {Scaling} in the {Digital} {Humanities}},
	shorttitle = {From {Qualitative} to {Quantitative} {Research}},
	url = {https://aclanthology.org/2023.konvens-main.5/},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 19th {Conference} on {Natural} {Language} {Processing} ({KONVENS} 2023)},
	publisher = {Association for Computational Lingustics},
	author = {Petersen-Frey, Fynn and Fischer, Tim and Schneider, Florian and Eiser, Isabel and Koch, Gertraud and Biemann, Chris},
	editor = {Georges, Munir and Herygers, Aaricia and Friedrich, Annemarie and Roth, Benjamin},
	month = sep,
	year = {2023},
	pages = {52--62},
}

@inproceedings{fischer_exploring_2024,
	address = {Miami, USA},
	title = {Exploring {Large} {Language} {Models} for {Qualitative} {Data} {Analysis}},
	url = {https://aclanthology.org/2024.nlp4dh-1.41/},
	doi = {10.18653/v1/2024.nlp4dh-1.41},
	abstract = {This paper explores the potential of Large Language Models (LLMs) to enhance qualitative data analysis (QDA) workflows within the open-source QDA platform developed at our university. We identify several opportunities within a typical QDA workflow where AI assistance can boost researcher productivity and translate these opportunities into corresponding NLP tasks: document classification, information extraction, span classification, and text generation. A benchmark tailored to these QDA activities is constructed, utilizing English and German datasets that align with relevant use cases. Focusing on efficiency and accessibility, we evaluate the performance of three prominent open-source LLMs - Llama 3.1, Gemma 2, and Mistral NeMo - on this benchmark. Our findings reveal the promise of LLM integration for streamlining QDA workflows, particularly for English-language projects. Consequently, we have implemented the LLM Assistant as an opt-in feature within our platform and report the implementation details. With this, we hope to further democratize access to AI capabilities for qualitative data analysis.},
	urldate = {2025-02-12},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Natural} {Language} {Processing} for {Digital} {Humanities}},
	publisher = {Association for Computational Linguistics},
	author = {Fischer, Tim and Biemann, Chris},
	editor = {Hämäläinen, Mika and Öhman, Emily and Miyagawa, So and Alnajjar, Khalid and Bizzoni, Yuri},
	month = nov,
	year = {2024},
	pages = {423--437},
}

@article{elliott_thinking_2018,
	title = {Thinking about the coding process in qualitative data analysis},
	volume = {23},
	issn = {1052-0147},
	url = {https://ora.ox.ac.uk/objects/uuid:5304bf7f-6214-4939-9f1b-b64415d4fac1},
	abstract = {Coding is a ubiquitous part of the qualitative research process, but it is often under-considered in research methods training and literature. This article explores a number of questions about the coding process which are often raised by beginning researchers, in the light of the recommendations of methods textbooks and the factors which contribute to an answer to these questions. I argue for a conceptualisation of coding as a decision-making process, in which decisions about aspects of coding such as density, frequency, size of data pieces to be coded, are all made by individual researchers in line with their methodological background, their research design and research questions, and the practicalities of their study. This has implications for the way that coding is carried out by researchers at all stages of their careers, as it requires that coding decisions should be made in the context of an individual study, not once and for all.},
	language = {en},
	number = {11},
	urldate = {2025-02-11},
	journal = {Qualitative Report},
	author = {Elliott, V.},
	year = {2018},
	note = {Publisher: Nova Southeastern University},
}

@article{elliott_thinking_2018-1,
	title = {Thinking about the coding process in qualitative data analysis},
	volume = {23},
	url = {https://ora.ox.ac.uk/objects/uuid:5304bf7f-6214-4939-9f1b-b64415d4fac1},
	number = {11},
	urldate = {2025-02-11},
	journal = {Qualitative report},
	author = {Elliott, Victoria},
	year = {2018},
	note = {Publisher: Nova Southeastern University},
}

@article{elliott_thinking_2018-2,
	title = {Thinking about the {Coding} {Process} in {Qualitative} {Data} {Analysis}},
	issn = {2160-3715, 1052-0147},
	url = {https://nsuworks.nova.edu/tqr/vol23/iss11/14/},
	doi = {10.46743/2160-3715/2018.3560},
	abstract = {Coding is a ubiquitous part of the qualitative research process, but it is often under-considered in research methods training and literature. This article explores a number of questions about the coding process which are often raised by beginning researchers, in the light of the recommendations of methods textbooks and the factors which contribute to an answer to these questions. I argue for a conceptualisation of coding as a decision-making process, in which decisions about aspects of coding such as density, frequency, size of data pieces to be coded, are all made by individual researchers in line with their methodological background, their research design and research questions, and the practicalities of their study. This has implications for the way that coding is carried out by researchers at all stages of their careers, as it requires that coding decisions should be made in the context of an individual study, not once and for all.},
	language = {en},
	urldate = {2025-02-11},
	journal = {The Qualitative Report},
	author = {Elliott, Victoria},
	month = nov,
	year = {2018},
}

@misc{noauthor_harnessing_nodate,
	title = {Harnessing {Large} {Language} {Models} ({LLMs}) for {Metadata} {Annotation} to {Accelerate} {Biotech} and {Pharma} {Research}},
	url = {https://ardigen.com/harnessing-large-language-models-llms-for-metadata-annotation-to-accelerate-biotech-and-pharma-research/},
	language = {en},
	urldate = {2025-02-11},
}

@book{ritchie_qualitative_2014-1,
	address = {Los Angeles},
	edition = {Second edition},
	title = {Qualitative research practice: a guide for social science students and researchers},
	isbn = {978-1-4462-0911-0},
	shorttitle = {Qualitative research practice},
	abstract = {Providing a clear and accessible account of the qualitative research process, this book discusses the different forms and uses of qualitative research, the design, data collection, analysis and reporting},
	language = {eng},
	publisher = {SAGE},
	author = {Lewis, Jane and McNaughton Nicholls, Carol and Ormston, Rachel},
	editor = {Ritchie, Jane},
	year = {2014},
	note = {OCLC: 858825421},
	keywords = {Forschungsmethode, Forschungsplanung, Kvalitativ metod, Methodologie, Qualitative Methode, Qualitative Research, Qualitative Sozialforschung, Qualitative research, Recherche qualitative, Research Design, Samhällsvetenskaplig forskning, Sciences sociales Recherche, Social Sciences methods, Social sciences Methodology, Social sciences Research, Sozialwissenschaften},
}

@book{ritchie_qualitative_2003,
	title = {Qualitative research practice},
	volume = {757},
	url = {https://www.torrossa.com/gs/resourceProxy?an=5017584&publisher=FZ7200},
	urldate = {2025-02-11},
	publisher = {sage London},
	author = {Ritchie, Jane and Lewis, Jane and Nicholls, Carol McNaughton and Ormston, Rachel},
	year = {2003},
}

@book{ritchie_qualitative_2003-1,
	title = {Qualitative research practice},
	volume = {757},
	url = {https://www.torrossa.com/gs/resourceProxy?an=5017584&publisher=FZ7200},
	urldate = {2025-02-11},
	publisher = {sage London},
	author = {Ritchie, Jane and Lewis, Jane and Nicholls, Carol McNaughton and Ormston, Rachel},
	year = {2003},
}

@book{creswell_30_2015,
	title = {30 {Essential} {Skills} for the {Qualitative} {Researcher}},
	isbn = {978-1-4833-2148-6},
	abstract = {30 Essential Skills for the Qualitative Researcher fills a gap in introductory literature on qualitative inquiry by providing practical "how-to" information for beginning researchers in the social, behavioral, and health sciences. Author John W. Creswell draws on years of teaching, writing, and conducting his own projects to offer effective techniques and procedures with many applied examples from research design, qualitative inquiry, and mixed methods. Creswell defines what a skill is, and acknowledges that while there may be more than 30 that an individual will use and perfect, the skills presented in this book are crucial for a new qualitative researcher starting a qualitative project.},
	language = {en},
	publisher = {SAGE Publications},
	author = {Creswell, John W.},
	month = jul,
	year = {2015},
	note = {Google-Books-ID: fkJsCgAAQBAJ},
	keywords = {Reference / Research, Social Science / Methodology},
}

@book{saldana_coding_2015,
	title = {The {Coding} {Manual} for {Qualitative} {Researchers}},
	isbn = {978-1-4739-4359-9},
	abstract = {Johnny Saldaña’s unique and invaluable manual demystifies the qualitative coding process with a comprehensive assessment of different coding types, examples and exercises. The ideal reference for students, teachers, and practitioners of qualitative inquiry, it is essential reading across the social sciences and neatly guides you through the multiple approaches available for coding qualitative data. Its wide array of strategies, from the more straightforward to the more complex, is skillfully explained and carefully exemplified providing a complete toolkit of codes and skills that can be applied to any research project. For each code Saldaña provides information about the method′s origin, gives a detailed description of the method, demonstrates its practical applications, and sets out a clearly illustrated example with analytic follow-up.   Now with a companion website, the book is supported by:   SAGE journal articles showing coding being applied to real research  Sample transcripts highlighting coding techniques  Links to CAQDAS sites to introduce relevant software  Practical student exercises Links to video and digital content   This international bestseller is an extremely usable, robust manual and is a must-have resource for qualitative researchers at all levels.  Click here for a listing of Johnny Saldaña′s upcoming workshops.},
	language = {en},
	publisher = {SAGE},
	author = {Saldana, Johnny},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: ZhxiCgAAQBAJ},
	keywords = {Reference / Research, Social Science / Research},
}

@article{holton_coding_2007,
	title = {The coding process and its challenges},
	volume = {3},
	url = {https://www.torrossa.com/gs/resourceProxy?an=5019441&publisher=FZ7200#page=298},
	urldate = {2025-02-11},
	journal = {The Sage handbook of grounded theory},
	author = {Holton, Judith A.},
	year = {2007},
	pages = {265--289},
}

@book{harding_qualitative_2013,
	title = {Qualitative {Data} {Analysis} from {Start} to {Finish}},
	isbn = {978-1-4462-9017-0},
	abstract = {Are you new to qualitative research? Are you planning to do interviews or focus groups and wondering what on earth you′ll do with the data once it′s collected? Do you have a pile of transcripts staring at you right now and are you lost as to how to identify themes, code your data and work out what it all means?   Fear not, help is here! In this brilliant new book, Jamie Harding breaks down the process of analysing qualitative data into simple, retraceable steps. After providing some top tips for designing your research and collecting your data, he takes you through the different stages of analysis, from the first reading of your transcripts, to presenting your findings in a report or dissertation. For each stage of the process there are demonstrations using real data and exercises for you to perform yourself. He unpicks what happens behind the scenes in qualitative data analysis - the bit that′s hard to learn without seeing it happen and trying it for yourself. While acknowledging that there are many different forms that qualitative data analysis can take, the book provides a series of ideas and examples that you will find invaluable when analysing your own data.    This book is perfect for all social science students who are struggling with data analysis and are looking for someone to guide the way.},
	language = {en},
	publisher = {SAGE},
	author = {Harding, Jamie},
	month = feb,
	year = {2013},
	note = {Google-Books-ID: 9YUQAgAAQBAJ},
	keywords = {Reference / Research, Social Science / Research},
}

@book{bernard_analyzing_2016,
	title = {Analyzing {Qualitative} {Data}: {Systematic} {Approaches}},
	isbn = {978-1-4833-4711-0},
	shorttitle = {Analyzing {Qualitative} {Data}},
	abstract = {The fully updated Second Edition of Analyzing Qualitative Data: Systematic Approaches by H. Russell Bernard, Amber Wutich, and Gery W. Ryan presents systematic methods for analyzing qualitative data with clear and easy-to-understand steps. The first half is an overview of the basics, from choosing a topic to collecting data, and coding to finding themes, while the second half covers different methods of analysis, including grounded theory, content analysis, analytic induction, semantic network analysis, ethnographic decision modeling, and more. Real examples drawn from social science and health literature along with carefully crafted, hands-on exercises at the end of each chapter allow readers to master key techniques and apply them to their own disciplines.},
	language = {en},
	publisher = {SAGE Publications},
	author = {Bernard, H. Russell and Wutich, Amber and Ryan, Gery W.},
	month = jun,
	year = {2016},
	note = {Google-Books-ID: yAi1DAAAQBAJ},
	keywords = {Reference / Research, Social Science / Research},
}

@book{bernard_research_2011,
	title = {Research {Methods} in {Anthropology}: {Qualitative} and {Quantitative} {Approaches}},
	isbn = {978-0-7591-1241-4},
	shorttitle = {Research {Methods} in {Anthropology}},
	abstract = {Research Methods in Anthropology is the standard textbook for methods classes in anthropology. Written in Russ Bernard's unmistakable conversational style, his guide has launched tens of thousands of students into the fieldwork enterprise with a combination of rigorous methodology, wry humor, and commonsense advice. Whether you are coming from a scientific, interpretive, or applied anthropological tradition, you will learn field methods from the best guide in both qualitative and quantitative methods.},
	language = {en},
	publisher = {Rowman Altamira},
	author = {Bernard, Harvey Russell},
	year = {2011},
	keywords = {Social Science / Anthropology / Cultural \& Social, Social Science / Anthropology / General, Social Science / Research, Social Science / Sociology / General},
}

@inproceedings{morton_wordfreak_2003,
	title = {{WordFreak}: {An} {Open} {Tool} for {Linguistic} {Annotation}},
	shorttitle = {{WordFreak}},
	url = {https://aclanthology.org/N03-4009/},
	urldate = {2025-02-11},
	booktitle = {Companion {Volume} of the {Proceedings} of {HLT}-{NAACL} 2003 - {Demonstrations}},
	author = {Morton, Thomas and LaCivita, Jeremy},
	year = {2003},
	pages = {17--18},
}

@inproceedings{morton_wordfreak_2003,
	address = {USA},
	series = {{NAACL}-{Demonstrations} '03},
	title = {{WordFreak}: an open tool for linguistic annotation},
	shorttitle = {{WordFreak}},
	url = {https://dl.acm.org/doi/10.3115/1073427.1073436},
	doi = {10.3115/1073427.1073436},
	abstract = {WordFreak is a natural language annotation tool that has been designed to be easy to extend to new domains and tasks. Specifically, a plug-in architecture has been developed which allows components to be added to WordFreak for customized visualization, annotation specification, and automatic annotation, without re-compilation. The APIs for these plug-ins provide mechanisms to allow automatic annotators or taggers to guide future annotation to supports active learning. At present WordFreak can be used to annotate a number of different types of annotation in English, Chinese, and Arabic including: constituent parse structure and dependent annotations, and ACE named-entity and coreference annotation. The Java source code for WordFreak is distributed under the Mozilla Public License 1.1 via SourceForge at: http://wordfreak.sourceforge.net. This site also provides screenshots, and a web deployable version of WordFreak.},
	urldate = {2025-02-11},
	booktitle = {Proceedings of the 2003 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics} on {Human} {Language} {Technology}: {Demonstrations} - {Volume} 4},
	publisher = {Association for Computational Linguistics},
	author = {Morton, Thomas and LaCivita, Jeremy},
	year = {2003},
	pages = {17--18},
}

@inproceedings{yimam_automatic_2014,
	title = {Automatic annotation suggestions and custom annotation layers in {WebAnno}},
	url = {https://aclanthology.org/P14-5016.pdf},
	urldate = {2025-02-11},
	booktitle = {Proceedings of 52nd annual meeting of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	author = {Yimam, Seid Muhie and Biemann, Chris and De Castilho, Richard Eckart and Gurevych, Iryna},
	year = {2014},
	pages = {91--96},
}

@inproceedings{yimam_automatic_2014-1,
	title = {Automatic annotation suggestions and custom annotation layers in {WebAnno}},
	url = {https://aclanthology.org/P14-5016.pdf},
	urldate = {2025-02-11},
	booktitle = {Proceedings of 52nd annual meeting of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	author = {Yimam, Seid Muhie and Biemann, Chris and De Castilho, Richard Eckart and Gurevych, Iryna},
	year = {2014},
	pages = {91--96},
}

@inproceedings{stone_computer_1963,
	address = {Detroit, Michigan},
	title = {A computer approach to content analysis: studies using the {General} {Inquirer} system},
	shorttitle = {A computer approach to content analysis},
	url = {http://portal.acm.org/citation.cfm?doid=1461551.1461583},
	doi = {10.1145/1461551.1461583},
	language = {en},
	urldate = {2025-02-10},
	booktitle = {Proceedings of the {May} 21-23, 1963, spring joint computer conference on - {AFIPS} '63 ({Spring})},
	publisher = {ACM Press},
	author = {Stone, Philip J. and Hunt, Earl B.},
	year = {1963},
	pages = {241},
}

@book{miles_qualitative_2019,
	title = {Qualitative {Data} {Analysis}: {A} {Methods} {Sourcebook}},
	isbn = {978-1-5063-5307-4},
	shorttitle = {Qualitative {Data} {Analysis}},
	abstract = {Miles, Huberman, and Saldaña’s Qualitative Data Analysis: A Methods Sourcebook is the authoritative text for analyzing and displaying qualitative research data. The Fourth Edition maintains the analytic rigor of previous editions while showcasing a variety of new visual display models for qualitative inquiry. Graphics are added to the now-classic matrix and network illustrations of the original co-authors. Five chapters have been substantially revised, and the appendix’s annotated bibliography includes new titles in research methods. Graduate students and established scholars from all disciplines will find this resource an innovative compendium of ideas for the representation and presentation of qualitative data. As the authors demonstrate, when researchers “think display,” their analyses of social life capture the complex and vivid processes of the people and institutions studied.},
	language = {en},
	publisher = {SAGE Publications},
	author = {Miles, Matthew B. and Huberman, A. Michael and Saldana, Johnny},
	month = jan,
	year = {2019},
	note = {Google-Books-ID: Bt0uuQEACAAJ},
	keywords = {Social Science / Research},
}

@article{rezazadegan_automatic_nodate,
	title = {Automatic {Speech} {Summarisation}: {A} {Scoping} {Review}},
	abstract = {Speech summarisation techniques take human speech as input and then output an abridged version as text or speech. Speech summarisation has applications in many domains from information technology to health care, for example improving speech archives or reducing clinical documentation burden. This scoping review maps the speech summarisation literature, with no restrictions on time frame, language summarised, research method, or paper type. We reviewed a total of 110 papers out of a set of 153 found through a literature search and extracted speech features used, methods, scope, and training corpora. Most studies employ one of four speech summarisation architectures: (1) Sentence extraction and compaction; (2) Feature extraction and classification or rank-based sentence selection; (3) Sentence compression and compression summarisation; and (4) Language modelling. We also discuss the strengths and weaknesses of these different methods and speech features. Overall, supervised methods (e.g. Hidden Markov support vector machines, Ranking support vector machines, Conditional random fields) performed better than unsupervised methods. As supervised methods require manually annotated training data which can be costly, there was more interest in unsupervised methods. Recent research into unsupervised methods focusses on extending language modelling, for example by combining Uni-gram modelling with deep neural networks.},
	language = {en},
	author = {Rezazadegan, Dana and Berkovsky, Shlomo and Quiroz, Juan C and Kocaballi, A Baki and Laranjo, Liliana and Coiera, Enrico},
}

@article{rennard_abstractive_2023,
	title = {Abstractive {Meeting} {Summarization}: {A} {Survey}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {Abstractive {Meeting} {Summarization}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00578/116993/Abstractive-Meeting-Summarization-A-Survey},
	doi = {10.1162/tacl_a_00578},
	abstract = {A system that could reliably identify and sum up the most important points of a conversation would be valuable in a wide variety of real-world contexts, from business meetings to medical consultations to customer service calls. Recent advances in deep learning, and especially the invention of encoder-decoder architectures, has significantly improved language generation systems, opening the door to improved forms of abstractive summarization—a form of summarization particularly well-suited for multi-party conversation. In this paper, we provide an overview of the challenges raised by the task of abstractive meeting summarization and of the data sets, models, and evaluation metrics that have been used to tackle the problems.},
	language = {en},
	urldate = {2025-01-17},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Rennard, Virgile and Shang, Guokan and Hunter, Julie and Vazirgiannis, Michalis},
	month = jul,
	year = {2023},
	pages = {861--884},
}

@article{papi_direct_2023,
	title = {Direct {Speech} {Translation} for {Automatic} {Subtitling}},
	volume = {11},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00607},
	doi = {10.1162/tacl_a_00607},
	abstract = {Automatic subtitling is the task of automatically translating the speech of
audiovisual content into short pieces of timed text, i.e., subtitles and their
corresponding timestamps. The generated subtitles need to conform to space and
time requirements, while being synchronized with the speech and segmented in a
way that facilitates comprehension. Given its considerable complexity, the task
has so far been addressed through a pipeline of components that separately deal
with transcribing, translating, and segmenting text into subtitles, as well as
predicting timestamps. In this paper, we propose the first direct speech
translation model for automatic subtitling that generates subtitles in the
target language along with their timestamps with a single model. Our experiments
on 7 language pairs show that our approach outperforms a cascade system in the
same data condition, also being competitive with production tools on both
in-domain and newly released out-domain benchmarks covering new scenarios.},
	urldate = {2025-01-17},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Papi, Sara and Gaido, Marco and Karakanta, Alina and Cettolo, Mauro and Negri, Matteo and Turchi, Marco},
	month = nov,
	year = {2023},
	pages = {1355--1376},
}

@misc{fernandes_surveysum_2024,
	title = {{SurveySum}: {A} {Dataset} for {Summarizing} {Multiple} {Scientific} {Articles} into a {Survey} {Section}},
	shorttitle = {{SurveySum}},
	url = {http://arxiv.org/abs/2408.16444},
	doi = {10.48550/arXiv.2408.16444},
	abstract = {Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. Our contributions are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. Our results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.},
	language = {en},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Fernandes, Leandro Carísio and Guedes, Gustavo Bartz and Laitz, Thiago Soares and Almeida, Thales Sales and Nogueira, Rodrigo and Lotufo, Roberto and Pereira, Jayr},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16444 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{takeshita_aclsum_2024,
	address = {Mexico City, Mexico},
	title = {{ACLSum}: {A} {New} {Dataset} for {Aspect}-based {Summarization} of {Scientific} {Publications}},
	shorttitle = {{ACLSum}},
	url = {https://aclanthology.org/2024.naacl-long.371},
	doi = {10.18653/v1/2024.naacl-long.371},
	language = {en},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Takeshita, Sotaro and Green, Tommaso and Reinig, Ines and Eckert, Kai and Ponzetto, Simone},
	year = {2024},
	pages = {6660--6675},
}

@inproceedings{gupta_sumpubmed_2021,
	address = {Online},
	title = {{SumPubMed}: {Summarization} {Dataset} of {PubMed} {Scientific} {Articles}},
	shorttitle = {{SumPubMed}},
	url = {https://aclanthology.org/2021.acl-srw.30},
	doi = {10.18653/v1/2021.acl-srw.30},
	abstract = {Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SUMPUBMED, using scientiﬁc articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SUMPUBMED. SUMPUBMED is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-speciﬁc scientiﬁc terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SUMPUBMED. Thus, SUMPUBMED opens new avenues for the future improvement of models as well as the development of new evaluation metrics.},
	language = {en},
	urldate = {2025-01-09},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Vivek and Bharti, Prerna and Nokhiz, Pegah and Karnick, Harish},
	year = {2021},
	pages = {292--303},
}

@inproceedings{sharma_espnet-summ_2023,
	title = {Espnet-{Summ}: {Introducing} a {Novel} {Large} {Dataset}, {Toolkit}, and a {Cross}-{Corpora} {Evaluation} of {Speech} {Summarization} {Systems}},
	shorttitle = {Espnet-{Summ}},
	url = {https://ieeexplore.ieee.org/document/10389641/?arnumber=10389641},
	doi = {10.1109/ASRU57964.2023.10389641},
	abstract = {Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus with 4400 {\textbackslash}mathrm h of conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and experiments demonstrate the benefit of multi-style training over fine-tuning. 1},
	urldate = {2025-01-10},
	booktitle = {2023 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Sharma, Roshan and Chen, William and Kano, Takatomo and Sharma, Ruchira and Arora, Siddhant and Watanabe, Shinji and Ogawa, Atsunori and Delcroix, Marc and Singh, Rita and Raj, Bhiksha},
	month = dec,
	year = {2023},
	keywords = {Automatic speech recognition, Conferences, Interviews, Oral communication, Task analysis, Training, cascade, end-to-end, long term context, speech summarization},
	pages = {1--8},
}

@article{lin_rouge_nodate,
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.},
	language = {en},
	author = {Lin, Chin-Yew},
}

@inproceedings{shen_large_2023,
	address = {Singapore},
	title = {Large {Language} {Models} are {Not} {Yet} {Human}-{Level} {Evaluators} for {Abstractive} {Summarization}},
	url = {https://aclanthology.org/2023.findings-emnlp.278},
	doi = {10.18653/v1/2023.findings-emnlp.278},
	language = {en},
	urldate = {2025-01-17},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Chenhui and Cheng, Liying and Nguyen, Xuan-Phi and You, Yang and Bing, Lidong},
	year = {2023},
	pages = {4215--4233},
}

@inproceedings{ghazimatin_podtile_2024,
	title = {{PODTILE}: {Facilitating} {Podcast} {Episode} {Browsing} with {Auto}-generated {Chapters}},
	url = {https://doi.org/10.1145/3627673.3680081},
	doi = {10.1145/3627673.3680081},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}, {CIKM} 2024, {Boise}, {ID}, {USA}, {October} 21-25, 2024},
	publisher = {ACM},
	author = {Ghazimatin, Azin and Garmash, Ekaterina and Penha, Gustavo and Sheets, Kristen and Achenbach, Martin and Semerci, Oguz and Galvez, Remi and Tannenberg, Marcus and Mantravadi, Sahitya and Narayanan, Divya and Kalaydzhyan, Ofeliya and Cole, Douglas and Carterette, Ben and Clifton, Ann and Bennett, Paul N. and Hauff, Claudia and Lalmas, Mounia},
	editor = {Serra, Edoardo and Spezzano, Francesca},
	year = {2024},
	pages = {4487--4495},
}

@inproceedings{ghazimatin_podtile_2024-1,
	title = {{PODTILE}: {Facilitating} {Podcast} {Episode} {Browsing} with {Auto}-generated {Chapters}},
	url = {https://doi.org/10.1145/3627673.3680081},
	doi = {10.1145/3627673.3680081},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}, {CIKM} 2024, {Boise}, {ID}, {USA}, {October} 21-25, 2024},
	publisher = {ACM},
	author = {Ghazimatin, Azin and Garmash, Ekaterina and Penha, Gustavo and Sheets, Kristen and Achenbach, Martin and Semerci, Oguz and Galvez, Remi and Tannenberg, Marcus and Mantravadi, Sahitya and Narayanan, Divya and Kalaydzhyan, Ofeliya and Cole, Douglas and Carterette, Ben and Clifton, Ann and Bennett, Paul N. and Hauff, Claudia and Lalmas, Mounia},
	editor = {Serra, Edoardo and Spezzano, Francesca},
	year = {2024},
	pages = {4487--4495},
}

@inproceedings{ghazimatin_podtile_2024-2,
	title = {{PODTILE}: {Facilitating} {Podcast} {Episode} {Browsing} with {Auto}-generated {Chapters}},
	url = {https://doi.org/10.1145/3627673.3680081},
	doi = {10.1145/3627673.3680081},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}, {CIKM} 2024, {Boise}, {ID}, {USA}, {October} 21-25, 2024},
	publisher = {ACM},
	author = {Ghazimatin, Azin and Garmash, Ekaterina and Penha, Gustavo and Sheets, Kristen and Achenbach, Martin and Semerci, Oguz and Galvez, Remi and Tannenberg, Marcus and Mantravadi, Sahitya and Narayanan, Divya and Kalaydzhyan, Ofeliya and Cole, Douglas and Carterette, Ben and Clifton, Ann and Bennett, Paul N. and Hauff, Claudia and Lalmas, Mounia},
	editor = {Serra, Edoardo and Spezzano, Francesca},
	year = {2024},
	pages = {4487--4495},
}

@inproceedings{nedoluzhko_towards_2019,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Towards {Automatic} {Minuting} of the {Meetings}},
	volume = {2473},
	url = {https://ceur-ws.org/Vol-2473/paper3.pdf},
	booktitle = {Proceedings of the 19th {Conference} {Information} {Technologies} - {Applications} and {Theory} ({ITAT} 2019), {Hotel} {Zornička}, {Donovaly}, {Slovakia}, {September} 20-24, 2019},
	publisher = {CEUR-WS.org},
	author = {Nedoluzhko, Anna and Bojar, Ondrej},
	editor = {Barancíková, Petra and Holena, Martin and Horváth, Tomás and Pleva, Matús and Rosa, Rudolf},
	year = {2019},
	pages = {112--119},
}

@inproceedings{zhong_qmsum_2021,
	address = {Online},
	title = {{QMSum}: {A} {New} {Benchmark} for {Query}-based {Multi}-domain {Meeting} {Summarization}},
	shorttitle = {{QMSum}},
	url = {https://aclanthology.org/2021.naacl-main.472/},
	doi = {10.18653/v1/2021.naacl-main.472},
	abstract = {Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/QMSum.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and Radev, Dragomir},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {5905--5921},
}

@misc{noauthor_podtile_nodate,
	title = {{PODTILE}: {Facilitating} {Podcast} {Episode} {Browsing} with {Auto}-generated {Chapters} {\textbar} {Proceedings} of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	url = {https://dl.acm.org/doi/10.1145/3627673.3680081},
	urldate = {2025-01-17},
}

@inproceedings{sharma_speech_2024,
	address = {Bangkok, Thailand},
	title = {Speech vs. {Transcript}: {Does} {It} {Matter} for {Human} {Annotators} in {Speech} {Summarization}?},
	shorttitle = {Speech vs. {Transcript}},
	url = {https://aclanthology.org/2024.acl-long.790/},
	doi = {10.18653/v1/2024.acl-long.790},
	abstract = {Reference summaries for abstractive speech summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method, we find that summaries are indeed different based on the source modality, and that speech-based summaries are more factually consistent and information-selective than transcript-based summaries. Transcript-based summaries are impacted by recognition errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public to facilitate the reproduction of our work and advance research in this area.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Roshan and Shon, Suwon and Lindsey, Mark and Dhamyal, Hira and Raj, Bhiksha},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {14779--14797},
}

@misc{enriquez_delivery_2021,
	title = {Delivery {Gig} {Worker} {Interviews} on {Automation} at {Work}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://datacommons.princeton.edu/discovery/doi/10.34770/4324-yn77},
	doi = {10.34770/4324-YN77},
	language = {en},
	urldate = {2025-01-14},
	publisher = {Princeton University},
	author = {Enriquez, Diana},
	year = {2021},
}

@inproceedings{lee_use_2019,
	address = {Cham},
	title = {Use of {Training}, {Validation}, and {Test} {Sets} for {Developing} {Automated} {Classifiers} in {Quantitative} {Ethnography}},
	isbn = {978-3-030-33232-7},
	doi = {10.1007/978-3-030-33232-7_10},
	abstract = {Using automated classifiers to code discourse data enables researchers to carry out analyses on large datasets. This paper presents a detailed example of applying training, validation and test sets frequently utilized in machine learning to develop automated classifiers for use in quantitative ethnography research. The method was applied to two dispositional constructs. Within one cycle of the process, reliable and valid automated classifiers were developed for Social Disposition. However, the automated coding scheme for Inclusive Disposition was rejected during the validation stage due to issues of overfitting. Nonetheless, the results demonstrate the beneficial potential of using preclassified datasets in enhancing the efficiency and effectiveness of the automation process.},
	language = {en},
	booktitle = {Advances in {Quantitative} {Ethnography}},
	publisher = {Springer International Publishing},
	author = {Lee, Seung B. and Gui, Xiaofan and Manquen, Megan and Hamilton, Eric R.},
	editor = {Eagan, Brendan and Misfeldt, Morten and Siebert-Evenstone, Amanda},
	year = {2019},
	keywords = {Automated classifiers, Qualitative coding, Quantitative ethnography},
	pages = {117--127},
}

@misc{lu_ai_2024,
	title = {The {AI} {Scientist}: {Towards} {Fully} {Automated} {Open}-{Ended} {Scientific} {Discovery}},
	shorttitle = {The {AI} {Scientist}},
	url = {http://arxiv.org/abs/2408.06292},
	doi = {10.48550/arXiv.2408.06292},
	abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
	month = sep,
	year = {2024},
	note = {arXiv:2408.06292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{parfenova_automating_2024,
	address = {Bangkok, Thailand},
	title = {Automating {Qualitative} {Data} {Analysis} with {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.acl-srw.17/},
	doi = {10.18653/v1/2024.acl-srw.17},
	abstract = {This PhD proposal aims to investigate ways of automating qualitative data analysis, specifically the thematic coding of texts. Despite existing methods vastly covered in literature, they mainly use Topic Modeling and other quantitative approaches which are far from resembling a human`s analysis outcome. This proposal examines the limitations of current research in the field. It proposes a novel methodology based on Large Language Models to tackle automated coding and make it as close as possible to the results of human researchers. This paper covers studies already done in this field and their limitations, existing software, the problem of duplicating the researcher bias, and the proposed methodology.},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 4: {Student} {Research} {Workshop})},
	publisher = {Association for Computational Linguistics},
	author = {Parfenova, Angelina and Denzler, Alexander and Pfeffer, Jörgen},
	editor = {Fu, Xiyan and Fleisig, Eve},
	month = aug,
	year = {2024},
	pages = {83--91},
}

@inproceedings{parfenova_automating_2024-1,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Automating the {Information} {Extraction} from {Semi}-{Structured} {Interview} {Transcripts}},
	isbn = {9798400701726},
	url = {https://dl.acm.org/doi/10.1145/3589335.3651230},
	doi = {10.1145/3589335.3651230},
	abstract = {This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.},
	urldate = {2025-01-14},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Parfenova, Angelina},
	year = {2024},
	pages = {983--986},
}

@inproceedings{spinoso-di_piano_qualitative_2023,
	address = {Singapore},
	title = {Qualitative {Code} {Suggestion}: {A} {Human}-{Centric} {Approach} to {Qualitative} {Coding}},
	shorttitle = {Qualitative {Code} {Suggestion}},
	url = {https://aclanthology.org/2023.findings-emnlp.993/},
	doi = {10.18653/v1/2023.findings-emnlp.993},
	abstract = {Qualitative coding is a content analysis method in which researchers read through a text corpus and assign descriptive labels or qualitative codes to passages. It is an arduous and manual process which human-computer interaction (HCI) studies have shown could greatly benefit from NLP techniques to assist qualitative coders. Yet, previous attempts at leveraging language technologies have set up qualitative coding as a fully automatable classification problem. In this work, we take a more assistive approach by defining the task of qualitative code suggestion (QCS) in which a ranked list of previously assigned qualitative codes is suggested from an identified passage. In addition to being user-motivated, QCS integrates previously ignored properties of qualitative coding such as the sequence in which passages are annotated, the importance of rare codes and the differences in annotation styles between coders. We investigate the QCS task by releasing the first publicly available qualitative coding dataset, CVDQuoding, consisting of interviews conducted with women at risk of cardiovascular disease. In addition, we conduct a human evaluation which shows that our systems consistently make relevant code suggestions.},
	urldate = {2025-01-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Spinoso-Di Piano, Cesare and Rahimi, Samira and Cheung, Jackie},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {14887--14909},
}

@incollection{lindgren_ai_2023,
	title = {{AI} ethnography},
	isbn = {978-1-80392-856-2 978-1-80392-855-5},
	url = {https://www.elgaronline.com/view/book/9781803928562/book-part-9781803928562-83.xml},
	urldate = {2025-01-14},
	booktitle = {Handbook of {Critical} {Studies} of {Artificial} {Intelligence}},
	publisher = {Edward Elgar Publishing},
	author = {Dippel, Anne and Sudmann, Andreas},
	editor = {Lindgren, Simon},
	month = nov,
	year = {2023},
	doi = {10.4337/9781803928562.00083},
	pages = {826--844},
}
