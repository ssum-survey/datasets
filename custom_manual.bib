@string{icassp = "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)"}
@string{interspeech = "Interspeech"}

@article{zhang-etal-2023-macsum,
    title = "{MACS}um: Controllable Summarization with Mixed Attributes",
    author = "Zhang, Yusen  and
      Liu, Yang  and
      Yang, Ziyi  and
      Fang, Yuwei  and
      Chen, Yulong  and
      Radev, Dragomir  and
      Zhu, Chenguang  and
      Zeng, Michael  and
      Zhang, Rui",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.46/",
    doi = "10.1162/tacl_a_00575",
    pages = "787--803",
    abstract = "Controllable summarization allows users to generate customized summaries with specified attributes. However, due to the lack of designated annotations of controlled summaries, existing work has to craft pseudo datasets by adapting generic summarization benchmarks. Furthermore, most research focuses on controlling single attributes individually (e.g., a short summary or a highly abstractive summary) rather than controlling a mix of attributes together (e.g., a short and highly abstractive summary). In this paper, we propose MACSum, the first human-annotated summarization dataset for controlling mixed attributes. It contains source texts from two domains, news articles and dialogues, with human-annotated summaries controlled by five designed attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We propose two simple and effective parameter-efficient approaches for the new task of mixed controllable summarization based on hard prompt tuning and soft prefix tuning. Results and analysis demonstrate that hard prompt models yield the best performance on most metrics and human evaluations. However, mixed-attribute control is still challenging for summarization tasks. Our dataset and code are available at https://github.com/psunlpgroup/MACSum."
}

@misc{papi2025mcifmultimodalcrosslingualinstructionfollowing,
      title={MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks}, 
      author={Sara Papi and Maike Züfle and Marco Gaido and Beatrice Savoldi and Danni Liu and Ioannis Douros and Luisa Bentivogli and Jan Niehues},
      year={2025},
      eprint={2507.19634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.19634}, 
}

@article{murray2009decisionaudio,
author = {Murray, Gabriel and Kleinbauer, Thomas and Poller, Peter and Becker, Tilman and Renals, Steve and Kilgour, Jonathan},
title = {Extrinsic summarization evaluation: A decision audit task},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1550-4875},
url = {https://doi.org/10.1145/1596517.1596518},
doi = {10.1145/1596517.1596518},
abstract = {In this work we describe a large-scale extrinsic evaluation of automatic speech summarization technologies for meeting speech. The particular task is a decision audit, wherein a user must satisfy a complex information need, navigating several meetings in order to gain an understanding of how and why a given decision was made. We compare the usefulness of extractive and abstractive technologies in satisfying this information need, and assess the impact of automatic speech recognition (ASR) errors on user performance. We employ several evaluation methods for participant performance, including post-questionnaire data, human subjective and objective judgments, and a detailed analysis of participant browsing behavior. We find that while ASR errors affect user satisfaction on an information retrieval task, users can adapt their browsing behavior to complete the task satisfactorily. Results also indicate that users consider extractive summaries to be intuitive and useful tools for browsing multimodal meeting data. We discuss areas in which automatic summarization techniques can be improved in comparison with gold-standard meeting abstracts.},
journal = {ACM Trans. Speech Lang. Process.},
month = oct,
articleno = {2},
numpages = {29},
keywords = {Summarization, abstraction, browsing, evaluation, extraction, interfaces}
}

@misc{egressy2025setllmpermutationinvariantllm,
      title={Set-{LLM}: A Permutation-Invariant {LLM}}, 
      author={Beni Egressy and Jan Stühmer},
      year={2025},
      publisher = {arXiv},
      note = {arXiv:2505.15433 [cs.LG]},
      url={https://arxiv.org/abs/2505.15433}, 
}


@inproceedings{yang1998visual,
  title={Visual tracking for multimodal human computer interaction},
  author={Yang, Jie and Stiefelhagen, Rainer and Meier, Uwe and Waibel, Alex},
  booktitle={Proceedings of the SIGCHI conference on Human factors in computing systems},
  pages={140--147},
  year={1998},
  url={https://dl.acm.org/doi/10.1145/274644.274666},
}

@misc{SeaLLMs-Audio,
    author = {Chaoqun Liu and Mahani Aljunied and Guizhen Chen and Hou Pong Chan and Weiwen Xu and Yu Rong and Wenxuan Zhang},
    title = {SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia},
    year = {2025},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/DAMO-NLP-SG/SeaLLMs-Audio}},
}


@INPROCEEDINGS{liu_hierarhical_2018,
  author={Liu, Tzu-En and Liu, Shih-Hung and Chen, Berlin},
  booktitle=icassp # ", 2019", 
  title={A Hierarchical Neural Summarization Framework for Spoken Documents}, 
  year={2019},
  volume={},
  number={},
  pages={7185-7189},
  keywords={Acoustics;Decoding;Training;Reinforcement learning;Silicon;Linear programming;Recurrent neural networks;Extractive spoken document summarization;reinforcement learning;convolutional neural network;recurrent neural network;hierarchical encoding},
  doi={10.1109/ICASSP.2019.8683758}}

@inproceedings{chen_yang_2020_multi,
    title = "Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization",
    author = "Chen, Jiaao  and
      Yang, Diyi",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.336/",
    doi = "10.18653/v1/2020.emnlp-main.336",
    pages = "4106--4118",
    abstract = "Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations{---}an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers{---}remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at \url{https://github.com/GT-SALT/Multi-View-Seq2Seq}."
}

@inproceedings{song_etal_2020_summarizing,
    title = "Summarizing Medical Conversations via Identifying Important Utterances",
    author = "Song, Yan  and
      Tian, Yuanhe  and
      Wang, Nan  and
      Xia, Fei",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.63/",
    doi = "10.18653/v1/2020.coling-main.63",
    pages = "717--729",
    abstract = "Summarization is an important natural language processing (NLP) task in identifying key information from text. For conversations, the summarization systems need to extract salient contents from spontaneous utterances by multiple speakers. In a special task-oriented scenario, namely medical conversations between patients and doctors, the symptoms, diagnoses, and treatments could be highly important because the nature of such conversation is to find a medical solution to the problem proposed by the patients. Especially consider that current online medical platforms provide millions of public available conversations between real patients and doctors, where the patients propose their medical problems and the registered doctors offer diagnosis and treatment, a conversation in most cases could be too long and the key information is hard to be located. Therefore, summarizations to the patients' problems and the doctors' treatments in the conversations can be highly useful, in terms of helping other patients with similar problems have a precise reference for potential medical solutions. In this paper, we focus on medical conversation summarization, using a dataset of medical conversations and corresponding summaries which were crawled from a well-known online healthcare service provider in China. We propose a hierarchical encoder-tagger model (HET) to generate summaries by identifying important utterances (with respect to problem proposing and solving) in the conversations. For the particular dataset used in this study, we show that high-quality summaries can be generated by extracting two types of utterances, namely, problem statements and treatment recommendations. Experimental results demonstrate that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance."
}

@inproceedings{jung_interactive_2023,
author = {Jung, Jeesu and Seo, Hyein and Jung, Sangkeun and Chung, Riwoo and Ryu, Hwijung and Chang, Du-Seong},
title = {Interactive User Interface for Dialogue Summarization},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584057},
doi = {10.1145/3581641.3584057},
abstract = {Summarization is one of the important tasks of natural language processing used to distill information. Recently, the sequence-to-sequence method was applied, in a general manner, to summarization tasks. The problem is that a large amount of information must be pre-trained for a specific domain, and information other than input statements cannot be utilized. To compensate for this shortcoming, controllable summarization has recently been in the spotlight. We introduced three properties into controllable summarization: 1) a new human-machine communication input format, 2) a robust constraint-sensitive summarization method for these formats, and 3) a practical interactive summarization interface available to the user. Experiments on the Wizard-of-Wikipedia dataset show that applying this input format and the constraint-sensitive method enhances summarization performance compared to the typical method. A user study shows that the interactive summarization interface is practical and that participants are evaluating it positively.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {934–957},
numpages = {24},
keywords = {Dialogue summarization, constraint-sensitive generation, neural networks, text tagging},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@ARTICLE{furui_ssum_2004,
  author={Furui, S. and Kikuchi, T. and Shinnaka, Y. and Hori, C.},
  journal={IEEE Transactions on Speech and Audio Processing}, 
  title={Speech-to-text and speech-to-speech summarization of spontaneous speech}, 
  year={2004},
  volume={12},
  number={4},
  pages={401-408},
  keywords={Data mining;Speech recognition;Speech synthesis;Concatenated codes;Broadcasting;Text recognition;Laboratories;Synthesizers;Natural languages;Compaction},
  doi={10.1109/TSA.2004.828699}}

@inproceedings{wang22n_interspeech,
  title     = {ESSumm: Extractive Speech Summarization from Untranscribed Meeting},
  author    = {Jun Wang},
  year      = {2022},
  booktitle = interspeech # " 2022",
  pages     = {3243--3247},
  doi       = {10.21437/Interspeech.2022-945},
  issn      = {2958-1796},
}

@inproceedings{chuang20b_interspeech,
  title     = {SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering},
  author    = {Yung-Sung Chuang and Chi-Liang Liu and Hung-yi Lee and Lin-shan Lee},
  year      = {2020},
  booktitle = interspeech # " 2020",
  pages     = {4168--4172},
  doi       = {10.21437/Interspeech.2020-1570},
  issn      = {2958-1796},
}

@inproceedings{retkowski-etal-2025-ai,
    title = "{The AI Co-Ethnographer}: How Far Can Automation Take Qualitative Research?",
    author = "Retkowski, Fabian  and
      Sudmann, Andreas  and
      Waibel, Alexander",
    editor = {H{\"a}m{\"a}l{\"a}inen, Mika  and
      {\"O}hman, Emily  and
      Bizzoni, Yuri  and
      Miyagawa, So  and
      Alnajjar, Khalid},
    booktitle = "Proceedings of the 5th International Conference on Natural Language Processing for Digital Humanities",
    month = may,
    year = "2025",
    address = "Albuquerque, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.nlp4dh-1.8/",
    pages = "73--90",
    ISBN = "979-8-89176-234-3",
    abstract = "Qualitative research often involves labor-intensive processes that are difficult to scale while preserving analytical depth. This paper introduces The AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for qualitative research and designed to move beyond the limitations of simply automating code assignments, offering a more integrated approach. AICoE organizes the entire process, encompassing open coding, code consolidation, code application, and even pattern discovery, leading to a comprehensive analysis of qualitative data."
}

@inproceedings{jamshid_lou_johnson_2020_end,
    title = "End-to-End Speech Recognition and Disfluency Removal",
    author = "Jamshid Lou, Paria  and
      Johnson, Mark",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.186/",
    doi = "10.18653/v1/2020.findings-emnlp.186",
    pages = "2051--2061",
    abstract = "Disfluency detection is usually an intermediate step between an automatic speech recognition (ASR) system and a downstream task. By contrast, this paper aims to investigate the task of end-to-end speech recognition and disfluency removal. We specifically explore whether it is possible to train an ASR model to directly map disfluent speech into fluent transcripts, without relying on a separate disfluency detection model. We show that end-to-end models do learn to directly generate fluent transcripts; however, their performance is slightly worse than a baseline pipeline approach consisting of an ASR system and a specialized disfluency detection model. We also propose two new metrics for evaluating integrated ASR and disfluency removal models. The findings of this paper can serve as a benchmark for further research on the task of end-to-end speech recognition and disfluency removal in the future."
}

@inproceedings{gaido_etal_2024_speech,
    title = "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?",
    author = "Gaido, Marco  and
      Papi, Sara  and
      Negri, Matteo  and
      Bentivogli, Luisa",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.789/",
    doi = "10.18653/v1/2024.acl-long.789",
    pages = "14760--14778",
    abstract = "The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST."
}

@ARTICLE{home_assist_heab2019,

  author={Haeb-Umbach, Reinhold and Watanabe, Shinji and Nakatani, Tomohiro and Bacchiani, Michiel and Hoffmeister, Bjorn and Seltzer, Michael L. and Zen, Heiga and Souden, Mehrez},

  journal={IEEE Signal Processing Magazine}, 

  title={Speech Processing for Digital Home Assistants: Combining Signal Processing With Deep-Learning Techniques}, 

  year={2019},

  volume={36},

  number={6},

  pages={111-124},

  keywords={Microphones;Speech recognition;Speech processing;Loudspeakers;Reverberation},

  doi={10.1109/MSP.2019.2918706}}

@book{Louviere_Flynn_Marley_2015, place={Cambridge}, title={Best-Worst Scaling: Theory, Methods and Applications}, publisher={Cambridge University Press}, author={Louviere, Jordan J. and Flynn, Terry N. and Marley, A. A. J.}, year={2015}} 

@inproceedings{hori2002automatic,
  title={Automatic speech summarization applied to English broadcast news speech},
  author={Hori, Chiori and Furui, Sadaoki and Malkin, Rob and Yu, Hua and Waibel, Alex},
  booktitle=icassp # ", 2002",
  volume={1},
  pages={I--9},
  year={2002},
  organization={IEEE},
  url = {https://ieeexplore.ieee.org/document/5743641},
}

@inproceedings{min_etal_2023_factscore,
    title = "{FA}ct{S}core: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    author = "Min, Sewon  and
      Krishna, Kalpesh  and
      Lyu, Xinxi  and
      Lewis, Mike  and
      Yih, Wen-tau  and
      Koh, Pang  and
      Iyyer, Mohit  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.741/",
    doi = "10.18653/v1/2023.emnlp-main.741",
    pages = "12076--12100",
    abstract = "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs{---}InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI{---}and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58{\%}). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2{\%} error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost {\$}26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via {\textquoteleft}pip install factscore{\textquoteleft}."
}

@misc{listennotes2025,
  author       = {{ListenNotes}},
  title        = {Podcast Stats: How many podcasts are there?},
  year         = {2025},
  howpublished = {\url{https://www.listennotes.com/podcast-stats/}},
  note         = {Accessed: 2025-05-19}
}

@inproceedings{bett2000multimodal,
  title={Multimodal Meeting Tracker.},
  author={Bett, Michael and Gross, Ralph and Yu, Hua and Zhu, Xiaojin and Pan, Yue and Yang, Jie and Waibel, Alex},
  booktitle={RIAO},
  pages={32--45},
  year={2000},
  organization={Paris, France}
}

@inproceedings{suhm94_icslp,
  title     = {Towards better language models for spontaneous speech},
  author    = {B. Suhm and Alex Waibel},
  year      = {1994},
  booktitle = {3rd International Conference on Spoken Language Processing (ICSLP 1994)},
  pages     = {831--834},
  doi       = {10.21437/ICSLP.1994-222},
  issn      = {2958-1796},
}

@inproceedings{zeppenfeld1997recognition,
  title={Recognition of conversational telephone speech using the Janus speech engine},
  author={Zeppenfeld, Torsten and Finke, Michael and Ries, Klaus and Westphal, Martin and Waibel, Alex},
  booktitle=icassp # ", 1997",
  volume={3},
  pages={1815--1818},
  year={1997},
  organization={IEEE},
  url = {https://ieeexplore.ieee.org/document/598889},
}

@inproceedings{gross2000towards,
  title={Towards a multimodal meeting record},
  author={Gross, Ralph and Bett, Michael and Yu, Hua and Zhu, Xiaojin and Pan, Yue and Yang, Jie and Waibel, Alex},
  booktitle={2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No. 00TH8532)},
  volume={3},
  pages={1593--1596},
  year={2000},
  organization={IEEE},
  url = {https://ieeexplore.ieee.org/document/871074},
}

@inproceedings{kano2023summarize,
  author    = {Takatomo Kano and Atsunori Ogawa and Marc Delcroix and Kohei Matsuura and Takanori Ashihara and William Chen and Shinji Watanabe},
  title     = {Summarize while translating: Universal model with parallel decoding for summarization and translation},
  booktitle = {2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  year      = {2023},
  publisher = {IEEE},
  pages     = {1--8},
  url = {https://ieeexplore.ieee.org/document/10389781},
}

@article{likert1932technique,
  title={A technique for the measurement of attitudes.},
  author={Likert, Rensis},
  journal={Archives of psychology},
  year={1932}
}

@inproceedings{zheng-llm-judge-2024,
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
title = {Judging {LLM}-as-a-{Judge} with {MT}-bench and {Chatbot Arena}},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2020},
numpages = {29},
location = {New Orleans, LA, USA},
series = {NIPS '23},
url = {https://dl.acm.org/doi/10.5555/3666122.3668142},
}

@phdthesis{Sharma2024,
author = "Roshan Sharma",
title = "{End-to-End Modeling for Abstractive Speech Summarization}",
year = "2024",
month = "4",
url = "https://kilthub.cmu.edu/articles/thesis/End-to-End_Modeling_for_Abstractive_Speech_Summarization/25451419",
doi = "10.1184/R1/25451419.v1",
school = "Carnegie Mellon University",
}

@misc{thakur2025judgingjudgesevaluatingalignment,
      title={Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges}, 
      author={Aman Singh Thakur and Kartik Choudhary and Venkat Srinik Ramayapally and Sankaran Vaidyanathan and Dieuwke Hupkes},
      year={2025},
      publisher = {arXiv},
      note = {arXiv:2406.12624 [cs.CL]},
      url={https://arxiv.org/abs/2406.12624}, 
}

@inproceedings{mastropaolo2023evaluatingcodesummarizationtechniques,
author = {Mastropaolo, Antonio and Ciniselli, Matteo and Di Penta, Massimiliano and Bavota, Gabriele},
title = {Evaluating Code Summarization Techniques: A New Metric and an Empirical Characterization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639174},
doi = {10.1145/3597503.3639174},
abstract = {Several code summarization techniques have been proposed in the literature to automatically document a code snippet or a function. Ideally, software developers should be involved in assessing the quality of the generated summaries. However, in most cases, researchers rely on automatic evaluation metrics such as BLEU, ROUGE, and METEOR. These metrics are all based on the same assumption: The higher the textual similarity between the generated summary and a reference summary written by developers, the higher its quality. However, there are two reasons for which this assumption falls short: (i) reference summaries, e.g., code comments collected by mining software repositories, may be of low quality or even outdated; (ii) generated summaries, while using a different wording than a reference one, could be semantically equivalent to it, thus still being suitable to document the code snippet. In this paper, we perform a thorough empirical investigation on the complementarity of different types of metrics in capturing the quality of a generated summary. Also, we propose to address the limitations of existing metrics by considering a new dimension, capturing the extent to which the generated summary aligns with the semantics of the documented code snippet, independently from the reference summary. To this end, we present a new metric based on contrastive learning to capture said aspect. We empirically show that the inclusion of this novel dimension enables a more effective representation of developers' evaluations regarding the quality of automatically generated summaries.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {218},
numpages = {13},
keywords = {code summarization, contrastive learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@misc{goyal2023newssummarizationevaluationera,
      title={News Summarization and Evaluation in the Era of {GPT-3}}, 
      author={Tanya Goyal and Junyi Jessy Li and Greg Durrett},
      year={2023},
      publisher = {arXiv},
      note = {arXiv:2209.12356 [cs.CL]},
      url={https://arxiv.org/abs/2209.12356}, 
}

@article{chowdhurytranscripterrors2024,
author = {Chowdhury, Priyanjana and Sarkar, Nabanika and Nath, Sanghamitra and Sharma, Utpal},
title = {Analyzing the Effects of Transcription Errors on Summary Generation of Bengali Spoken Documents},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3678005},
doi = {10.1145/3678005},
abstract = {Automatic speech recognition (ASR) has become an indispensable part of the AI domain, with various speech technologies reliant on it. The quality of speech recognition depends on the amount of annotated data used to train an ASR system, among other factors. For a low-resourced language, this is a severe constraint and thus ASR quality is often poor. Humans can read through text containing ASR-errors, provided the context of the sentence is preserved. Yet in cases of transcripts generated by ASR systems of low-resource languages, multiple important words are misrecognized and the context is mostly lost; discerning such a text becomes nearly impossible. This article analyzes the types of transcription errors that occur while generating ASR transcripts of spoken documents in Bengali, an under-resourced language predominantly spoken in India and Bangladesh. The transcripts of the Bengali spoken document are generated using the ASR of Google Cloud Speech. The article also explores if there is an effect of such transcription errors in generating speech summaries of these spoken documents. Summarization is carried out extractively; sentences are selected from the ASR-generated text of the spoken document. Speech summaries are created by aggregating the speech-segments from the original speech of the selected sentences. Subjective evaluation shows the “readability” of the spoken summaries are not degraded by ASR errors, but the quality is affected due to the reliance on intermediate text-summary containing transcription errors.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {136},
numpages = {28},
keywords = {Low-resource language, extractive speech sumarization, automatic speech recognition, transcription error, speech summary}
}

@misc{schneider2025policiesevaluationonlinemeeting,
      title={Policies and Evaluation for Online Meeting Summarization}, 
      author={Felix Schneider and Marco Turchi and Alex Waibel},
      year={2025},
      publisher = {arXiv},
      note = {arXiv:2502.03111 [cs.CL]},
      url={https://arxiv.org/abs/2502.03111}, 
}

@inproceedings{Kroll_2024,
   title={Optimizing the role of human evaluation in LLM-based spoken document summarization systems},
   url={http://dx.doi.org/10.21437/Interspeech.2024-2268},
   DOI={10.21437/interspeech.2024-2268},
   booktitle=interspeech # " 2024",
   publisher={ISCA},
   author={Kroll, Margaret and Kraus, Kelsey},
   year={2024},
   month=sep, pages={1935–1939},
}


@Article{rathdifferentgroundtruths1961,
  author={G. J. Rath and A. Resnick and T. R. Savage},
  title={{The formation of abstracts by the selection of sentences. Part I. Sentence selection by men and machines}},
  journal={American Documentation},
  year=1961,
  volume={12},
  number={2},
  pages={139-141},
  month={April},
  keywords={},
  doi={10.1002/asi.5090120210},
  abstract={Auto‐abstracting techniques based on high‐frequency words show an extremely small variation among themselves in the selection of sentences to form abstracts. Human selection of sentences, although less variable than chance expectancy, is considerably more variable than the machine methods. There was very little agreement between the subjects and machine methods in their selection of representative sentences.},
  url={https://ideas.repec.org/a/bla/amedoc/v12y1961i2p139-141.html}
}

@inproceedings{vedantam2015cider,
author = {Vedantam, Ramakrishna and Zitnick, C. and Parikh, Devi},
year = {2015},
month = {06},
pages = {4566-4575},
title = {CIDEr: Consensus-based image description evaluation},
doi = {10.1109/CVPR.2015.7299087},
booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2015, Boston, MA, USA, June 7-12, 2015},
}

@inproceedings{spice2016,
  title     = {{SPICE: }Semantic Propositional Image Caption Evaluation},
  author    = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  year      = {2016},
  booktitle = {ECCV},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-46454-1_24},
}
@inproceedings{NEURIPS2021_e4d2b6e6,
 author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27263--27277},
 publisher = {Curran Associates, Inc.},
 title = {BARTScore: Evaluating Generated Text as Text Generation},
 url = {https://proceedings.neurips.cc/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf},
 volume = {34},
 year = {2021}
}

@INPROCEEDINGS{merine2022risks,

  author={Merine, Regina and Purkayastha, Saptarshi},

  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 

  title={Risks and Benefits of AI-generated Text Summarization for Expert Level Content in Graduate Health Informatics}, 

  year={2022},

  volume={},

  number={},

  pages={567-574},

  keywords={Annotations;Plagiarism;Education;Medical services;Transformers;Robustness;Bioinformatics;Text Summarization;BERT;Artificial Intelli-gence;Health Informatics},

  doi={10.1109/ICHI54592.2022.00113}}


@misc{shandilya2021fair,
  author       = {Anurag Shandilya and
                  Abhisek Dash and
                  Abhijnan Chakraborty and
                  Kripabandhu Ghosh and
                  Saptarshi Ghosh},
  title        = {Fairness for Whom? Understanding the Reader's Perception of Fairness
                  in Text Summarization},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.12406},
  publisher = {arXiv},
  note = {arXiv:2101.12406 [cs.IR]},
  timestamp    = {Tue, 02 Feb 2021 09:52:17 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-12406.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{keswani2021dialects,
author = {Keswani, Vijay and Celis, L. Elisa},
title = {Dialect Diversity in Text Summarization on Twitter},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450108},
doi = {10.1145/3442381.3450108},
abstract = {Discussions on Twitter involve participation from different communities with different dialects and it is often necessary to summarize a large number of posts into a representative sample to provide a synopsis. Yet, any such representative sample should sufficiently portray the underlying dialect diversity to present the voices of different participating communities representing the dialects. Extractive summarization algorithms perform the task of constructing subsets that succinctly capture the topic of any given set of posts. However, we observe that there is dialect bias in the summaries generated by common summarization approaches, i.e., they often return summaries that under-represent certain dialects. The vast majority of existing “fair” summarization approaches require socially salient attribute labels (in this case, dialect) to ensure that the generated summary is fair with respect to the socially salient attribute. Nevertheless, in many applications, these labels do not exist. Furthermore, due to the ever-evolving nature of dialects in social media, it is unreasonable to label or accurately infer the dialect of every social media post. To correct for the dialect bias, we employ a framework that takes an existing text summarization algorithm as a blackbox and, using a small set of dialect-diverse sentences, returns a summary that is relatively more dialect-diverse. Crucially, this approach does not need the posts being summarized to have dialect labels, ensuring that the diversification process is independent of dialect classification/identification models. We show the efficacy of our approach on Twitter datasets containing posts written in dialects used by different social groups defined by race or gender; in all cases, our approach leads to improved dialect diversity compared to standard text summarization approaches.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3802–3814},
numpages = {13},
keywords = {Text Summarization, Fairness, Dialect Diversity},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@misc{kumar2022meetingsummarizationsurveystate,
      title={Meeting Summarization: A Survey of the State of the Art}, 
      author={Lakshmi Prasanna Kumar and Arman Kabiri},
      year={2022},
      publisher = {arXiv},
      note = {arXiv:2212.08206 [cs.CL]},
      url={https://arxiv.org/abs/2212.08206}, 
}


@INPROCEEDINGS{jiminez2020,

  author={Jiménez, Rafael Zequeira and Naderi, Babak and Möller, Sebastian},

  booktitle={2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)}, 

  title={Effect of Environmental Noise in Speech Quality Assessment Studies using Crowdsourcing}, 

  year={2020},

  volume={},

  number={},

  pages={1-6},

  keywords={Crowdsourcing;Headphones;Computers;Working environment noise;Quality assessment;Background noise;Reliability;speech quality assessment;crowdsourcing;background noise;background noise impact},

  doi={10.1109/QoMEX48832.2020.9123144}}


@misc{cornell2023chime,
  author       = {Samuele Cornell and
                  Matthew Wiesner and
                  Shinji Watanabe and
                  Desh Raj and
                  Xuankai Chang and
                  Paola Garc{\'{\i}}a and
                  Yoshiki Masuyama and
                  Zhong{-}Qiu Wang and
                  Stefano Squartini and
                  Sanjeev Khudanpur},
  title        = {The CHiME-7 {DASR} Challenge: Distant Meeting Transcription with Multiple
                  Devices in Diverse Scenarios},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.13734},
  doi          = {10.48550/ARXIV.2306.13734},
  publisher = {arXiv},
  note = {arXiv:2306.13734 [eess.AS]},
  timestamp    = {Wed, 13 Nov 2024 15:45:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-13734.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      publisher = {arXiv},
      note = {arXiv:2303.08774 [cs.CL]},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The {Llama 3} Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      publisher = {arXiv},
      note = {arXiv:2407.21783 [cs.AI]},
      url={https://arxiv.org/abs/2407.21783}, 
}


@article{Brazil2019PROMETHEUS,
  title={PROMETHEUS},
  author={Brian Brazil and Ellen Troutman-Zaig and Rebecca A Demarest},
  journal={Poems for the Millennium, Volume Three},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:209053629}
}


@misc{barnes2025summarizationmetricsspanishbasque,
      title={Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?}, 
      author={Jeremy Barnes and Naiara Perez and Alba Bonet-Jover and Begoña Altuna},
      year = {2025},
      month = mar,
      publisher = {arXiv},
      note = {arXiv:2503.17039 [cs.CL]},
      keywords = {Computer Science - Computation and Language},
     url={https://arxiv.org/abs/2503.17039}, 
}